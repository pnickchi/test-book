<!-- Google tag (gtag.js) -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-DZWGQVJSHE"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DZWGQVJSHE');
</script>

# Introduction {.unnumbered}

> **The most important maxim for data analysis to heed, and one which many statisticians seem to have shunned, is this: "Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise."**

<p style="text-align: right;">*John W. Tukey* [-@tukey1962, p. 13]</p>

Data collection worldwide has proven to be a valuable tool for uncovering significant insights across various **populations of interest**. Whether it involves capturing political preferences in a specific demographic ahead of an upcoming election or assessing the effectiveness of an innovative medical treatment through a randomized clinical trial compared to a standard treatment, data plays a crucial role in enhancing our understanding. At times, this understanding can become quite complex, especially when attempting to untangle the relationships between different variables within a given population or even across two or more populations.

![Image by [*Manfred Stege*](https://pixabay.com/users/manfredsteger-1848497/) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-emotion-bored-6230197/).](img/question.png){width="300"}


In a vast and diverse field like data science, it is essential to craft effective and transparent solutions that ensure proper data management. However, collecting data from entire populations—essentially conducting a full census—can often be unfeasible due to resource constraints, such as limited finances, workforce, or technical infrastructure. Despite these limitations, our primary goal remains to gain insights about any given population of interest, even when data is scarce. In this context, **statistical inference** serves as a powerful tool that enables us to derive insights, even with limited data.


## The Test Mind Map

::: {#fig-test-general-mindmap}
```{mermaid}
mindmap
  root((Frequentist
  Hypothesis 
  Testings
  ))
    Simulation Based<br/>Tests
    Classical<br/>Tests
      (Chapter 1: <br/>Tests for One<br/>Continuous<br/>Population Mean)
        {{Unbounded<br/>Response}}
        {{Proportion between<br/>0 and 1<br/>obtained from a <br/>Binary Response}}
      (Chapter 2: <br/>Tests for Two<br/>Continuous<br/>Population Means)
        Two<br/>Independent<br/>Populations
          {{Unbounded<br/>Responses}}
          {{Proportions between<br/>0 and 1<br/>obtained from two <br/>Binary Responses}}
        Two<br/>Related<br/>Populations or<br/>Measurements
          {{Unbounded<br/>Responses}}
      (Chapter 3: <br/>ANOVA related <br/>Tests for<br/>k Continuous<br/>Population Means)
        {{Unbounded<br/>Responses}}
```

A general hypothesis testing mind map outlining all techniques explored in this book. Depending on the overall approach to be used, these techniques are divided into two broad categories: classical and simulation-based tests.
:::

## The Test Workflow

::: {#fig-classical-hypothesis-testing-workflow}
![](img/mini-test-book-hypothesis-testing-workflow.png){width="1500"}

A classical-based hypothesis testing workflow structured in four substages: *general settings*, *hypotheses definitions*, *test flavour and components*, and *inferential conclusions*.
:::
