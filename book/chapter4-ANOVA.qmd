<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DZWGQVJSHE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-DZWGQVJSHE');
</script>

# ANOVA-related Tests for $k$ Continuous Population Means {#sec-anova}

```{r}
#| include: false

library(reticulate)
library(DT)

# Setting up Python dependencies
py_require("pandas")
py_require("numpy")
py_require("statsmodels")
py_require("scipy")
pandas <- import("pandas")
numpy <- import("numpy")
sklearn <- import("sklearn")
statsmodels <- import("statsmodels")
scipy.stats <- import("scipy.stats")

options(htmlwidgets.TOJSON_ARGS = list(na = 'string')) 

print_as_dt <- function(x){
  DT::datatable(x,
                rownames = FALSE,
                options = list(
                  dom = 'ltipr', 
                  autoWidth = TRUE,
                  columnDefs = list(
                    list(className = 'dt-left', targets = "_all"),
                    list(width = '200px', targets = "_all")
                    )
                  )
                )
}
```

::::: LO
::: LO-header
Learning Objectives
:::

::: LO-container
By the end of this chapter, you will be able to:

-   Explain how analysis of variance (ANOVA) **partitions** total variation into **between-group** and **within-group** components.
-   Interpret the meaning of the $F$-statistic and its role in testing mean differences across groups.
-   Describe the difference between **main effects** and **interaction effects** in a two-way ANOVA.
-   Identify potential **interaction patterns** via appropriate plotting when conducting a two-way ANOVA.
-   Conduct one-way and two-way ANOVA in a **reproducible manner** using the hypothesis testing workflow via `R` and `Python`.
-   Identify the assumptions underlying one-way and two-way ANOVA, including **independence**, **normality**, and **homoscedasticity**.
-   Evaluate **model diagnostics** to assess whether ANOVA assumptions are met.
:::
:::::

It is time to expand our hypothesis testing framework from comparing two population means to a generalized approach for $k$ population means by exploring **analysis of variance (ANOVA)** as shown in @fig-test-chapter-3-mindmap. ANOVA is a foundational test in frequentist statistics designed to compare means across multiple populations under specific distributional assumptions, which we will examine in this chapter. Originally formalized by Ronald A. Fisher [@fisher1925], ANOVA extends the concept of the two-sample $t$-test by partitioning the total variability of our outcome of interest into components attributable to **main effects**, **interactions**, and **random error**. This partitioning enables us to determine whether observed differences in groups are improbable to appear under the null hypothesis, which states that all groups share the same population mean.

::: {#fig-test-chapter-3-mindmap}
```{mermaid}
mindmap
  root((Frequentist
  Hypothesis 
  Testings
  ))
    Simulation Based<br/>Tests
    Classical<br/>Tests
      (Chapter 2: <br/>Tests for One<br/>Continuous<br/>Population Mean)
      (Chapter 3: <br/>Tests for Two<br/>Continuous<br/>Population Means)
      (Chapter 4: <br/>ANOVA-related <br/>Tests for<br/>k Continuous<br/>Population Means)
        {{Unbounded<br/>Responses}}
          One<br/>Factor type<br/>Feature
            )One way<br/>ANOVA(
          Two<br/>Factor type<br/>Features
            )Two way<br/>ANOVA(
      (Chapter 5: <br/>Chi-squared<br/>Tests)
```

A specific hypothesis testing mind map outlining the techniques explored in this chapter, which include ANOVA-related tests for $k$ population means.
:::

::::: Heads-up
::: Heads-up-header
Heads-up on some historical background in statistics!
:::
::: Heads-up-container
While Ronald A. Fisher (1890–1962) is rightly recognized as a significant figure in modern statistics for developing key concepts such as ANOVA, it is imperative to recognize the broader context of his legacy. Beyond his pioneering contributions to statistics, Fisher was also a prominent advocate of eugenics, a movement that promoted pseudoscientific ideas about human heredity and social hierarchy. His involvement with the eugenics community is well documented [@mackenzie1981]. These views have been called out by certain members of the scientific community [@tarran2020]. That said, we (the authors of this mini-book) consider that students and scholars need to distinguish Fisher’s statistical insights from the unethical ideologies he supported, while also acknowledging the social responsibility to critique the historical misuse of science.

Fisher’s statistical work remains central to frequentist inference. However, educators and researchers must confront the historical connection between statistical innovation and eugenic thinking in early 20th-century science [@tabery2015]. By explicitly addressing this context, we can acknowledge the rigour of Fisher’s methodological contributions while rejecting the discriminatory worldviews he promoted. This approach aligns with contemporary efforts to ensure that teaching statistics is grounded not only in subject matter accuracy but also in ethical reflection and historical accountability [@kennedyshaffer02024].
:::
:::::

This chapter builds on the concepts introduced in the earlier works by @sec-one-continuous-mean and @sec-two-continuous-means, which concentrated on hypothesis testing for one and two groups, respectively. While those chapters primarily addressed pairwise comparisons, we will now shift our focus to multiple comparisons:

- The **one-way ANOVA**, discussed in @sec-one-way-ANOVA, will assume a **single categorical factor with** $k$ levels. This approach allows us to test whether at least one group mean is significantly different from the others.
- In contrast, the **two-way ANOVA**, from @sec-two-way-ANOVA, will incorporate an **additional categorical factor with** $m$ levels, permitting the analysis of both main effects and interaction effects. Interaction terms in ANOVA are especially useful for revealing cases where the effect of one factor depends on the level of another, a situation often encountered in complex experimental designs.

::::: Heads-up
::: Heads-up-header
Heads-up on the importance of ANOVA in data science!
:::
::: Heads-up-container
In modern data science, ANOVA remains significant, particularly in **A/B/n testing** (the generalization of **A/B testing** to multiple treatments). This approach allows for the simultaneous evaluation of various treatment variations or designs. ANOVA is valued for its interpretability and its flexibility in handling both **balanced** designs (where there is an equal number of replicates for each experimental treatment) and **unbalanced** designs (where there are unequal numbers of replicates). These strengths have solidified its role in both academic research and practical applications.

![Image by [*Manfred Steger*](https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3699345) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-download-data-transfer-3947910/).](img/pulling.png){width="80%"}
:::
:::::

The **simulated datasets** discussed in this chapter relate to an experimental context known as A/B/n testing, which is an expansion of traditional A/B testing as previously discussed. In a typical A/B testing, participants are randomly assigned to one of two strategies, referred to as treatments: treatment $A$ (the **control treatment**) and treatment $B$ (the **experimental treatment**). This approach enables us to infer **causation** concerning the following inquiry:

> **Will changing from treatment $A$ to treatment $B$ cause my outcome of interest, $Y$, to increase (or decrease, if that is the case)?**

In an ANOVA setting, the outcome must be **continuous**. The primary goal of A/B testing is to determine whether there is a statistically significant difference between treatments $A$ and $B$ concerning the outcome. Additionally, because we are conducting a proper randomized experiment, we can infer causation (which goes further than mere association), as treatment randomization enables us to get rid of the effect of further **confounders**.

::::: Heads-up
::: Heads-up-header
Heads-up on confounding!
:::
::: Heads-up-container
In causal inference, confounding refers to the mixing of effects between the outcome of interest, denoted as $Y$, the randomized factor $X$ (which is a two-level factor in A/B testing, corresponding to treatment $A$ and treatment $B$), and a third, uncontrollable factor known as the confounder $C$. This confounder is associated with the factor $X$ and independently affects the outcome $Y$, as shown by @fig-confounding.

![Diagram depicting confounding [@DSCI554].](img/confounding.png){#fig-confounding fig-align="center" width="70%"}
:::
:::::

When conducting experiments with more than two treatments in A/B testing, the experiment is referred to as **A/B/n testing**. In this case, the "*n*" does not indicate the sample size; rather, it simply represents any number of additional treatments beyond treatments $A$ and $B$. With this clarification, we can now proceed with our two ANOVA cases (one and two-way) with their corresponding datasets to illustrate how this statistical approach can be applied via the structured test workflow from @fig-hypothesis-testing-workflow found in @sec-test-workflow.

![Image by [*Pabitra Kaity*](https://pixabay.com/users/pabitrakaity-7844390/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=6370790) via [*Pixabay*](https://pixabay.com/illustrations/ads-digital-marketing-advertisements-6370790/).](img/ads.png){fig-align="center" width="50%"}

## One-way ANOVA {#sec-one-way-ANOVA}

Imagine you are part of a data-driven marketing team at a well-known tech company that operates a global online store. This store is heavily invested in using statistical experimentation on its website to enhance the customer shopping experience, with the goal of increasing profit margins in the long term. As part of this effort, the team frequently conducts A/B/n testings. These A/B/n testings specifically aim to **increase the customer conversion score**, which is the primary outcome measure. This **continuous score** is a unitless and standardized index of engagement, incorporating various elements such as clicks, time spent on the website, and the probability of making a purchase. The baseline of this score is set at $50$. This score measures customer responsiveness on the online store: **the higher the score, the greater the customer responsiveness**.

<br>

![Image by [*Manfred Steger*](https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3699345) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-observation-internship-3974186/).](img/study.png){width="80%"}

To start with the first of these two A/B/n testings, let us suppose the heads of the marketing and sales departments have come up with the groundbreaking idea of modifying the overall webpage design in two different experimental ways to investigate whether these modifications somehow **cause** the **mean customer conversion score** to change. For the sake of this case, the department heads aim to compare how this score behaves between these **two experimental webpage designs** and the **current webpage design**.

### Study Design {#sec-one-way-ANOVA-study-design}

The first stage of our testing workflow involves the main statistical inquiry we aim to address. Therefore, let us translate the above situation into a concrete question that is meaningful for both the stakeholders (namely, the marketing and sales department heads) and for you as part of the data-driven marketing team. We can frame the main inquiry as follows:

> **Will the change across the two experimental webpage designs and the current design cause any statistical difference in the population mean customer conversion score?**

![Image by [*Manfred Stege*](https://pixabay.com/users/manfredsteger-1848497/) via [*Pixabay*](https://pixabay.com/vectors/laptop-games-jigsaw-puzzle-puzzle-7426707/).](img/puzzle.png){width="300"}

This statistical inquiry represents the foundational step in our testing workflow. We transform a practical business question into a testable inferential statement (specifically, a causal one). By framing the problem as whether the average customer conversion score varies across different webpage designs, we connect stakeholder demands with statistical analysis.

In terms on how this A/B/n testing would be designed, let us consider the following aspects:

1. The experiment will solely focus on **webpage design**, which we consider a **controllable factor** determined by the **experimenter** (that is you, as part of the data-driven marketing team). This factor includes three different webpage layouts (or designs): $D_1$ (the current layout), $D_2$ (a new and experimental layout), and $D_3$ (another new and experimental layout). This setup creates a three-level factor. To summarize in statistical terms, we have:
    - One factor: **webpage design**.
    - Three **treatments** (the three layout options), which classifies this study as A/B/n testing. Treatment $D_1$ is referred to as the **control treatment**, whereas $D_2$ and $D_3$ are the **experimental treatments**.
    - The **continuous outcome variable $Y$**, defined as the customer conversion score, which has been explained previously.
2. **Before conducting the experiment**, assume that your team has performed a proper **power analysis** to determine the total sample size of customers to be recruited for the A/B/n testing. This power analysis has indicated an overall sample size of $n = 600$ customers (namely, **experimental units**). Consequently, each treatment group will consist of $\frac{n}{3} = 200$ customers, making this a **balanced experiment**.

::::: Heads-up
::: Heads-up-header
Heads-up on the importance of having a balanced experiment!
:::
::: Heads-up-container
In the **design and analysis of experiments**, having a balanced experiment (where each treatment receives the same number of experimental units) offers two advantages:

- Statistically, a balanced experiment helps **minimize the uncertainty in estimating the population parameters** necessary for hypothesis testing, which is essential for addressing our main statistical inquiries. 
- On the other hand, from a practical point of view, a balanced experiment makes analysis robust against **biased parameter estimation**, which might be caused by an uneven exposure of experimental units to the different treatments.

![Image by [*Manfred Stege*](https://pixabay.com/users/manfredsteger-1848497/) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-emotion-confused-6230199/).](img/question-2.png){width="250"}
:::
:::::

### Data Collection and Wrangling {#sec-one-way-ANOVA-data-collection-wrangling}

Having designed our statistical experiment, it is now time to execute it, which we refer to as **data collection**. To ensure we accurately assess the causality of our statistical inquiry regarding changes in the population mean customer conversion score across three different webpage designs, we must randomly assign our $n = 600$ experimental units evenly among the three treatments. This means we will collect $\frac{n}{3} = 200$ data points for each treatment. To achieve this, we can use **a suitable online tool** to route website traffic uniformly, potentially employing stratified randomization based on additional covariates, such as device type and geographical location. This approach will help us allocate the $n = 600$ experimental units to one of the three treatments effectively.

::::: Heads-up
::: Heads-up-header
Heads-up on the importance of treatment randomization in statistical experimentation!
:::
::: Heads-up-container
In A/B and A/B/n testing, online tools such as [Optimizely](https://www.optimizely.com) and [ABTasty](https://www.abtasty.com) enable experimenters to randomly assign web traffic units (i.e., experimental units) to different treatments of interest. These tools also offer several additional useful features, which are beyond the scope of this textbook. 

Treatment randomization is a valuable statistical tool because:

- It ensures that, on average, **observed** and **unobserved confounding variables** are evenly distributed across treatments. This helps to avoid obtaining biased estimates of population parameters when evaluating causality between outcomes and the treatments of interest.
- As discussed further in @sec-one-way-ANOVA-test-flavour, treatment randomization also guarantees that all observed data points are **statistically independent** concerning the **random noise** associated with their corresponding measurements. This statistical independence in the random noise (often referred to as the **random component**, which is also assumed to be **identically distributed**) is a critical assumption in ANOVA hypothesis testing. If these probabilistic assumptions are not met, the ANOVA test statistic and null distribution may not be appropriate for our observed data and inferential inquiries.
:::
:::::

<br>

![Image by [*Manfred Stege*](https://pixabay.com/users/manfredsteger-1848497/) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-portfolio-work-folder-3947911/).](img/screen.png){width="50%"}

Let us discuss our data collection process. Assume you have already conducted this A/B/n testing and recruited $n = 600$ customers for this experiment. These customers have been randomly assigned to one of the three webpage designs, and their conversion scores have been measured accordingly. To begin our inferential process, the following code snippets in `R` and `Python` will load the dataset, which contains the following columns:

- `webpage_design`: A factor-type column representing the webpage design to which each experimental unit was assigned ($D_1$, $D_2$ or $D_3$).
- `conversion_score`: A numeric, continuous column that records the measured customer conversion scores as previously described at the beginning of this one-way ANOVA section.

::: {.panel-tabset}

## **`R` Code**

``` {.r}
# Loading dataset
OneWay_ABn_customer_data <- read.csv("data/ABn_customer_data_one_factor.csv")

# Showing the first 100 customers of the A/B/n testing
head(OneWay_ABn_customer_data, n = 100)
```

## **`Python` Code**

``` {.python}
# Importing library
import pandas as pd

# Loading dataset
OneWay_ABn_customer_data = pd.read_csv("data/ABn_customer_data_one_factor.csv")

# Showing the first 100 customers of the A/B/n testing
print(OneWay_ABn_customer_data.head(100))
```

:::

::: {.panel-tabset}

## **`R` Output**

```{r}
#| label: tbl-ANOVA-ABn-data-r-one
#| tbl-cap: "First 100 rows of our A/B/n simulated data with webpage design as a standalone experimental factor."
#| echo: false
#| message: false

# Loading dataset
OneWay_ABn_customer_data <- read.csv("data/ABn_customer_data_one_factor.csv")

# Showing the first 100 customers of the A/B/n testing
head(OneWay_ABn_customer_data, n = 100) |> 
  print_as_dt()
```

## **`Python` Output**

```{python}
#| echo: false
#| message: false

# Importing library
import pandas as pd

# Loading dataset
OneWay_ABn_customer_data = pd.read_csv("data/ABn_customer_data_one_factor.csv")
```


```{r}
#| label: tbl-ANOVA-ABn-data-py-one
#| tbl-cap: "First 100 rows of our A/B/n simulated data with webpage design as a standalone experimental factor."
#| echo: false

# Showing the first 100 customers of the A/B/n testing
print_as_dt(head(py$OneWay_ABn_customer_data, 100))
```

:::

::::: Tip
::: Tip-header
Tip on the generative modelling process for the simulated data! 
:::
::: Tip-container
We have already clarified that the datasets used in this chapter, such as `OneWay_ABn_customer_data`, are simulated. We chose this approach to explain ANOVA so that the data used in our test workflow meets the modelling assumptions. This ensures that we can provide a clear and comprehensive explanation for the reader on this inferential tool. 

If you are interested in the generative modelling process for `OneWay_ABn_customer_data` and would like to reproduce it in either `R` or `Python`, please refer to @sec-one-way-ANOVA-sim. It is worth noting that the datasets used in this chapter are the ones generated using `R`, as both `R` and `Python` use different pseudo-random number generators, even when using the same simulation seeds.
:::
:::::

Now, after collecting the data, it is time to proceed with the data splitting into training and testing. For this specific case, assume the team in charge of this analysis decided to execute a 50-50 random data splitting which would yield a training set size of $n_{\text{training}} = 300$ and testing set size of $n_{\text{testing}} = 300$. Nevertheless, to keep the balanced nature of our experiment, we have to ensure that both sets will end up with 100 customers allocated to each one of the three treatments. Therefore, we need to implement a stratified randomization (where each treatment is a strata) as follows:

- `R`: This code performs a stratified random split of the `OneWay_ABn_customer_data` dataset into training and testing sets, ensuring reproducibility and balance across different levels of `webpage_design`. The library [{rsample}](https://rsample.tidymodels.org) [@rsample] is utilized for executing the stratified random split. Note that the `set.seed()` function is used to fix the random seed, allowing the results to be replicated. Then, the `initial_split()` function divides the `OneWay_ABn_customer_data` into two parts, allocating 50% for training and 50% for testing while maintaining proportional representation of each category in the `webpage_design` variable (the `strata` argument ensures this). The `training()` and `testing()` functions extract the respective subsets from the split object. Lastly, to implement a sanity check, we retrieve the dimensions of each subset (number of rows and columns) and examine the relative proportions of `webpage_design` levels within each split. This verification step ensures that the experimental treatments remain balanced in both training and testing sets, which is essential for unbiased modeling and valid inference in one-way ANOVA.

![Image by [*Manfred Steger*](https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3699345) via [*Pixabay*](https://pixabay.com/vectors/pixel-cells-pixel-social-network-3704049/).](img/paint.png){width="525"}

- `Python`: The code executes a stratified data splitting process similar to its `R` equivalent, utilizing the `train_test_split()` function from [scikit-learn](https://scikit-learn.org/stable/) [@scikit-learn]. A fixed random seed is set using the `random_state` parameter to ensure reproducibility, so that each execution generates the same random partition. The dataset, `OneWay_ABn_customer_data`, is then divided into two equal subsets: 50% for training and 50% for testing. The `stratify` argument in `train_test_split()` guarantees that the `webpage_design` maintains consistent proportional representation in both subsets, thus preserving balance across experimental treatments. After completing the split, we conduct the same sanity check as described above

::: {.panel-tabset}

## **`R` Code**

``` {.r}
# Loading library
library(rsample)

# Seed for reproducibility
set.seed(123)

# Randomly splitting into training and testing sets by webpage_design
OneWay_ABn_customer_data_splitting <- initial_split(OneWay_ABn_customer_data,
  prop = 0.5, strata = webpage_design
)

# Assigning data points to training and testing sets
OneWay_ABn_training <- training(OneWay_ABn_customer_data_splitting)
OneWay_ABn_testing <- testing(OneWay_ABn_customer_data_splitting)

# Sanity check
cat("Training shape:", dim(OneWay_ABn_training), "\n")
cat("Testing shape:", dim(OneWay_ABn_testing), "\n\n")
cat("Training proportions:\n")
print(prop.table(table(OneWay_ABn_training$webpage_design)))
cat("\nTesting proportions:\n")
print(prop.table(table(OneWay_ABn_testing$webpage_design)))
```

## **`Python` Code**

``` {.python}
# Importing library
from sklearn.model_selection import train_test_split

# Seed for reproducibility
random_state = 123

# Randomly splitting into training and testing sets by webpage_design
OneWay_ABn_training, OneWay_ABn_testing = train_test_split(
    OneWay_ABn_customer_data,
    test_size=0.5,
    stratify=OneWay_ABn_customer_data["webpage_design"],
    random_state=random_state
)

# Sanity check
print("Training shape:", OneWay_ABn_training.shape)
print("Testing shape:", OneWay_ABn_testing.shape)
print("\nTraining proportions:")
print(OneWay_ABn_training["webpage_design"].value_counts(normalize=True))
print("\nTesting proportions:")
print(OneWay_ABn_testing["webpage_design"].value_counts(normalize=True))
```

:::

::: {.panel-tabset}

## **`R` Output**

```{r}
#| echo: false
#| message: false

# Loading library
library(rsample)

# Seed for reproducibility
set.seed(123)

# Randomly splitting into training and testing sets by webpage_design
OneWay_ABn_customer_data_splitting <- initial_split(OneWay_ABn_customer_data,
  prop = 0.5, strata = webpage_design
)

# Assigning data points to training and testing sets
OneWay_ABn_training <- training(OneWay_ABn_customer_data_splitting)
OneWay_ABn_testing <- testing(OneWay_ABn_customer_data_splitting)

# Sanity check
cat("Training shape:", dim(OneWay_ABn_training), "\n")
cat("Testing shape:", dim(OneWay_ABn_testing), "\n\n")
cat("Training proportions:\n")
print(prop.table(table(OneWay_ABn_training$webpage_design)))
cat("\nTesting proportions:\n")
print(prop.table(table(OneWay_ABn_testing$webpage_design)))
```

## **`Python` Output**

```{python}
#| echo: false
#| message: false

# Importing library
from sklearn.model_selection import train_test_split

# Seed for reproducibility
random_state = 123

# Randomly splitting into training and testing sets by webpage_design
OneWay_ABn_training, OneWay_ABn_testing = train_test_split(
    OneWay_ABn_customer_data,
    test_size=0.5,
    stratify=OneWay_ABn_customer_data["webpage_design"],
    random_state=random_state
)

# Sanity check
print("Training shape:", OneWay_ABn_training.shape)
print("Testing shape:", OneWay_ABn_testing.shape)
print("\nTraining proportions:")
print(OneWay_ABn_training["webpage_design"].value_counts(normalize=True))
print("\nTesting proportions:")
print(OneWay_ABn_testing["webpage_design"].value_counts(normalize=True))
```

:::

### Exploratory Data Analysis {#sec-one-way-ANOVA-EDA}

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-one-summary-stats-r
#| tbl-cap: "Descriptive statistics of one-factor A/B/n testing via training set."
#| message: false
# Loading library
library(tidyverse)

# Computing summaries
OneWay_summary_table <- OneWay_ABn_training |>
  group_by(webpage_design) |>
  summarise(
    mean_conversion = round(mean(conversion_score), 2),
    sd_conversion = round(sd(conversion_score), 2),
    n = n()
  ) |>
  arrange(webpage_design)
OneWay_summary_table
```

## **`Python`**

```{python}
#| label: tbl-one-summary-stats-py
#| tbl-cap: "Descriptive statistics of one-factor A/B/n testing via training set."
#| message: false
# Importing library
import numpy as np

# Importing R training set via R library reticulate
OneWay_ABn_training = r.OneWay_ABn_training

OneWay_summary_table = (
    OneWay_ABn_training
    .groupby("webpage_design")
    .agg(
        mean_conversion=("conversion_score", lambda x: round(np.mean(x), 2)),
        sd_conversion=("conversion_score", lambda x: round(np.std(x, ddof=1), 2)),
        n=("conversion_score", "count")
    )
    .reset_index()
    .sort_values("webpage_design")
)
OneWay_summary_table
```

:::

```{r}
#| label: fig-hist-webpage-design-one
#| fig-cap: "Histograms of conversion score by webpage design via training data."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

hist_one_way <- ggplot(
  OneWay_ABn_training,
  aes(x = conversion_score, fill = webpage_design)
) +
  geom_histogram(
    alpha = 0.7,
    position = "identity",
    bins = 30,
    color = "white"
  ) +
  labs(
    x = "Conversion Score",
    y = "Count",
    fill = "Webpage Design"
  ) +
  theme_bw(base_size = 14) +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, vjust = 0.5),
    strip.text = element_text(size = 20),
    legend.position = "bottom",
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 20),
    legend.key.size = unit(1.4, "cm"),
    legend.spacing.x = unit(1, "cm")
  ) +
  scale_fill_manual(
    values = c(
      "D1" = "#E69F00",
      "D2" = "#56B4E9",
      "D3" = "#009E73"
    )
  )
hist_one_way
```


```{r}
#| label: fig-boxplot-webpage-design-one
#| fig-cap: "Side-by-side boxplots of conversion score by webpage design via training data."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

boxplots_webpage_design_one_way <- ggplot(OneWay_ABn_training, aes(
  x = webpage_design, y = conversion_score,
  fill = webpage_design
)) +
  geom_boxplot(alpha = 0.7, outlier.color = "black") +
  labs(
    x = "Webpage Design",
    y = "Conversion Score"
  ) +
  theme_minimal(base_size = 14) +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("D1" = "#E69F00", "D2" = "#56B4E9", "D3" = "#009E73"))
boxplots_webpage_design_one_way
```


### Testing Settings {#sec-one-way-ANOVA-testing-settings}

### Hypothesis Definitions {#sec-one-way-ANOVA-hypothesis-definitions}

### Test Flavour and Components {#sec-one-way-ANOVA-test-flavour}

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-one-fitting-r
#| tbl-cap: "One-way ANOVA: Conversion score by webpage design via testing set."
#| message: false
# Loading library
library(broom)

# Running one-way ANOVA
OneWay_aov <- aov(conversion_score ~ webpage_design,
  data = OneWay_ABn_testing
)

# Displaying full one-way ANOVA table
tidy(OneWay_aov) |>
  mutate_if(is.numeric, round, 2)
```

## **`Python`**

```{python}
#| label: tbl-one-fitting-py
#| tbl-cap: "One-way ANOVA: Conversion score by webpage design via testing set."
#| message: false
# Importing library
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Importing R testing set via R library reticulate
OneWay_ABn_testing = r.OneWay_ABn_testing

# Running one-way ANOVA
OneWay_model = ols('conversion_score ~ C(webpage_design)', data=OneWay_ABn_testing).fit()
OneWay_aov = sm.stats.anova_lm(OneWay_model, typ=2)

# Displaying full one-way ANOVA table
OneWay_aov = OneWay_aov.round(2)
OneWay_aov
```

:::

### Inferential Conclusions {#sec-one-way-ANOVA-inferential-conclusions}

```{r}
#| label: fig-q-q-plot-one-way
#| fig-cap: "Diagnostic Q-Q plot for one-way ANOVA of conversion score by webpage design via residuals."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

# Extract residuals from model
OneWay_resid <- augment(OneWay_aov)

# Q–Q plot for one-way ANOVA
qqplot_oneway <- ggplot(OneWay_resid, aes(sample = .resid)) +
  stat_qq(size = 2, color = "#6A3D9A", alpha = 0.6) +
  stat_qq_line(color = "black", linewidth = 1) +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
  ) +
  theme_bw(base_size = 14) +
  theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 15.5)
  )
qqplot_oneway
```

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-one-shapiro-wilks-r
#| tbl-cap: "Shapiro–Wilks normality test for one-way ANOVA via residuals."
#| message: false
# Extracting residuals
OneWay_resid <- residuals(OneWay_aov)

# Running Shapiro–Wilk test
OneWay_shapiro <- shapiro.test(OneWay_resid)

# Displaying outputs
tidy(OneWay_shapiro) |>
  mutate_if(is.numeric, round, 2)
```

## **`Python`**

```{python}
#| label: tbl-one-shapiro-wilks-py
#| tbl-cap: "Shapiro–Wilks normality test for one-way ANOVA via residuals."
#| message: false
# Importing library
from scipy.stats import shapiro

# Extracting residuals
OneWay_resid = OneWay_model.resid

# Running Shapiro–Wilk tests
OneWay_shapiro = shapiro(OneWay_resid)

# Displaying outputs
OneWay_shapiro = pd.DataFrame({
    "W Statistic": [round(OneWay_shapiro.statistic, 2)],
    "p-value": [round(OneWay_shapiro.pvalue, 2)]
})
OneWay_shapiro
```

:::

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-one-levene-r
#| tbl-cap: "Levene’s test for homogeneity of variances for one-way ANOVA via testing data."
#| message: false
# Loading library
library(car)

# Running Levene test
OneWay_levene <- leveneTest(conversion_score ~ as.factor(webpage_design),
  data = OneWay_ABn_testing
)

# Displaying outputs
tidy(OneWay_levene) |>
  mutate_if(is.numeric, round, 2)
```

## **`Python`**

```{python}
#| label: tbl-one-levene-py
#| tbl-cap: "Levene’s test for homogeneity of variances for one-way ANOVA via testing data."
#| message: false
# Importing library
from scipy.stats import levene

# Running Levene test
groups_OneWay = [group["conversion_score"].values for _, group in OneWay_ABn_testing.groupby("webpage_design")]
stat1, p1 = levene(*groups_OneWay)

# Displaying outputs
OneWay_levene = pd.DataFrame({
    "Levene Statistic": [round(stat1, 2)],
    "p-value": [round(p1, 2)]
})
OneWay_levene
```

:::

### Storytelling {#sec-one-way-ANOVA-storytelling}

## Two-way ANOVA {#sec-two-way-ANOVA}

### Study Design {#sec-two-way-ANOVA-study-design}

### Data Collection and Wrangling {#sec-two-way-ANOVA-data-collection-wrangling}

::: {.panel-tabset}

## **`R` Code**

``` {.r}
# Loading dataset
TwoWay_ABn_customer_data <- read.csv("data/ABn_customer_data_two_factors.csv")

# Showing the first 100 customers of the A/B/n testing
head(TwoWay_ABn_customer_data, n = 100)
```

## **`Python` Code**

``` {.python}
# Loading dataset
TwoWay_ABn_customer_data = pd.read_csv("data/ABn_customer_data_two_factors.csv")

# Showing the first 100 customers of the A/B/n testing
print(TwoWay_ABn_customer_data(100))
```

:::

::: {.panel-tabset}

## **`R` Output**

```{r}
#| label: tbl-ANOVA-ABn-data-r-two
#| tbl-cap: "First 100 rows of our A/B/n simulated data with webpage design and discount framing as a experimental factors."
#| echo: false
#| message: false

# Loading dataset
TwoWay_ABn_customer_data <- read.csv("data/ABn_customer_data_two_factors.csv")

# Showing the first 100 customers of the A/B/n testing
head(TwoWay_ABn_customer_data, n = 100) |> 
  print_as_dt()
```

## **`Python` Output**

```{python}
#| echo: false
#| message: false

# Loading dataset
TwoWay_ABn_customer_data = pd.read_csv("data/ABn_customer_data_two_factors.csv")
```


```{r}
#| label: tbl-ANOVA-ABn-data-py-two
#| tbl-cap: "First 100 rows of our A/B/n simulated data with webpage design and discount framing as a experimental factors."
#| echo: false

# Showing the first 100 customers of the A/B/n testing
print_as_dt(head(py$TwoWay_ABn_customer_data, 100))
```

:::



::: {.panel-tabset}

## **`R` Code**

``` {.r}
# Seed for reproducibility
set.seed(123)

# Create a combined stratification variable
TwoWay_ABn_customer_data <- TwoWay_ABn_customer_data |>
  mutate(strata = interaction(webpage_design, discount_framing))

# Randomly splitting into training and testing sets by both factors
TwoWay_ABn_customer_data_splitting <- initial_split(
  TwoWay_ABn_customer_data,
  prop = 0.5,
  strata = strata
)

# Assigning data points to training and testing sets
TwoWay_ABn_training <- training(TwoWay_ABn_customer_data_splitting)
TwoWay_ABn_testing  <- testing(TwoWay_ABn_customer_data_splitting)

# Sanity check
cat("Training shape:", dim(TwoWay_ABn_training), "\n")
cat("Testing shape:", dim(TwoWay_ABn_testing), "\n\n")

cat("Training proportions:\n")
print(prop.table(table(TwoWay_ABn_training$webpage_design, TwoWay_ABn_training$discount_framing)))

cat("\nTesting proportions:\n")
print(prop.table(table(TwoWay_ABn_testing$webpage_design, TwoWay_ABn_testing$discount_framing)))
```

## **`Python` Code**

``` {.python}
# Seed for reproducibility
random_state = 123

# Create a combined stratification variable
TwoWay_ABn_customer_data["strata"] = (
    TwoWay_ABn_customer_data["webpage_design"].astype(str)
    + "_"
    + TwoWay_ABn_customer_data["discount_framing"].astype(str)
)

# Randomly splitting into training and testing sets by both factors
TwoWay_ABn_training, TwoWay_ABn_testing = train_test_split(
    TwoWay_ABn_customer_data,
    test_size=0.5,
    stratify=TwoWay_ABn_customer_data["strata"],
    random_state=random_state
)

# Sanity check
print("Training shape:", TwoWay_ABn_training.shape)
print("Testing shape:", TwoWay_ABn_testing.shape)

print("\nTraining proportions:")
print(TwoWay_ABn_training.groupby(["webpage_design", "discount_framing"]).size().div(len(TwoWay_ABn_training)))

print("\nTesting proportions:")
print(TwoWay_ABn_testing.groupby(["webpage_design", "discount_framing"]).size().div(len(TwoWay_ABn_testing)))
```

:::

::: {.panel-tabset}

## **`R` Output**

```{r}
#| echo: false
#| message: false

# Seed for reproducibility
set.seed(123)

# Create a combined stratification variable
TwoWay_ABn_customer_data <- TwoWay_ABn_customer_data |>
  mutate(strata = interaction(webpage_design, discount_framing))

# Randomly splitting into training and testing sets by both factors
TwoWay_ABn_customer_data_splitting <- initial_split(
  TwoWay_ABn_customer_data,
  prop = 0.5,
  strata = strata
)

# Assigning data points to training and testing sets
TwoWay_ABn_training <- training(TwoWay_ABn_customer_data_splitting)
TwoWay_ABn_testing  <- testing(TwoWay_ABn_customer_data_splitting)

# Sanity check
cat("Training shape:", dim(TwoWay_ABn_training), "\n")
cat("Testing shape:", dim(TwoWay_ABn_testing), "\n\n")

cat("Training proportions:\n")
print(prop.table(table(TwoWay_ABn_training$webpage_design, TwoWay_ABn_training$discount_framing)))

cat("\nTesting proportions:\n")
print(prop.table(table(TwoWay_ABn_testing$webpage_design, TwoWay_ABn_testing$discount_framing)))
```

## **`Python` Output**

```{python}
#| echo: false
#| message: false

# Seed for reproducibility
random_state = 123

# Create a combined stratification variable
TwoWay_ABn_customer_data["strata"] = (
    TwoWay_ABn_customer_data["webpage_design"].astype(str)
    + "_"
    + TwoWay_ABn_customer_data["discount_framing"].astype(str)
)

# Randomly splitting into training and testing sets by both factors
TwoWay_ABn_training, TwoWay_ABn_testing = train_test_split(
    TwoWay_ABn_customer_data,
    test_size=0.5,
    stratify=TwoWay_ABn_customer_data["strata"],
    random_state=random_state
)

# Sanity check
print("Training shape:", TwoWay_ABn_training.shape)
print("Testing shape:", TwoWay_ABn_testing.shape)

print("\nTraining proportions:")
print(TwoWay_ABn_training.groupby(["webpage_design", "discount_framing"]).size().div(len(TwoWay_ABn_training)))

print("\nTesting proportions:")
print(TwoWay_ABn_testing.groupby(["webpage_design", "discount_framing"]).size().div(len(TwoWay_ABn_testing)))
```

:::

### Exploratory Data Analysis {#sec-two-way-ANOVA-EDA}

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-two-summary-stats-r
#| tbl-cap: "Descriptive statistics of two-factor A/B/n testing via training set."
#| message: false
# Computing summaries
TwoWay_summary_table <- TwoWay_ABn_training |>
  group_by(webpage_design, discount_framing) |>
  summarise(
    mean_conversion = round(mean(conversion_score), 2),
    sd_conversion = round(sd(conversion_score), 2),
    n = n()
  ) |>
  arrange(webpage_design, discount_framing)
TwoWay_summary_table
```

## **`Python`**

```{python}
#| label: tbl-two-summary-stats-py
#| tbl-cap: "Descriptive statistics of two-factor A/B/n testing via training set."
#| message: false
# Importing R training set via R library reticulate
TwoWay_ABn_training = r.TwoWay_ABn_training

# Computing summaries
TwoWay_summary_table = (
    TwoWay_ABn_training
    .groupby(["webpage_design", "discount_framing"])
    .agg(
        mean_conversion=("conversion_score", lambda x: round(np.mean(x), 2)),
        sd_conversion=("conversion_score", lambda x: round(np.std(x, ddof=1), 2)),
        n=("conversion_score", "count")
    )
    .reset_index()
    .sort_values(["webpage_design", "discount_framing"])
)
TwoWay_summary_table
```

:::

```{r}
#| label: fig-hist-webpage-design-two
#| fig-cap: "Histograms of conversion score by webpage design and discount framing via training data."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

hist_two_way <- ggplot(
  TwoWay_ABn_training,
  aes(x = conversion_score, fill = webpage_design)
) +
  geom_histogram(
    alpha = 0.7,
    position = "identity",
    bins = 30,
    color = "white"
  ) +
  facet_wrap(~discount_framing) +
  labs(
    x = "Conversion Score",
    y = "Count",
    fill = "Webpage Design"
  ) +
  theme_bw(base_size = 14) +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, vjust = 0.5),
    strip.text = element_text(size = 20),
    legend.position = "bottom",
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 20),
    legend.key.size = unit(1.4, "cm"),
    legend.spacing.x = unit(1, "cm")
  ) +
  scale_fill_manual(
    values = c(
      "D1" = "#E69F00",
      "D2" = "#56B4E9",
      "D3" = "#009E73"
    )
  )
hist_two_way
```



```{r}
#| label: fig-boxplot-webpage-design-two
#| fig-cap: "Side-by-side boxplots of conversion score by webpage design and discount framing via training data."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

boxplots_webpage_design_two_way <- ggplot(TwoWay_ABn_training, aes(
  x = discount_framing, y = conversion_score,
  fill = webpage_design
)) +
  geom_boxplot(alpha = 0.7, outlier.color = "black") +
  labs(
    x = "Discount Framing",
    y = "Conversion Score",
    fill = "Webpage Design"
  ) +
  theme_minimal(base_size = 14) +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    legend.position = "bottom",               
    legend.title = element_text(size = 20),    
    legend.text = element_text(size = 20),     
    legend.key.size = unit(1.4, "cm"),         
    legend.spacing.x = unit(2, "cm"),          
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  scale_fill_manual(values = c("D1" = "#E69F00", "D2" = "#56B4E9", "D3" = "#009E73"))
boxplots_webpage_design_two_way
```


```{r}
#| label: fig-interaction-plot
#| fig-cap: "Interaction plot of conversion score by webpage design and discount framing via training data."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

# Compute means for each combination
interaction_means <- TwoWay_ABn_training |>
  group_by(discount_framing, webpage_design) |>
  summarise(
    mean_conversion = mean(conversion_score),
    .groups = "drop"
  )
interaction_means$discount_framing <- factor(
  interaction_means$discount_framing,
  levels = c("Low", "High")
)

# Interaction plot
interaction_plot <- ggplot(interaction_means, aes(
  x = discount_framing,
  y = mean_conversion,
  color = webpage_design,
  group = webpage_design
)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 4) +
  labs(
    x = "Discount Framing",
    y = "Mean Conversion Score",
    color = "Webpage Design"
  ) +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title = element_text(size = 20),
    legend.title = element_text(size = 20),
    legend.text = element_text(size = 20),
    legend.key.size = unit(1.4, "cm"),
    legend.spacing.x = unit(2, "cm"),
    legend.position = "bottom"
  ) +
  scale_color_manual(
    values = c("D1" = "#E69F00", "D2" = "#56B4E9", "D3" = "#009E73")
  )
interaction_plot
```

### Testing Settings {#sec-two-way-ANOVA-testing-settings}

### Hypothesis Definitions {#sec-two-way-ANOVA-hypothesis-definitions}

### Test Flavour and Components {#sec-two-way-ANOVA-test-flavour}

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-two-fitting-r
#| tbl-cap: "Two-way ANOVA: Conversion score by webpage design and discount framing via testing set."
#| message: false
# Running two-way ANOVA
TwoWay_aov <- aov(conversion_score ~ webpage_design * discount_framing,
  data = TwoWay_ABn_testing
)

# Displaying full two-way ANOVA table
TwoWay_aov <- tidy(TwoWay_aov) |>
  mutate(across(where(is.numeric) & !matches("p.value"), ~ round(.x, 2)))
TwoWay_aov
```

## **`Python`**

```{python}
#| label: tbl-two-fitting-py
#| tbl-cap: "Two-way ANOVA: Conversion score by webpage design and discount framing via testing set."
#| message: false
# Importing R testing set via R library reticulate
TwoWay_ABn_testing = r.TwoWay_ABn_testing

# Running two-way ANOVA
TwoWay_model = ols('conversion_score ~ C(webpage_design) * C(discount_framing)', data=TwoWay_ABn_testing).fit()
TwoWay_aov = sm.stats.anova_lm(TwoWay_model, typ=2)

# Round all numeric columns except p-values
cols_to_round = [c for c in TwoWay_aov.columns if c != "PR(>F)"]
TwoWay_aov[cols_to_round] = TwoWay_aov[cols_to_round].round(2)

# Displaying full two-way ANOVA table
TwoWay_aov
```

:::

### Inferential Conclusions {#sec-two-way-ANOVA-inferential-conclusions}

```{r}
#| label: fig-q-q-plot-two-way
#| fig-cap: "Diagnostic Q-Q plot for two-way ANOVA of conversion score by webpage design and discount framing via residuals."
#| warning: false
#| echo: false
#| message: false
#| fig.height: 8
#| fig.width: 14

# Extract residuals from model
TwoWay_aov <- aov(conversion_score ~ webpage_design * discount_framing,
  data = TwoWay_ABn_testing
)
TwoWay_resid <- augment(TwoWay_aov)

# Q–Q plot for two-way ANOVA
qqplot_twoway <- ggplot(TwoWay_resid, aes(sample = .resid)) +
  stat_qq(size = 2, color = "#6A3D9A", alpha = 0.6) +
  stat_qq_line(color = "black", linewidth = 1) +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
  ) +
  theme_bw(base_size = 14) +
  theme(
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20),
    axis.text = element_text(size = 15.5)
  )
qqplot_twoway
```

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-two-shapiro-wilks-r
#| tbl-cap: "Shapiro–Wilks normality test for two-way ANOVA via residuals."
#| message: false
# Extracting residuals
TwoWay_resid <- residuals(TwoWay_aov)

# Running Shapiro–Wilk test
TwoWay_shapiro <- shapiro.test(TwoWay_resid)

# Displaying outputs
tidy(TwoWay_shapiro) |>
  mutate_if(is.numeric, round, 2)
```

## **`Python`**

```{python}
#| label: tbl-two-shapiro-wilks-py
#| tbl-cap: "Shapiro–Wilks normality test for two-way ANOVA via residuals."
#| message: false
# Extracting residuals
TwoWay_resid = TwoWay_model.resid

# Running Shapiro–Wilk tests
TwoWay_shapiro = shapiro(TwoWay_resid)

# Displaying outputs
TwoWay_shapiro = pd.DataFrame({
    "W Statistic": [round(TwoWay_shapiro.statistic, 2)],
    "p-value": [round(TwoWay_shapiro.pvalue, 2)]
})
TwoWay_shapiro
```

:::

::: {.panel-tabset}

## **`R`**

```{r}
#| label: tbl-two-levene-r
#| tbl-cap: "Levene’s test for homogeneity of variances for two-way ANOVA via testing data."
#| message: false
# Loading library
library(car)

# Running Levene test
TwoWay_levene <- leveneTest(conversion_score ~ as.factor(webpage_design) * as.factor(discount_framing),
  data = TwoWay_ABn_testing
)

# Displaying outputs
tidy(TwoWay_levene) |>
  mutate_if(is.numeric, round, 2)
```

## **`Python`**

```{python}
#| label: tbl-two-levene-py
#| tbl-cap: "Levene’s test for homogeneity of variances for two-way ANOVA via testing data."
#| message: false
# Running Levene test
TwoWay_ABn_testing["group"] = TwoWay_ABn_testing["webpage_design"].astype(str) + "_" + TwoWay_ABn_testing["discount_framing"].astype(str)
groups_TwoWay = [group["conversion_score"].values for _, group in TwoWay_ABn_testing.groupby("group")]
stat1, p1 = levene(*groups_TwoWay)

# Displaying outputs
TwoWay_levene = pd.DataFrame({
    "Levene Statistic": [round(stat1, 2)],
    "p-value": [round(p1, 2)]
})
TwoWay_levene
```

:::

### Storytelling {#sec-two-way-ANOVA-storytelling}

## Chapter Summary {#sec-chapter-4-summary}
