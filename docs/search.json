[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Mini Test Book (in development)",
    "section": "",
    "text": "Preface\n\nHave you ever felt overwhelmed by the numerous fundamental hypothesis tests you need to learn in statistical inference courses?\n\nWe have experienced this sense of overwhelm throughout our academic journeys as well. However, we also understand that statistical inference is a powerful tool for gaining insights into complex populations across various fields of study. Whether analyzing electoral preferences in political science or assessing the effectiveness of innovative medical treatments in randomized clinical trials, the applications are extensive. Hence, in response to these challenges, we have created this mini-book as a handy resource to help structure and simplify the learning of different fundamental hypothesis tests. Our goal is to present these concepts in a reader-friendly manner while clearly explaining the necessary statistical jargon, making these inferential methods accessible to a broader audience.\n\n\n\nImage by manfredsteger via Pixabay.\n\n\nNote that, after conducting extensive research into the available educational literature, we discovered that there is no comprehensive and frequentist resource that explains various inferential methods simultaneously using two essential programming languages in the field of data science: R (R Core Team 2024) and Python(Van Rossum and Drake 2009). Furthermore, we could not find reproducible and transparent tools that would enable learners to implement and adapt these methods in their own computational environments. Based on our teaching experience, these shortcomings hinder effective learning in the practice of statistical inference, especially given the numerous tests required to achieve mastery.\nTo address this gap, we have developed a bilingual and frequentist resource in both R and Python, which features a common test workflow consisting of eight distinct stages applicable to each hypothesis test: study design, data collection and wrangling, exploratory data analysis, testing settings, hypothesis definitions, test flavour and components, inferential conclusions, and storytelling. Additionally, all the tests we discuss are organized through different mind maps to help readers visualize their learning process. Finally, by offering this mini-book as an Open Educational Resource (OER) in Quarto via a GitHub repository, we aim to inspire and empower academic communities worldwide to share and adapt this knowledge to suit their specific needs.\n\n\nLicense\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/meet-the-authors.html",
    "href": "book/meet-the-authors.html",
    "title": "Meet the Authors",
    "section": "",
    "text": "G. Alexi Rodríguez-Arelis\nThe Mini Test Book is a collaborative project created by three educators and researchers from the University of British Columbia (UBC). This open educational resource integrates their expertise in statistic, bioinformatics, and data science pedagogy to make concepts related to hypothesis testing and model evaluation more accessible to learners. Each author provides a unique perspective, encompassing areas such as teaching, curriculum design, applied machine learning, and bioinformatics research. They are united by a common commitment to clarity, reproducibility, and open science. The following sections will introduce the authors and outline their academic and professional backgrounds.",
    "crumbs": [
      "Meet the Authors"
    ]
  },
  {
    "objectID": "book/meet-the-authors.html#g.-alexi-rodríguez-arelis",
    "href": "book/meet-the-authors.html#g.-alexi-rodríguez-arelis",
    "title": "Meet the Authors",
    "section": "",
    "text": "I’m an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia (UBC). Throughout my academic and professional journey, I’ve been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I’m incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Meet the Authors"
    ]
  },
  {
    "objectID": "book/meet-the-authors.html#ekaterina-kate-manskaia",
    "href": "book/meet-the-authors.html#ekaterina-kate-manskaia",
    "title": "Meet the Authors",
    "section": "Ekaterina (Kate) Manskaia",
    "text": "Ekaterina (Kate) Manskaia\n\n\n\n\n\nI’m a multidisciplinary professional with a blend of expertise in bioinformatics, computational chemistry, and business analysis. Currently, I’m pursuing a Doctoral degree in Bioinformatics at the University of British Columbia (UBC), where my research focuses on leveraging generative AI and machine learning to uncover new drug candidates for cancer and infectious diseases. I combine these technologies with biostatistical analysis to validate hypotheses and ensure meaningful insights. Alongside my research, I’ve led projects in computational chemistry, managed complex data systems, and mentored students in Python programming, machine learning, and data science. For the past few years, I’ve also been working as a Teaching Assistant in UBC’s Master of Data Science program.",
    "crumbs": [
      "Meet the Authors"
    ]
  },
  {
    "objectID": "book/meet-the-authors.html#payman-nickchi",
    "href": "book/meet-the-authors.html#payman-nickchi",
    "title": "Meet the Authors",
    "section": "Payman Nickchi",
    "text": "Payman Nickchi\n\n\n\n\n\nI am a Postdoctoral Research and Teaching Fellow in the Department of Statistics and the Master of Data Science (MDS) program at the University of British Columbia (UBC). I completed my PhD in Statistics at Simon Fraser University (SFU), where my research focused on biostatistics and goodness-of-fit tests using empirical distribution functions. I am currently teaching statistical courses in the MDS program at UBC. My passion for statistics, teaching, and data science led me to this role. Outside of work, I enjoy swimming and capturing the night sky through astrophotography.",
    "crumbs": [
      "Meet the Authors"
    ]
  },
  {
    "objectID": "book/privacy-policy.html",
    "href": "book/privacy-policy.html",
    "title": "Website Privacy Policy",
    "section": "",
    "text": "Information Collection and Use\nYour privacy is important to us. This policy outlines how this online textbook created for courses at the University of British Columbia (UBC) (“we,” “us,” or “our”) collects, uses, and protects your information.\nWe use Google Analytics, a web analytics service provided by Google, LLC. (“Google”). Google Analytics uses cookies to help analyze how students interact with the textbook, including tracking which sections are accessed most frequently. Information generated by cookies about your use of our website (including IP address) will be transmitted to and stored by Google on servers in the United States.\nGoogle will use this information solely for evaluating textbook usage, compiling usage reports to enhance the educational effectiveness of the textbook, and providing related services.\nYou may refuse the use of cookies by selecting the appropriate settings in your browser; however, please note this may affect your textbook browsing experience.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/privacy-policy.html#personal-information",
    "href": "book/privacy-policy.html#personal-information",
    "title": "Website Privacy Policy",
    "section": "Personal Information",
    "text": "Personal Information\nWe do not collect personally identifiable information through Google Analytics. Any personally identifiable information, such as your name and email address, would only be collected if voluntarily submitted for specific educational purposes (e.g., feedback or course-related inquiries). We will never sell or distribute your personal information to third parties.\nFor any questions or concerns, please contact us at alexrod@stat.ubc.ca.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/chapter1-intro.html",
    "href": "book/chapter1-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The Test Workflow\nJohn W. Tukey (1962, 13)\nData collection worldwide has proven to be a valuable tool for uncovering significant insights across various populations of interest. Whether it involves capturing political preferences in a specific demographic ahead of an upcoming election or assessing the effectiveness of an innovative medical treatment through a randomized clinical trial compared to a standard treatment, data plays a crucial role in enhancing our understanding. At times, this understanding can become quite complex, especially when attempting to untangle the relationships between different variables within a given population or even across two or more populations.\nIn a vast and diverse field like data science, it is crucial to craft effective and transparent solutions that facilitate proper data analysis. However, conducting a full census to collect data from an entire population can often be impractical due to resource limitations such as budget constraints, workforce shortages, or insufficient technical infrastructure. Despite these challenges, our primary objective remains to gain insights about any population of interest via some class of analysis, even when data availability is limited. In this regard, statistical inference is a powerful tool that allows us to draw insights even with limited data. That said, it is important to emphasize that the process of statistical inference begins with asking the right questions, even before data collection occurs.\nIn light of this context, we need to establish the appropriate stages of the statistical inference process, along with a useful tool to help select the right hypothesis test based on our specific context, research questions, variable types, and parameters of interest. This is why this mini-book focuses on two key components:\nThis mini-book on hypothesis testing is intended to serve as a practical manual rather than a traditional statistical textbook. Furthermore, it focuses on providing applied examples in each chapter without any additional exercises for the reader. We aim to explain the necessary mathematical formulas in straightforward language, avoiding formal proofs for these expressions. Additionally, we will establish conventions using admonitions to offer key insights and links to supplementary and more in-depth material.\nThe statement above summarizes the essence of our testing workflow, which requires a detailed examination in this section. Primarily, it is crucial to understand that mastering all hypothesis tests involves more than just knowing their mathematical formulas or coding functions; it requires a disciplined and structured process. Whether we are evaluating evidence against a null hypothesis—the status quo of our population parameter(s) of interest—or reporting the uncertainty of an estimated effect, the workflow outlined by Figure 1.1 is intended to align your main inferential inquiries with the most suitable test flavour. Regardless of the flavour chosen, this workflow is designed to ensure that our conclusions are not only statistically valid but also based on clear and purposeful reasoning.\nThe workflow for hypothesis testing consists of eight stages, which will be discussed in detail in the following sections:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/chapter1-intro.html#sec-test-workflow",
    "href": "book/chapter1-intro.html#sec-test-workflow",
    "title": "1  Introduction",
    "section": "",
    "text": "There is a single test workflow for many different flavours!\n\n\n\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\nStudy design: This initial stage, referred to as the main inferential inquiries, outlines the primary questions we aim to answer through our analysis.\nData collection and wrangling: The inquiries established in the first stage will guide the design of our data collection, utilizing a specific sampling scheme. Once the data is collected, it must be wrangled and split into two sets: training and test.\nExploratory data analysis: In this stage, we classify variables to provide preliminary insights using descriptive statistics and visualizations via the training set.\nTesting settings: We must revisit the significance level used in our power analysis (i.e., the procedure used to obtain the minimum sample size \\(n\\) of data points to be collected). Additionally, we need to list all modelling parameters that will be tested.\nHypothesis definitions: With the modelling parameters to test, we need to define our hypotheses: the null hypothesis versus the alternative hypothesis. These should be framed in relation to the main inferential inquiries.\nTest flavour and components: At this stage, we choose the most appropriate test flavour and indicate the respective assumptions. Depending on whether the test is classical or simulation-based, we will then identify the necessary components to compute the critical values or \\(p\\)-values (via the test set) for the next stage.\nInferential conclusions: The goal of this stage is to determine whether we should reject the null hypothesis based on the critical values or \\(p\\)-values obtained. This stage also includes running the model diagnostics to check our corresponding assumptions.\nStorytelling: Finally, communicate the findings through a clear and engaging narrative that is accessible to your stakeholders.\n\n\n\n\n\n\n\nFigure 1.1: A hypothesis testing workflow structured in eight stages: study design, data collection and wrangling, exploratory data analysis, testing settings, hypothesis definitions, test flavour and components, inferential conclusions, and storytelling.\n\n\n\n\n1.1.1 Study Design\nThis is the initial stage of the hypothesis testing workflow, which involves what we refer to as main inferential inquiries. These inquiries are typically posed by stakeholders who wish to conduct a study to better understand a specific population of interest and its associated parameters. In practice, these parameters (such as a population mean or variance) are unknown but considered fixed. This approach, where population parameters are treated as fixed yet unknown, corresponds to the frequentist paradigm.\n\n\nHeads-up on the frequentist paradigm!\n\n\nIn the frequentist paradigm, statistical inference relies on the concept that probabilities represent long-run relative frequencies of events observed through repeated experimentation or observation. In this approach, we estimate population parameters by examining the distribution of outcomes derived from multiple independent realizations of a random process.\n\n\n\nImage by Manfred Stege via Pixabay.\n\n\nAdditionally, this paradigm assumes that the parameters governing a population are fixed but unknown. As a result, all randomness is attributed to the data-generating process, not to the parameters themselves.\n\n\nIt is essential to clearly define the main inferential inquiries based on the following principles:\n\nWe need to consult stakeholders about what the study aims to understand regarding their population of interest before entering the second workflow stage, which involves data collection through sampling.\nThe main inferential inquiries should align with the stakeholders’ research questions. These inquiries should be established at the beginning of this workflow stage and must be meaningful and comprehensive enough to guide the entire inferential investigation.\n\n\n\n1.1.2 Data Collection and Wrangling\nOnce the inferential questions are defined, the next step is to collect and prepare the data for analysis. This stage encompasses sampling strategies to ensure the data is representative of the population and data wrangling to clean and structure the dataset appropriately.\n\n\nTip on sampling techniques!\n\n\nIt is important to emphasize the need to choose the most suitable sampling technique based on the population’s structure and the research questions guiding our main inferential inquiries. Making the right choice of sampling technique is essential for ensuring that our inferential results are accurate, precise, and generalizable to the population of interest. Here are some fundamental (though not exhaustive) sampling techniques:\n\nSimple random sampling: Every individual in the population has an equal probability of being sampled. This is the most basic sampling technique and is probabilistically straightforward, but it may be too simplistic for complex populations in practice.\nSystematic sampling: If we have a complete list of individuals in our population, we can sample at regular intervals after selecting a random starting point.\nStratified sampling: The population is divided into distinct groups called strata. These strata are defined in function of the characteristics of the individuals (e.g., age, income, education, etc.). Data is then sampled proportionally from each stratum or through optimal allocation.\nCluster sampling: The population is divided into groups known as clusters, such as households or geographic areas. A random sample is then collected from these clusters.\n\nDuring the stage of data collection and wrangling of our hypothesis testing workflow, it is crucial to dedicate adequate resources to plan and execute data collection using the most suitable sampling technique. Since the scope of this mini-book does not cover sampling in depth, we recommend reviewing the work by Lohr (2021) for more detailed information on various sampling techniques. This resource includes handy practical examples on this vast field.\n\n\nWhen it comes to the wrangling aspect of this stage, once we have sampled our data, it is necessary to structure it in a suitable format (e.g., a proper data frame) using our chosen language, such as the R {tidyverse} (Wickham et al. 2019) or Python {pandas} (The Pandas Development Team 2024).\n\n\nHeads-up on coding tabs!\n\n\nThis mini-book is designed to be “bilingual,” meaning that all hands-on coding can be done in either R or Python. For each specific example presented in any chapter, you will find two tabs: one for R and one for Python. We will first display the input code, followed by the corresponding output.\n\n\n\nImage by Manfred Stege via Pixabay.\n\n\nWith this format, you can tailor your coding journey based on your language preferences and interests as you advance through the mini-book.\n\n\nAfter we have wrangled our data, we need to split it into two sets:\n\nTraining set. This set is used solely for exploratory data analysis (EDA) and allows us to gain graphical and descriptive insights into how the sample of individuals behaves concerning our main inferential inquiries.\nTest set. This set is reserved for our chosen hypothesis testing to be used.\n\nThe data splitting is analogous to the standard practice in machine learning to split our data for model training and testing to prevent data leakage in predictive inquiries.\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nNevertheless, you might wonder:\n\nWhy are we also doing this for an inferential inquiry?\n\nStatistically speaking, the practice of data splitting helps avoid what is known as double dipping. Double dipping occurs when the same data is used both for EDA to generate hypotheses and then again for formal statistical testing. Supported by numerical simulations, it can be demonstrated that double-dipping increases the probability of committing a Type I error, which occurs when we incorrectly reject the null hypothesis \\(H_0\\) while it is actually true for the population of interest.\nFor example, consider a one-sample \\(t\\)-test in a double-dipping context. We might be tempted to formulate our null and alternative hypotheses based on our observed sample mean. For instance, we could state our hypotheses as follows:\n\\[\\text{$H_0$: } \\mu \\geq 10\\]\nversus\n\\[\\text{$H_1$: } \\mu &lt; 10,\\] based on a sample mean of \\(\\bar{x} = 9.5\\). If we were to proceed with the statistical test using this same data, we would be falling into the double-dipping trap!\n\n\nTip on a further double-dipping resource!\n\n\nData splitting is generally not a common practice in statistical inference, despite its frequent use in machine learning. Hence, for more information on double-dipping in statistical inference, Chapter 6 from Reinhart (2015) offers in-depth insights and practical examples.\n\n\n\n\n1.1.3 Exploratory Data Analysis\nOnce the data is cleaned and structured, it is essential to develop a descriptive understanding of our variables of interest through EDA using the training set. The first step is to classify the variables (e.g., numerical, binary, categorical, ordinal, etc.), which will guide us in selecting the most appropriate descriptive statistics and visualizations to examine the relationships between these variables. For instance, we can explore the distribution of these variables and identify any outliers present in the training set. Note EDA is intended to uncover preliminary trends before conducting formal inferential analysis, and these findings should be communicated to our stakeholders during the final storytelling.\n\n\n\nImage by Manfred Stege via Pixabay.\n\n\nAdditionally, the classification of variables during EDA will provide valuable insights for formulating and setting up our hypotheses, while also helping us choose the most suitable test flavour. The insights gained from EDA, along with the identified preliminary trends, will shape our expectations for the entire workflow and facilitate a more nuanced statistical interpretation of the main inferential inquiries. Furthermore, EDA aids in justifying our modelling assumptions later in the process. Finally, we must clarify that any insights gained from EDA cannot be generalized to the entire population; they pertain only to the sampled data within the training set.\n\n\n1.1.4 Testing Settings\nThis stage allows us to define all our population parameters based on the main inferential inquiries, along with the standards that will guide us through the subsequent stages of the workflow:\n\nWe need to use the same significance level, denoted as \\(\\alpha\\), that was employed in the power analysis when planning data collection for our sampling technique. The significance level denotes the probability of committing Type I error as in Table 1.1 (i.e., the probability of encountering a false positive).\nRegarding the population parameters of interest, we should begin with a formal statistical definition using Greek letters (for further information, see Appendix A).\n\n\n\n\nTable 1.1: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect (True positive)\n\n\nFail to reject \\(H_0\\)\nCorrect (True negative)\nType II error (False negative)\n\n\n\n\n\n\n\n\nHeads-up on power analysis!\n\n\nIn hypothesis testing, power analysis is a crucial preliminary step used to determine the minimum sample size \\(n\\) necessary to detect a signal that allows us to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\). This analysis ensures that our inferential process can effectively distinguish true population effects from random noise. Note that power analysis requires three key components as inputs:\n\nThe significance level \\(\\alpha\\).\nThe desired power \\(1 - \\beta\\) (which relates to correctly rejecting \\(H_0\\) in favour of \\(H_1\\), resulting in a true positive as in Table 1.1).\nThe effect size—a measure of the magnitude of the association (or causation) that the test is designed to detect.\n\nThese three components allow power analysis to provide the minimum sample size \\(n\\) needed to avoid an underpowered study (which occurs when there is a high probability of committing a Type II error as in Table 1.1, denoted as \\(\\beta\\)) or an overly large \\(n\\) that could waste resources.\n\n\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nTo effectively communicate our insights to stakeholders via our final storytelling, it is necessary to translate all modelling parameters and hypotheses into clear, plain language for those who may not have a technical background. We should remember that \\(H_0\\) must be stated in a way that indicates a status quo in any given parameter(s), meaning there is nothing noteworthy in the context of our inferential study. On the other hand, \\(H_1\\) must imply a departure from this status quo, indicating that there is indeed something of interest to consider in our inferential analysis.\n\n\n1.1.5 Hypothesis Definitions\nWith the settings clarified, the next step is to explicitly define the null and alternative hypotheses. The null hypothesis, \\(H_0\\), typically asserts that there is no effect or difference—this serves as the default assumption to be tested. As mentioned, \\(H_0\\) represents the status quo in this context. In contrast, the alternative hypothesis, \\(H_1\\), indicates the presence of an effect or difference that the data scientist aims to detect. These definitions are derived directly from the main inferential inquiries and insights gained from the EDA using the training set.\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nIt is crucial to emphasize that clarity in hypothesis definitions is vital for selecting the appropriate type of test and accurately interpreting its results. The formulation of \\(H_0\\) and \\(H_1\\) establishes the logical framework for the subsequent analysis. For instance, a null hypothesis might state that the mean test scores for two groups are equal, while the alternative hypothesis claims that they differ. These hypotheses must be mutually exclusive to support a valid statistical decision.\n\n\n1.1.6 Test Flavour and Components\nAfter specifying the hypotheses, the next step is to choose a suitable statistical test and compute its components. The choice of test—referred to here as the flavour—depends on the data structure and the underlying modelling assumptions. We must decide whether to use a classical test, which relies on theoretical distributions (e.g., \\(t\\)-test, \\(z\\)-test, chi-squared test, etc.), or a simulation-based method, which employs resampling techniques to empirically estimate the null distribution.\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nNext, we calculate the observed effect derived from the previously untouched test set. For classical tests, the test statistic (calculated using the observed effect and its measure of uncertainty, the standard error) is compared against a theoretical null distribution (i.e., the distribution under the null hypothesis, \\(H_0\\)). In the case of simulation-based tests, the observed effect is compared to a null distribution that is generated from repeated permutations or resamplings. This stage provides the statistical framework necessary to evaluate the strength of evidence against the null hypothesis.\n\n\n1.1.7 Inferential Conclusions\nThis stage involves converting the test components obtained from the previous stage into numerical outputs that will allow us to draw meaningful conclusions about our main inferential inquiries. Depending on the type of hypothesis testing, we can obtain the following numerical outputs to support these conclusions:\n\nCritical value: In a classical hypothesis test, the critical value serves as a threshold under the null theoretical distribution and is obtained via our previously set up significance level \\(\\alpha\\). This value helps determine whether we can reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\). We directly compare our observed test statistic against this threshold. If the test statistic exceeds the critical value, we reject \\(H_0\\) in favour of \\(H_1\\); if it does not, we fail to reject \\(H_0\\). This approach emphasizes the magnitude of the observed effect relative to the null distribution.\n\\(p\\)-value: This approach can be employed in either a classical test or a simulation-based test. In the case of a classical test, the \\(p\\)-value is derived from the observed test statistic under the theoretical null distribution. Conversely, when using a simulation-based test, the \\(p\\)-value is associated with the observed effect under the empirical null distribution. Regardless of the type of test used, we compare the \\(p\\)-value to our predetermined significance level \\(\\alpha\\). If the \\(p\\)-value is smaller than \\(\\alpha\\), we reject \\(H_0\\) in favour of \\(H_1\\); otherwise, we fail to reject \\(H_0\\). This process provides a probabilistic measure of how surprising the data are under the null distribution (i.e., the status quo).\n\n\n\nHeads-up on the practical use of critical and \\(p\\)-values!\n\n\nNote that the interpretation of critical and \\(p\\)-values must go beyond binary decision-making (statistically speaking); it should also involve the direction, magnitude, and practical implications of the estimated effects, especially in applied contexts. Therefore, we must pay attention not only to statistical significance but also to whether the observed differences or associations are substantively significant in an applied context. This refers to practical significance.\nSince we are working within a frequentist framework, it is important to remember that critical and \\(p\\)-values are sensitive to sample sizes. In large samples, very small effects can appear statistically significant, even if they have little practical significance. On the other hand, important effects may be overlooked in studies that lack sufficient statistical power. Understanding these nuances helps us avoid misinterpretation and overgeneralization, especially when communicating results to non-statistical stakeholders. Therefore, careful interpretation is essential to bridge the gap between mathematical output and real-world insights.\n\n\n\n\n\nImage by Manfred Steger via [Pixabay](https://pixabay.com/vectors/pixel-cells-pixel-digital-3704070/.\n\n\nThe final step in this stage involves model diagnostics. These diagnostics are conducted to verify that the assumptions of data modelling are met (such as normality, independence, homoscedasticity, etc.). If these assumptions are violated, we must reconsider the validity of our inferential conclusions. This process helps prevent drawing incorrect conclusions due to mis-specified models or flawed data structures. If any assumptions are not satisfied, we should return to the previous stage, select a different type of test, and proceed accordingly.\n\n\n1.1.8 Storytelling\nThis final stage involves translating our statistical results into language and formats that are accessible primarily to our stakeholders who posed the main inferential questions. The statistical literacy of these stakeholders can vary; they may include research fellows, corporate leadership, policymakers, or even the general public. The ultimate goal of this stage is to craft a data story that either supports or questions a hypothesis based on our study findings. Clear communication should incorporate key EDA and inferential results.\nEffective storytelling must begin with a succinct statement that encompasses the main inferential inquiries, outlines our null and alternative hypotheses in plain language, specifies the chosen significance level (with a tailored explanation for our specific stakeholders), explains why we selected our particular test flavour (i.e., the rationale behind our data modelling assumptions), and reports uncertainty quantification. This transparent listing of all our inferential elements allows stakeholders to assess the reliability of our analysis while gaining a fair understanding of any study limitations.\n\n\n\nImage by Manfred Stege via Pixabay.\n\n\nIt is important to clarify the difference between statistical significance and practical significance in our storytelling when presenting significant results. Conversely, when discussing non-significant results, we should frame them in a way that indicates the sampled data and study design did not provide enough evidence against the status quo represented by the null hypothesis. This lack of evidence, in light of a non-significant result, is an excellent opportunity to discuss potential issues such as an underpowered study, limitations in sample size, or bias in data collection.\n\n\nHeads-up on uncertainty quantification!\n\n\nWhen presenting our findings to stakeholders, it is essential to include uncertainty quantification as a key component of our storytelling. This process incorporates the uncertainty associated with our point estimates of observed effects, as these estimates are derived from random sampling of our population of interest. We express this uncertainty through confidence intervals, which show a range of plausible values where our model parameters may lie (or may not!). The information conveyed by these intervals includes:\n\nA narrow confidence interval around a parameter estimate indicates larger precision in the estimation. For stakeholders, this signifies a more reliable estimate, likely resulting from a sufficiently large sample size, low variability in the sample data or well-specified data modelling.\nA wide confidence interval around a parameter estimate indicates lower precision in the estimation. For stakeholders, this represents a less reliable estimate, which may stem from a small sample size, high variability in the sampled data or mis-specification of the data modelling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/chapter1-intro.html#sec-test-mind-map",
    "href": "book/chapter1-intro.html#sec-test-mind-map",
    "title": "1  Introduction",
    "section": "1.2 The Test Mind Map",
    "text": "1.2 The Test Mind Map\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nFigure 1.2 outlines the conceptual and organizational structure of this mini-book through its corresponding chapters. This mind map for frequentist hypothesis testings is divided into two main branches: classical and simulation-based tests. The classical tests are further categorized based on the number of groups being compared (one, two, or \\(k\\) groups), the nature of the variable of interest (unbounded continuous data or proportions derived from binary outcomes), and whether the measurements are independent or related:\n\nChapter 2 focuses on hypothesis tests applied to a single population mean across two different types of responses. For unbounded responses, traditional tests such as the one-sample \\(t\\)-test are introduced. For binary responses transformed into proportions (e.g., the fraction of success in a Bernoulli trial), the chapter covers tests for one population proportion, including the \\(z\\)-test for proportions.\nChapter 3 extends the single-group approach to comparisons between two groups, concentrating on both independent and related populations. For two independent populations with unbounded responses, it discusses the two-sample \\(t\\)-test. For binary outcomes between two groups, the chapter explains inference on two proportions using methods like the \\(z\\)-test for two proportions. It also addresses related populations (e.g., pre/post measurements or matched pairs) by introducing the paired-sample \\(t\\)-test, highlighting how dependency affects the testing framework.\nChapter 4 generalizes the two-group comparison to \\(k\\) groups using analysis of variance (ANOVA) techniques. This chapter focuses on continuous, unbounded response variables and explains how ANOVA partitions total variation to detect mean differences across groups.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Frequentist\n  Hypothesis \n  Testings\n  ))\n    Simulation Based&lt;br/&gt;Tests\n    Classical&lt;br/&gt;Tests\n      (Chapter 2: &lt;br/&gt;Tests for One&lt;br/&gt;Continuous&lt;br/&gt;Population Mean)\n        {{Unbounded&lt;br/&gt;Response}}\n        {{Proportion between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from a &lt;br/&gt;Binary Response}}\n      (Chapter 3: &lt;br/&gt;Tests for Two&lt;br/&gt;Continuous&lt;br/&gt;Population Means)\n        Two&lt;br/&gt;Independent&lt;br/&gt;Populations\n          {{Unbounded&lt;br/&gt;Responses}}\n          {{Proportions between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from two &lt;br/&gt;Binary Responses}}\n        Two&lt;br/&gt;Related&lt;br/&gt;Populations or&lt;br/&gt;Measurements\n          {{Unbounded&lt;br/&gt;Responses}}\n      (Chapter 4: ANOVA related &lt;br/&gt;Tests for&lt;br/&gt;k Continuous&lt;br/&gt;Population Means)\n        {{Unbounded&lt;br/&gt;Responses}}\n\n\n\n\n\n\n\n\nFigure 1.2: A general hypothesis testing mind map outlining all techniques explored in this book. Depending on the overall approach to be used, these techniques are divided into two broad categories: classical and simulation-based tests.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/chapter1-intro.html#sec-chapter-1-summary",
    "href": "book/chapter1-intro.html#sec-chapter-1-summary",
    "title": "1  Introduction",
    "section": "1.3 Chapter Summary",
    "text": "1.3 Chapter Summary\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nThis opening chapter of the mini-book on hypothesis testing introduces the foundational motivations, principles, and practical frameworks that underlie the frequentist approach to statistical inference. We begin by emphasizing hypothesis testing as a central inferential tool in data science and research, which enables practitioners to draw population-level conclusions based on finite sample evidence. Additionally, we revisit the frequentist paradigm, which highlights the logic of repeated sampling and the fixed nature of population parameters.\nWe also outline the practical components that structure formal hypothesis testing through a comprehensive workflow. This eight-stage framework encompasses everything from study design and data collection to communicating inferential conclusions. Each stage is briefly introduced as a foundation for the upcoming chapters, illustrating how test results are closely linked to modelling assumptions, data preparation strategies, and communication objectives.\n\n\n\n\nLohr, S. L. 2021. Sampling: Design and Analysis. Chapman; Hall/CRC. https://doi.org/https://doi.org/10.1201/9780429298899.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete Guide. 1st ed. San Francisco, CA: No Starch Press. https://www.statisticsdonewrong.com/index.html.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas: Pandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/chapter2-one-pop.html",
    "href": "book/chapter2-one-pop.html",
    "title": "2  Tests for One Continuous Population Mean",
    "section": "",
    "text": "2.1 One-sample t-test for the mean\nThis chapter introduces statistical tests designed to analyze a single sample, which is a fundamental task in data analysis across many disciplines. Whether you’re evaluating whether the average recovery time from a treatment differs from a known standard, assessing whether student test scores exceed a benchmark, or testing if the proportion of success in a group differs from an expected rate, these methods help determine whether the observed values are statistically significant or simply due to chance.\nThere are several statistical tests used to evaluate hypotheses about a single sample. The appropriate test depends on the type of variable (mean or proportion), sample size, and whether population parameters like variance are known.\nWe test whether a population mean equals a specific value. The right test depends on:\nIn this chapter, we focus on statistical tests used to evaluate hypotheses about a single population mean or proportion, based on sample data. These tests help determine whether a sample provides sufficient evidence to conclude that the population mean (or proportion) differs from a specified value.\nWe cover two cases for the mean — depending on whether the population variance is known or unknown — and one test for binary outcomes where we’re testing a population proportion.\nKey tests include:\nUse this test when: - The population variance is unknown, and - The sample is either normally distributed or large enough to rely on the central limit theorem.\nImagine you want to assess whether a new method of teaching introductory physics improves student performance compared to the traditional method previously used. To explore this, you test the new method at the University of British Columbia (UBC) and compare the results to historical data from students who were taught using the traditional approach. This historical data serves as your reference value.\nSuppose the population has an unknown average physics score, denoted as:\n\\[\n\\mu \\quad \\text{(mean physics score at UBC)}\n\\]\nSince we do not have access to the grades of all students, we take a random sample from the population. Let this sample consist of \\(n\\) students, with observed scores:\n\\[\nX_1, X_2, \\dots, X_n\n\\]\nThe central question becomes:\nIf, for example, the historical average physics score is known to be 75, then our question becomes more specific:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tests for One Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter2-one-pop.html#one-sample-t-test-for-the-mean",
    "href": "book/chapter2-one-pop.html#one-sample-t-test-for-the-mean",
    "title": "2  Tests for One Continuous Population Mean",
    "section": "",
    "text": "Is the mean physics score in our sample statistically different from a given reference value?\n\n\n\nIs the mean physics score in the sample statistically different from 75?\n\n\n2.1.1 Hypotheses\nWe can formally express this with the following hypotheses:\n\n\nNull hypothesis \\(H_0\\): \\(\\mu = 75\\)\n\n\nAlternative hypothesis \\(H_1\\): \\(\\mu \\ne 75\\)\n\n\nUnder the null hypothesis, we assume that the average score under the new method is equal to the historical average of 75. If the null is rejected, we conclude that there is a statistically significant difference, suggesting that the new method may lead to either higher or lower average performance.\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nWhere: - ( s ) is the sample standard deviation (used instead of ( \\(\\sigma\\) ))\nThis statistic follows a t-distribution with ( n - 1 ) degrees of freedom.\n\n2.1.2 Study Design\nIn this example we use the Palmer Station Penguins dataset collected by the LTER in Antarctica (2007 – 2009).\nThe dataset spans three penguin species and includes continuous variables such as flipper length, bill size, and body mass.\n\nResearch question:Is the average flipper length of penguins significantly different from 200 mm?\n\n\n2.1.3 Data Collection & Wrangling\nWe obtain the dataset Palmer Station Penguins dataset collected by the ‘LTER’\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   4.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(42)\n\npenguins_clean &lt;- penguins %&gt;% drop_na()\n\n# 80/20 split\ntrain_index &lt;- sample(seq_len(nrow(penguins_clean)), \n                      size = 0.8 * nrow(penguins_clean))\ntrain_set &lt;- penguins_clean[train_index, ]\ntest_set &lt;- penguins_clean[-train_index, ]\n\n\n\n\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\n\n# 80/20 train–test split\ntrain_set, test_set = train_test_split(\n    penguins_clean, test_size=0.2, random_state=42\n)\n\n\n\n\n\n2.1.4 Exploratory Data Analysis (EDA)\nBefore conducting the statistical test, we begin with an exploratory analysis to understand the distribution and characteristics of the flipper_length_mm variable.\nFirst, we examine summary statistics such as the mean, standard deviation, and quartiles. This helps us get a sense of the central tendency and spread of the data:\n\n\nR Code\nPython Code\n\n\n\n\nsummary(train_set$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  172.0   190.0   198.0   201.5   214.0   231.0 \n\n\n\n\n\nprint(train_set[\"flipper_length_mm\"].describe())\n\ncount    266.00000\nmean     201.00000\nstd       13.91592\nmin      172.00000\n25%      190.00000\n50%      197.00000\n75%      213.00000\nmax      231.00000\nName: flipper_length_mm, dtype: float64\n\n\n\n\n\nNext, we visualize the distribution of flipper lengths using a histogram. This allows us to assess whether the data are approximately symmetric and whether any outliers are present:\n\nimport matplotlib.pyplot as plt\n\ntrain_set[\"flipper_length_mm\"].hist(edgecolor=\"black\", color=\"skyblue\")\nplt.title(\"Distribution of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\nTo explore the relationship between flipper length and another continuous variable, we create a scatter plot of flipper length versus body mass. This helps us visually assess whether larger penguins tend to have longer flippers, and whether this relationship is linear or varies across ranges:\n\nplt.scatter(\n    train_set[\"body_mass_g\"],\n    train_set[\"flipper_length_mm\"],\n    alpha=0.6\n)\nplt.title(\"Body Mass vs. Flipper Length\")\nplt.xlabel(\"Body Mass (g)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.show()\n\n\n\n\n\n\n\nNow, we can perform one‑Sample t-Test\n\n\nR Code\nPython Code\n\n\n\n\nt_test &lt;- t.test(train_set$flipper_length_mm, mu = 200)\nt_test\n\n\n    One Sample t-test\n\ndata:  train_set$flipper_length_mm\nt = 1.7746, df = 265, p-value = 0.0771\nalternative hypothesis: true mean is not equal to 200\n95 percent confidence interval:\n 199.8316 203.2435\nsample estimates:\nmean of x \n 201.5376 \n\n\n\n\n\nimport scipy.stats as stats\n\nt_stat, p_value = stats.ttest_1samp(\n    train_set[\"flipper_length_mm\"], popmean=200\n)\nprint(f\"t = {t_stat:.3f},  p = {p_value:.4f}\")\n\nt = 1.172,  p = 0.2422\n\n\n\n\n\nA one-sample t-test was conducted to determine whether the average flipper length of penguins is significantly different from 200 mm. Based on a training sample, the test produced a t-statistic of t and a p-value of p.\nGiven a significance level of 0.05, if the p-value is less than 0.05, we reject the null hypothesis and conclude that the average flipper length is significantly different from 200 mm. If not, we do not have sufficient evidence to say it differs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tests for One Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter2-one-pop.html#one-sample-z-test-for-the-mean",
    "href": "book/chapter2-one-pop.html#one-sample-z-test-for-the-mean",
    "title": "2  Tests for One Continuous Population Mean",
    "section": "\n2.2 One-sample z-test for the mean",
    "text": "2.2 One-sample z-test for the mean\nWe use this test when: - The population variance σ² is known, and - The sample comes from a normally distributed population, or the sample size is large (typically ( n )).\nThe test statistic is:\n\\[ z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}} \\]\nWhere: - ( \\(\\bar{x}\\) ) is the sample mean\n- ( \\(\\mu_0\\) ) is the hypothesized population mean\n- ( \\(\\sigma\\) ) is the known population standard deviation\n- ( n ) is the sample size\nWe compare the calculated ( z )-value to a standard normal distribution to compute a p-value or make a decision based on a critical value.\nLet’s revisit the earlier scenario: you want to assess whether a new method of teaching introductory physics at the University of British Columbia (UBC) improves student performance compared to historical results. This time, assume that the population standard deviation is known from previous years of large-scale data collection. This allows us to use a z-test. Suppose the average physics score in the population is: $$\nμ(mean physics score at UBC)\n$$\n\\[\n\\mu \\quad \\text{(mean physics score at UBC)}\n\\] And we collect a random sample of \\(n\\) students, recording their scores:\n\\[\nX1,X2,…,XnX_1, X_2, \\dots, X_n\n\\]\nThe Key question becomes:\n\nIs the sample mean statistically different from a known reference value?\n\nIf, for example, the historical average physics score is known to be 75, then our question becomes more specific:\n\nIs the mean physics score in the sample statistically different from 75?\n\n\n2.2.1 Hypotheses\nWe can formally express this with the following hypotheses:\n\n\nNull hypothesis \\(H_0\\): \\(\\mu = 75\\)\n\n\nAlternative hypothesis \\(H_1\\): \\(\\mu \\ne 75\\)\n\n\nUnder the null hypothesis, we assume that the new teaching method does not significantly change the average score. If the null is rejected, we infer that the method may lead to higher or lower performance.\nThe test statistic is:\n\\[z=xˉ−μ0σ/nz = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\]\nWhere: • \\(\\bar{x}\\) is the sample mean\n• \\(\\mu_0\\) is the reference value (e.g., 75)\n• \\(\\sigma\\) is the known population standard deviation • \\(n\\) is the sample size\nThis statistic follows a standard normal distribution (\\(\\mathcal{N}(0, 1)\\))..\n\n2.2.2 Study Design\nWe again use the Palmer Station Penguins dataset collected by the LTER in Antarctica (2007 – 2009).\nIt includes measurements such as flipper length, bill size, and body mass across three penguin species.\n\nResearch question:Is the average flipper length of penguins significantly different from 200 mm, assuming a known population standard deviation?\n\n\n2.2.3 Data Collection & Wrangling\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(dplyr)\nset.seed(42)\n\npenguins_clean &lt;- penguins %&gt;% drop_na()\ntrain_index &lt;- sample(seq_len(nrow(penguins_clean)), \n                      size = 0.8 * nrow(penguins_clean))\ntrain_set &lt;- penguins_clean[train_index, ]\ntest_set &lt;- penguins_clean[-train_index, ]\n\n\n\n\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins_clean = penguins.dropna()\ntrain_set, test_set = train_test_split(\n    penguins_clean, test_size=0.2, random_state=42\n)\n\n\n\n\n\n2.2.4 Exploratory Data Analysis (EDA)\n\n\nR Code\nPython Code\n\n\n\n\nsummary(train_set$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  172.0   190.0   198.0   201.5   214.0   231.0 \n\n\n\n\n\nprint(train_set[\"flipper_length_mm\"].describe())\n\ncount    266.00000\nmean     201.00000\nstd       13.91592\nmin      172.00000\n25%      190.00000\n50%      197.00000\n75%      213.00000\nmax      231.00000\nName: flipper_length_mm, dtype: float64\n\nimport matplotlib.pyplot as plt\n\ntrain_set[\"flipper_length_mm\"].hist(edgecolor=\"black\", color=\"skyblue\")\nplt.title(\"Distribution of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\nplt.scatter(\n    train_set[\"body_mass_g\"],\n    train_set[\"flipper_length_mm\"],\n    alpha=0.6\n)\nplt.title(\"Body Mass vs. Flipper Length\")\nplt.xlabel(\"Body Mass (g)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tests for One Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter2-one-pop.html#one-sample-z-test-for-proportions",
    "href": "book/chapter2-one-pop.html#one-sample-z-test-for-proportions",
    "title": "2  Tests for One Continuous Population Mean",
    "section": "\n2.3 One-sample z-test for proportions",
    "text": "2.3 One-sample z-test for proportions\nUse this test when: - The variable is binary (success/failure, yes/no, etc.), and - You want to test a population proportion ( p ), using a large enough sample.\nThe test statistic is:\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\\]\nWhere: - ( \\(\\hat{p}\\) ) is the sample proportion\n- ( \\(p_0\\) ) is the hypothesized population proportion\n- ( \\(n\\) ) is the sample size\nJust like the one-sample t-test is used to assess differences in means, the one-sample z-test is used when: • You are working with proportions (e.g., fraction of penguins with a specific characteristic), • The sample size is sufficiently large to justify using the normal approximation. This test is ideal for binary outcomes: for example, whether a penguin is of the Adelie species or not. Suppose you want to test whether the proportion of Adelie penguins in your sample differs from a historically established value. Let’s say historical records suggest that 45% of penguins in a certain region were Adelie. You want to know whether the proportion in your current sample is statistically different from 0.45.\nThe central question becomes: &gt; ** Is the proportion of Adelie penguins in the sample different from 0.45?? We are again working with the Palmer Station Penguins** dataset.\n\n2.3.1 Hypotheses\nWe can formally express this with the following hypotheses:\n\n\nNull hypothesis \\(H_0\\): \\(p = 0.45\\)\n\n\nAlternative hypothesis \\(H_1\\): \\(p \\ne 0.45\\) Where: • \\(p\\) is the true proportion of Adelie penguins in the population. We will estimate \\(p\\) using \\(\\hat{p}\\), the sample proportion of Adelie penguins. The test statistic is:\n\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\\]\nWhere: - ( \\(\\hat{p}\\) ) is the sample proportion\n- ( \\(p_0\\) ) is the hypothesized population proportion\n- ( \\(n\\) ) is the sample size\nThis follows a standard normal distribution (\\(z\\)-distribution) under the null hypothesis.\n\n2.3.2 Study Design\nOnce again, we turn to the Palmer Station Penguins dataset, which includes species information for each penguin observed.\n\nResearch question:\n* Is the proportion of Adelie penguins in the training dataset significantly different from 45%?**\n\n\n2.3.3 Data Collection & Wrangling\nWe continue with the same cleaned and split dataset from the earlier section.\n\n\nR Code\nPython Code\n\n\n\n\ntrain_set$is_adelie &lt;- ifelse(train_set$species == \"Adelie\", 1, 0)\nn &lt;- nrow(train_set)\nnum_adelie &lt;- sum(train_set$is_adelie)\np_hat &lt;- num_adelie / n\ncat(\"Sample size:\", n, \"\\n\")\n\nSample size: 266 \n\ncat(\"Number of Adelie penguins:\", num_adelie, \"\\n\")\n\nNumber of Adelie penguins: 113 \n\ncat(\"Sample proportion:\", round(p_hat, 3), \"\\n\")\n\nSample proportion: 0.425 \n\n\n\n\n\n# Add a binary column: 1 if Adelie, 0 otherwise\ntrain_set[\"is_adelie\"] = (train_set[\"species\"] == \"Adelie\").astype(int)\n# Total number of observations\nn = train_set.shape[0]\n# Number of Adelie penguins\nnum_adelie = train_set[\"is_adelie\"].sum() \n# Sample proportion\np_hat = num_adelie / n\nprint(f\"Sample size: {n}\")\n\nSample size: 266\n\nprint(f\"Number of Adelie penguins: {num_adelie}\")\n\nNumber of Adelie penguins: 115\n\nprint(f\"Sample proportion: {p_hat:.3f}\")\n\nSample proportion: 0.432\n\n\n\n\n\n\n2.3.4 Exploratory Data Analysis (EDA)\nLet’s visualize the distribution of penguin species in our training sample.\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\n\nggplot(train_set, aes(x = species)) +\n  geom_bar(fill = \"orange\", color = \"black\") +\n  labs(title = \"Penguin Species Count in Training Set\",\n       x = \"Species\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\ntrain_set[\"species\"].value_counts().plot(\nkind=\"bar\", color=\"orange\", edgecolor=\"black\"\n)\nplt.title(\"Penguin Species Count in Training Set\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Species\")\nplt.xticks(rotation=0)\n\n(array([0, 1, 2]), [Text(0, 0, 'Adelie'), Text(1, 0, 'Gentoo'), Text(2, 0, 'Chinstrap')])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Performing the One-Sample z-Test\nNow we perform the z-test for one proportion. Assuming a historical reference proportion of 0.45 for Adelie penguins:\n\n\nR Code\nPython Code\n\n\n\n\np0 &lt;- 0.45\nse &lt;- sqrt(p0 * (1 - p0) / n)\nz_stat &lt;- (p_hat - p0) / se\np_value &lt;- 2 * (1 - pnorm(abs(z_stat)))\n\ncat(\"z =\", round(z_stat, 3), \", p =\", round(p_value, 4), \"\\n\")\n\nz = -0.826 , p = 0.4089 \n\n\n\n\n\nimport numpy as np\nimport scipy.stats as stats\n# Reference proportion\np0 = 0.45\n# Sample proportion\np_hat = num_adelie / n\n# Standard error under H0\nse = np.sqrt(p0 * (1 - p0) / n)\n# z-statistic\nz_stat = (p_hat - p0) / se\n# Two-tailed p-value\np_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\nprint(f\"z = {z_stat:.3f}, p = {p_value:.4f}\")\n\nz = -0.579, p = 0.5624\n\n\n\n\n\nA one-sample z-test for proportions was conducted to determine whether the proportion of female penguins differs significantly from 0.45. Based on the sample data, the test produced a z-statistic of z and a p-value of p. Given a significance level of 0.05, if the p-value is less than 0.05, we reject the null hypothesis and conclude that the proportion of female penguins is significantly different from 0.45. Otherwise, we do not have sufficient evidence to say it differs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tests for One Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html",
    "href": "book/chapter3-two-pop.html",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "",
    "text": "3.1 Two sample Student’s t-test for Independent Samples\nThis chapter introduces statistical tests designed to compare two samples which is a fundamental task in data analysis across many disciplines. Whether you’re comparing average recovery times between two medical treatments, student test scores under different teaching methods, comparing the proportion among two samples, or reaction times under varying stress conditions, these methods help determine whether observed differences are statistically significant or simply due to chance.\nIn this chapter, we review tests for comparing two continuous population means under two conditions: when the populations are independent and when they are dependent. Throughout the sections below, we provide details about these tests and required formula for each case. Broadly speaking, there are two main types of tests to compare the means between two continuous populations:\nThe choice of test depends on the structure of your data. This chapter introduces both types of comparisons, beginning with independent samples. Each section includes definitions, theoretical background, and R/Python code examples using real or simulated data sets to help ground the concepts in practice. We also review the theoretical background and example codes to test for proportions in two populations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html#sec-t-test-ind-samples",
    "href": "book/chapter3-two-pop.html#sec-t-test-ind-samples",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "",
    "text": "3.1.1 Review\nIn this section we talk about two sample student’s t-test for independent samples. Independent samples arise when the observations in one group do not influence or relate to the observations in the other. In statistical terms we call this two independent samples. A classic example from educational research is described below:\nSuppose you’re interested in whether a new method of teaching introductory physics improves student performance and learning experience. To investigate this, you decide to test the method at two universities: the University of British Columbia (UBC) and Simon Fraser University (SFU). You apply the new teaching method at SFU and compare the results to students taught with the traditional method at UBC.\nIn this scenario, students at UBC and SFU form two distinct, unrelated groups. Since the students are not paired or matched across schools, and each individual belongs to only one group, the samples are independent. Note that the samples are drawn from two independent population: students at UBC and SFU, respectively.\nLet us assume that each population has an unknown average or mean physics score denoted by:\n\\[\n    \\mu_1 \\quad \\text{(mean for UBC)}, \\quad \\mu_2 \\quad \\text{(mean for SFU)}.\n\\]\nSince we do not have access to all students’ grades, we take a random sample from each school. Suppose:\n\nFrom UBC (Population 1), we obtain a sample of size \\(n\\), denoted as: \\[X_1, X_2, \\ldots, X_n\\]\nFrom SFU (Population 2), we obtain a sample of size \\(m\\), denoted as: \\[Y_1, Y_2, \\ldots, Y_m\\]\n\nNote that the sample sizes \\(n\\) and \\(m\\) do not necessarily have to be equal. Now, the central question becomes:\nIs there a statistically significant difference between the mean physics scores among two groups?\nIn formal terms, we test the hypotheses:\n\\[H_0: \\mu_1 = \\mu_2 \\quad \\text{versus} \\quad H_A: \\mu_1 \\ne \\mu_2\\] Now that we reviewed the test concept, let’s try to understand it in a read data set. The steps below follows closely with the roadmap that we introduced in Chapter 1.\n\n3.1.2 Study design\nFor this example, we will be using Auto data set from ISLR package. This data set contains gas mileage, horsepower, and other information for 392 vehicles. Some of variables of interest are: 1) cylinders an integer (numerical) value between 4 and 8 which indicates the number of cylinders of car, and 2) horsepower which shows engine horsepower. You may wondering if the mean of horsepower in cars with 8 cylinders is statistically different than the means in cars with 4 cylinders?\n\n3.1.3 Data Collection and Wrangling\nTo answer this question, we obtain the data set which is available in ISLR package. Note that we consider this data a random sample from population of cars. First we create a new copy of this data set to avoid touching the actual data (this is optional). Also we filter rows to those cars with 4 or 8 cylinders only.\n\n# Get a copy of data set\nauto_data &lt;- Auto\n\n# Filter rows to cars with 4 or 8 cylinders\nauto_data &lt;- auto_data |&gt;filter(cylinders %in% c(4,8) )\n\nFinally, we randomly create test and train set from this data set. We use a proportion of 50-50 between train and test.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Splitting the data set into train and test sets\ntrain_indices &lt;- sample(seq_len(nrow(auto_data)), size = 0.50 * nrow(auto_data))\ntrain_auto &lt;- auto_data[train_indices, ]\ntest_auto &lt;- auto_data[-train_indices, ]\n\n\n3.1.4 Explanatory Data Analysis\nOnce we have the data and it is split into training and test sets, the next step is to begin exploratory data analysis (EDA) on train set. This step is crucial, as it helps us gain a better understanding of the distribution of variables in our data set. The horsepower variable in data set is a numerical variable. The cylinders variable is an integer variable that helps to divide observations into two groups.\nIn particular, we are interested in the distribution of horsepower in two different groups (cars with 4 cylinders vs cars with 8 cylinders). Using a histogram for this variable is a good choice as we have a variable with numerical values.\n\n\n\n\n\n\n\nFigure 3.2: Side-by-side histogram of horsepower by number of cylinders for train data.\n\n\n\n\nWe also look at some descriptive statistics of horsepower in both groups for better understanding of data. The descriptive statistics in cars with 4 cylinders:\n\nsummary(train_auto |&gt;filter(cylinders == 4) |&gt;select(horsepower))\n\n   horsepower    \n Min.   : 46.00  \n 1st Qu.: 68.00  \n Median : 78.50  \n Mean   : 78.33  \n 3rd Qu.: 88.00  \n Max.   :113.00  \n\n\nand with 8 cylinders:\n\nsummary(train_auto |&gt;filter(cylinders == 8) |&gt;select(horsepower))\n\n   horsepower \n Min.   :105  \n 1st Qu.:140  \n Median :150  \n Mean   :160  \n 3rd Qu.:175  \n Max.   :225  \n\n\nLooking at summary statistics, there is a bit of overlap between distribution of horsepower among two groups but it does not seem to be much. In fact they seem to be quite separated. Also there is a clear different in their mean and the following plot also confirms this:\n\n\n\n\n\n\n\nFigure 3.3: Density plot of horsepower by number of cylinders.\n\n\n\n\n\n3.1.5 Testing Settings\nWe use a significant level of \\(\\alpha = 0.05\\) to run the test. Considering the data we have is a sample from a population of cars we have the following:\n\n\n\\(\\mu_{1}\\) is the mean of horsepower for cars with 4 cylinders in the population.\n\n\\(\\mu_{2}\\) is the mean of horsepower for cars with 8 cylinders in the population.\n\n3.1.6 Hypothesis Definitions\nWe now define the null and alternative hypothesis. Recall the main inquiry we had:\nYou may wondering if the average of horsepower in cars with 4 cylinders is statistically different than the means in cars with 8 cylinders?\nThis translates into the following null and alternative hypotheses:\n\\[H_0: \\mu_{1} = \\mu_{2} \\quad vs \\quad H_a: \\mu_{1} \\neq \\mu_{2}\\]\nNote that the alternative hypothesis is two-sided, as our question does not favor either group and only asks whether the means are different (i.e., group one could be less than or greater than group two). Also the hypothesis tests the unknown parameters in the population which are \\(\\mu_{1}\\) and \\(\\mu_{2}\\).\n\n3.1.7 Test Flavour and Components\nTo test this hypothesis, we use the two-sample student’s t-test for independent samples, which compares the sample means and incorporates variability within and between the samples. Note that in this case the samples are independent as clearly cars with 4 cylinders are independent from cars with 8 cylinders.\nNow we need to compute a test statistic from the sample. Assuming equal population variances, the test statistic is:\n\\[t = \\frac{(\\bar{X} - \\bar{Y})}{ \\sqrt{ S^{2}_p (\\frac{1}{n} + \\frac{1}{m}})} \\tag{3.1}\\] where:\n\n\n\\(\\bar{X}\\) is the mean of horsepower for cars with 4 cylinders in the sample\n\n\\(\\bar{Y}\\) is the mean of horsepower for cars with 8 cylinders in the sample\n\n\\(S^{2}_p\\) is the \\(\\textbf{pooled sample variane}\\) and is computed as: \\[S^{2}_p = \\frac{(n - 1)S_X^2 + (m - 1)S_Y^2}{n + m - 2} \\tag{3.2}\\]\n\n\n\\(S_X^2\\) and \\(S_Y^2\\) are the sample variances of the two groups.\n\n\n\n\nHeads-up!\n\n\n\nNote that all elements in Equation 3.1 which is our statistic are computed based on sample.\nYou can think of pooled sample variance defined in Equation 3.2 as a weighted average of sample variances of \\(S_X^2\\) and \\(S_Y^2\\).\nWhy it is a weighted average?\nThe weights are \\(\\frac{(n - 1)}{n + m - 2}\\) and \\(\\frac{(m - 1)}{n + m - 2}\\). These weights reflect how much information each sample’s variance provides — larger samples get more weight. You can see it better if you write it explicitly:\n\n\\[\nS_p^{2}\n= \\frac{(n - 1)}{n + m - 2} \\, S_X^2 + \\frac{(m - 1)}{n + m - 2} \\, S_Y^2\n\\]\n\nYou can see it is literally a weighted mean where weights sum to one:\n\n\\[\n\\frac{n-1}{n+m-2} + \\frac{m-1}{n+m-2} = 1\n\\]\n\n\n\n\n\nTip: Equal variance assumption!\n\n\n\nThe assumption in this test is that variances among two groups are equal meaning that if we look at the random variable of horsepower in both populations, the variance of this random variable is roughly equal in two groups (cars with 4 cylinders and cars with 8 cylinders).\nNote that we do not have access to population and this is rather an assumption that we make with consultation with experts or justifying it based on previous studies. We will introduce the test without equal variance assumption in the next section.\nThere are some statistical methods designed to test if the variances of different groups are the same or not. Similar to any hypothesis testing, these tests work on a random sample from the population to run the test. Some of the tests are F-test for Equality of Variances, Levene’s Test, and Bartlett’s Test.\nDescribing these tests is not the focus of this book. You can read more about them HERE/LINK\n\n\n\n\n3.1.8 Inferential Conclusions\nAs you can see, the test statistic defined in Equation 3.1 computes the difference between \\(\\bar{X}\\) and \\(\\bar{Y}\\) and scale it based on the standard deviation of this difference. Now the question is whether this difference is significant or not? In order to answer this question we need to know the behavior of statistic that we defined in Equation 3.1 (and denoted by \\(t\\)) and have a better understanding of what are typical values of this statistic.\n\n\n\nHeads-up!\n\n\nNote that \\(t\\) itself is a random variable as it would change from sample to sample.\n\n\n\nKnowing the distribution of this statistic helps us to compute \\(\\textit{p-value}\\) of the test as follows:\n\\[\\textit{p-value} = 2 \\times Pr(T_{(n+m-2)} \\ge |t|)\\]\nLooking at the formula, we can see that we are essentially calculating how much is it likely to see an observation as big as \\(t\\) or as extreme as \\(t\\) (which we computed from our sample) under a t-distribution with a certain degrees of freedom (df) which we are denoting by \\(T_{(n+m-2)}\\).\n\n\n\nTip:\n\n\n\nWe skipped the theory behind it but under the assumption that null hypothesis is correct (i.e. \\(\\mu_1=\\mu_2\\)) then the test statistic defined in Equation 3.1 (\\(t\\)) follows a t-distribution with \\(n + m -2\\) degrees of freedom. We denoted this distribution by \\(T_{(n+m-2)}\\).\nNote: The probability is multiplied by two since we have a two sided hypothesis (alternative is \\(\\mu_1 \\neq \\mu_2\\)). For a one sided test (when alternative hypothesis is \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\)) we do not need to multiply by two.\nNow we compare the \\(\\textit{p-value}\\) to our significance level. If the \\(\\textit{p-value}\\) is less than the significance level, then we have evidence against the null hypothesis. The reasoning is as follows: we performed the calculation under the assumption that the null hypothesis is true. If the null hypothesis is true, then the test statistic we computed should follow a \\(t\\)-distribution with \\(n + m - 2\\) degrees of freedom. If the p-value is smaller than our chosen significance level, this means it is unlikely that our observed result comes from a \\(t\\)-distribution with \\(n + m - 2\\) degrees of freedom. In other words, it is unlikely that the null hypothesis is correct.\n\n\n\n\nNote that our observation from the sample might still lead us to an incorrect conclusion (since there is variability among samples). Our tolerance for this type of error is determined by the significance level. If \\(\\textit{p-value}\\) is not less than significant level then we do not have any evidence to reject the null hypothesis. Otherwise (if \\(\\textit{p-value}\\) is less than significant level) we do have evidence against the null hypothesis\nNow let us see how to run the two-sample test in R and Python using some read data. Note that we already used train data for EDA purposes. For the purpose of hypothesis testing now we use test data to avoid double dipping.\n\n3.1.9 How to run the test in R and Python?\nThe following lines of code in tabset show you how to run the test in R or Python. Note that generally there are two ways of running this test in R as shown below (there might be other ways but we focus on these two for now). They both give the same result and you are welcome to use either of them. Here is a quick explanation from a coding perspective:\n\nIn Option 1, we first select the cars with 4 or 8 cylinders and save each of them in a vector (cylinders_4 and cylinders_8). We then use t.test function to run the test. Note that we pass them as x = cylinders_4 for first group and y = cylinders_8 for the second group.\nIn Option 2, we use a formula to tell R what is the variable that records the outcome of interest (in this example horsepower variable) and what is the grouping variable (in this example cylinders). This approach is more concise and easier to read, especially when working directly with a data frame. Note that we need to let R know where it can find horsepower and cylinders which we do by setting data = test_auto. Using horsepower ~ cylinders essentially tells R that my varriable of interest for t.test is in horsepower and the grouping variable (4 or 8 cylinders) is in cylinders. Note that the order matters and you can not write it as cylinders ~ horsepower.\n\n\n\nR Code - Option 1\nR Code - Option 2\nPython Code\n\n\n\n\n# Create a vector to hold horsepower values for cars with 4 cylinders\ncylinders_4 &lt;- test_auto |&gt;filter(cylinders == 4) |&gt;select(horsepower)\n\n# Create a vector to hold horsepower values for cars with 8 cylinders\ncylinders_8 &lt;- test_auto |&gt;filter(cylinders == 8) |&gt;select(horsepower)\n\n# Run the test\nt.test(x = cylinders_4, y = cylinders_8, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  cylinders_4 and cylinders_8\nt = -21.344, df = 149, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -85.12730 -70.70086\nsample estimates:\nmean of x mean of y \n  78.2381  156.1522 \n\n\n\n\n\n# Use the formula horsepower ~ cylinders to run the test \nt.test(horsepower ~ cylinders, data = test_auto, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  horsepower by cylinders\nt = -21.344, df = 149, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 4 and group 8 is not equal to 0\n95 percent confidence interval:\n -85.12730 -70.70086\nsample estimates:\nmean in group 4 mean in group 8 \n        78.2381        156.1522 \n\n\n\n\n\nfrom scipy import stats\nimport pandas as pd\n\n# Read test_auto dataframe in Python as df dataframe\ndf = pd.read_csv('data/test_auto.csv')\n\n# Select cars with 4 and 8 cylinders\ncylinders_4 = df[df[\"cylinders\"] == 4][\"horsepower\"]\ncylinders_8 = df[df[\"cylinders\"] == 8][\"horsepower\"]\n\n# Run the test\nt_stat, p_val = stats.ttest_ind(cylinders_4, cylinders_8, equal_var = True)\n\n# Print t statistic value\nprint(f\"T-statistic: {t_stat}\")\n\nT-statistic: -21.34403814660459\n\n# Print p-value of the test\nprint(f\"P-value: {p_val}\")\n\nP-value: 3.6294706302411423e-47\n\n\n\n\n\nDetails about R code and output\nIn order to run this test in R, similar to what we learned in Chapter 2 we can use t.test function in R. The function can be used to perform one or two sample t-tests. The relevant arguments of the function are as follows:\n\n\nx is (non-empty) numeric vector of data values.\n\ny is also (non-empty) numeric vector of data values (can be NULL if you run a one sample test).\n\nvar.equal is a binary value (TRUE/FALSE) to indicate if R needs to assume equal variance between population of two groups or not. By default the value of var.equal is FALSE. We manually set it to TRUE to implement equal variance assumption in our test.\n\nUsing Option 1 or Option 2 for R, in both outputs we can see the following elements:\n\nt is the test statistic defined in Equation 3.1\ndf is the degrees of freedom for the test. You can manually calculate and confirm/see that it equals \\(n+m-2\\).\np-value is the \\(\\textit{p-value}\\) of the test. Note that, by default, this is for a two-sided test. If you need to conduct a one-sided test, you can either divide the p-value by two or use the alternative argument in the t.test function. You can read more about it in help documentation of t.test function (type and run help(t.test) in R console and look for alternative argument.)\n95 percent confidence interval provides the 95% confidence interval for the parameter of \\(\\mu_1 - \\mu_2\\). You can control the level of your confidence interval by setting conf.level to other values between 0 and 1. By default it is set to conf.level = 0.95.\nsample estimates gives the sample means for each group, i.e. \\(\\bar{X}\\) and \\(\\bar{Y}\\).\n\nDetails about Python code and output\nIn this example, we perform the test in Python. We use pandas package to read data in Python and scipy for running the test. Here is a breakdown of key points:\n\ncylinders_4 and cylinders_8 are pandas objects containing the horsepower values for cars with 4 and 8 cylinders, respectively.\nThe function stats.ttest_ind computes the t-test for the means of two independent samples.\nThe argument equal_var=True tells Python to assume equal population variances (similar to setting var.equal = TRUE in t.test function R).\nThe function returns two values: t_stat which is the computed t-statistic and p_val which is the \\(\\textit{p-value}\\) of the test. By default, ttest_ind performs a two-sided test. For a one-sided test, you would need to adjust the \\(\\textit{p-value}\\) accordingly (e.g., divide by two if the t-statistic is in the direction of interest).\nThis Python approach is directly comparable to using t.test in R, with the main conceptual difference being how arguments are named and handled.\n\n3.1.10 Storytelling\nFinally, based on the sample we have and the analysis we conducted, we can draw a conclusion about our initial question: Is the mean horsepower of cars with 8 cylinders statistically different from that of cars with 4 cylinders?\nWe observed that the \\(p-\\textit{value}\\) of the test was extremely small compared to the significance level \\(\\alpha = 0.05\\). This provides strong evidence against the null hypothesis. In simple terms, this means: There is a meaningful difference in the average horsepower between cars with 4 cylinders and those with 8 cylinders.\nSo, if you are shopping for a car with more horsepower, it looks like going for one with 8 cylinders might be the way to go — assuming you don’t mind the extra fuel costs of course!!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html#sec-welch-t-test-ind-samples",
    "href": "book/chapter3-two-pop.html#sec-welch-t-test-ind-samples",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "\n3.2 Two sample Welch’s t-test for independent samples",
    "text": "3.2 Two sample Welch’s t-test for independent samples\n\n3.2.1 Review\nIn this section, we discuss the two-sample Welch’s t-test for independent samples. This test is similar to the two-sample Student’s t-test we described earlier in Section 3.1, with one important difference: it does not assume equal variances between the two groups. We use the Welch’s t-test when we have no reason or evidence to believe that the variable of interest has the same variance in both population groups.\n\n3.2.2 Study Design\nWe will be using Auto data set from ISLR package in this section too. Now the main statistical question of interest remains the same as before: You may wondering if the mean of horsepower in cars with 8 cylinders is statistically different than the means in cars with 4 cylinders? but we do not make an equal variance assumption anymore.\nIn this context, unequal variances mean that the spread or variability of horsepower values could differ between the two groups, so we allow each group to have its own variance estimate. Now we are applying a two sample Welch’s t-test for independent samples.\n\n3.2.3 Data Collection and Wrangling\nTo answer the question, we obtain the data set which is available in ISLR package. The following codes are exactly the same as described in Data Collection and Wrangling of Section 3.1 and are shown here as a review. Essentially the data sets (train and test) that we use in this section are the same as previous example.\n\n# Get a copy of data set\nauto_data &lt;- Auto\n\n# Filter rows to cars with 4 or 8 cylinders\nauto_data &lt;- auto_data |&gt;filter(cylinders %in% c(4,8) )\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Splitting the data set into train and test sets\ntrain_indices &lt;- sample(seq_len(nrow(auto_data)), size = 0.50 * nrow(auto_data))\ntrain_auto &lt;- auto_data[train_indices, ]\ntest_auto &lt;- auto_data[-train_indices, ]\n\n\n3.2.4 Explanatory Data Analysis\nOnce we have the data and it is split into training and test sets, the next step is to begin exploratory data analysis (EDA) on train set. Recall that the cylinders variable is an integer variable that helps to divide observations into two groups.\nWe are still interested in the distribution of horsepower in two different groups (cars with 4 cylinders vs cars with 8 cylinders). Using a histogram for this variable is a good choice as we have a variable with numerical values. The following plot is the same as before and we are showing it here as a review.\n\n\n\n\n\n\n\nFigure 3.4: Side-by-side histogram of horsepower by number of cylinders for train data.\n\n\n\n\nThe descriptive statistics in cars with 4 cylinders:\n\nsummary(train_auto |&gt;filter(cylinders == 4) |&gt;select(horsepower))\n\n   horsepower    \n Min.   : 46.00  \n 1st Qu.: 68.00  \n Median : 78.50  \n Mean   : 78.33  \n 3rd Qu.: 88.00  \n Max.   :113.00  \n\n\nand with 8 cylinders:\n\nsummary(train_auto |&gt;filter(cylinders == 8) |&gt;select(horsepower))\n\n   horsepower \n Min.   :105  \n 1st Qu.:140  \n Median :150  \n Mean   :160  \n 3rd Qu.:175  \n Max.   :225  \n\n\nOur conclusion remains the same. Looking at summary statistics, there is a bit of overlap between distribution of horsepower among two groups but it does not seem to be much. In fact they seem to be quite separated. Also there is a clear different in their mean and the following plot also confirms this:\n\n\n\n\n\n\n\nFigure 3.5: Density plot of horsepower by number of cylinders.\n\n\n\n\n\n3.2.5 Testing Settings\nWe use a significant level of \\(\\alpha = 0.05\\) to run the test. Considering the data we have is a sample from a population of cars we have the following parameters of interset in our population:\n\n\n\\(\\mu_{1}\\) is the mean of horsepower for cars with 4 cylinders in the population.\n\n\\(\\mu_{2}\\) is the mean of horsepower for cars with 8 cylinders in the population.\n\n3.2.6 Hypothesis Definitions\nWe now define the null and alternative hypothesis. Recall the main inquiry we had:\nYou may wondering if the average of horsepower in cars with 4 cylinders is statistically different than the means in cars with 8 cylinders?\nThis translates into the following null and alternative hypotheses:\n\\[H_0: \\mu_{1} = \\mu_{2} \\quad vs \\quad H_a: \\mu_{1} \\neq \\mu_{2}\\]\nNote that the alternative hypothesis is two-sided, as our question does not favor either group and only asks whether the means are different (i.e., group one could be less than or greater than group two). Also the hypothesis tests the unknown parameters in the population which are \\(\\mu_{1}\\) and \\(\\mu_{2}\\).\n\n3.2.7 Test Flavour and Components\nAs noted before we use Welch’s t-test if the assumption of equal variances is questionable. This test adjusts the standard error and degrees of freedom (df) of the test accordingly. As a result the test statistic and df of the test are different than what we described in Section 3.1. The Welch’s test statistic is computed as:\n\\[t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{S_X^2}{n} + \\frac{S_Y^2}{m}}} \\tag{3.3}\\]\nwhere:\n\n\n\\(\\bar{X}\\) is the mean of horsepower for cars with 4 cylinders in the sample\n\n\\(\\bar{Y}\\) is the mean of horsepower for cars with 8 cylinders in the sample\n\n\\(S_X^2\\) and \\(S_Y^2\\) are the sample variances of the two groups.\n\n\\(n\\) and \\(m\\) are the sample sizes in two groups (not necessarily the same).\n\n\n\nQuick review!\n\n\n\nNote that, as before, all elements in Equation 3.3 (statistic) are computed based on the sample.\nAdditionally, the assumption of unequal variance between two populations can be tested using a variety of statistical tests.\nWe do not discuss these tests in this book as the focus of this book lies elsewhere. If you are interested to read more about it, refer to REF.\n\n\n\n\n3.2.8 Inferential Conclusions\nAs you can see, the test statistic in Equation 3.3 computes the difference between averages of two samples and adjusts it based on the standard deviation of their differences. The only change from Student’s t-test is the standard deviation that is being used in the denominator. Again the question is whether this difference is significant or not?\nIn order to answer this question we need to know the behavior of statistic that we defined in Equation 3.3 and have a better understanding of what are typical values of this statistic. Knowing the distribution of this statistic helps us to compute the \\(\\textit{p-value}\\) of the test as follows:\n\\[\\textit{p-value} = 2 \\times Pr(T_{\\nu} \\ge |t|)\\]\n\n\nTip on degrees of freedom!\n\n\n\nWe skipped the theory behind it but under the assumption that null hypothesis is correct, it can be shown that the test statistic defined in Equation 3.3 still follows a t-distribution but with a different degrees of freedom (df).\nNote that this degree of freedom is not necessarily an integer number (could be a real number).\nThe Greek sign \\(\\nu\\) is used here to show the degree of freedom of t-distribution shown in \\(T_{\\nu}\\).\nThe degree of freedom is computed as\n\n\\[\\nu = \\frac{\\left( \\frac{s_1^2}{n} + \\frac{s_2^2}{m} \\right)^2}\n{\\frac{\\left( \\frac{s_1^2}{n} \\right)^2}{n - 1} + \\frac{\\left( \\frac{s_2^2}{m} \\right)^2}{m - 1}}\\]\n\n\n\n3.2.9 How to run the test in R and Python?\nThe following lines of code in tabset show you how to run the Welch’s test in R or Python.\n\n\nA quick reminder!\n\n\n\nIn Option 1, we first select the cars with 4 or 8 cylinders and save them in a vector (cylinders_4 and cylinders_8). We then use t.test function to run the test.\nIn Option 2, we use a formula to tell R what is the variable that records the outcome of interest (in this example horsepower variable) and what is the grouping variable (in this example cylinders).\nUsing Option 2 is more concise and easier to read, especially when working directly with a data frame.\nNote that we need to let R know where it can find horsepower and cylinders which we do by setting data = test_auto.\n\n\n\n\n\nR Code - Option 1\nR Code - Option 2\nPython Code\n\n\n\n\n# Create a vector to hold horsepower values for cars with 4 cylinders\ncylinders_4 &lt;- test_auto |&gt;filter(cylinders == 4) |&gt;select(horsepower)\n\n# Create a vector to hold horsepower values for cars with 8 cylinders\ncylinders_8 &lt;- test_auto |&gt;filter(cylinders == 8) |&gt;select(horsepower)\n\n# Run the test\nt.test(x = cylinders_4, y = cylinders_8, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  cylinders_4 and cylinders_8\nt = -16.92, df = 55.789, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -87.13950 -68.68866\nsample estimates:\nmean of x mean of y \n  78.2381  156.1522 \n\n\n\n\n\n# Use the formula horsepower ~ cylinders to run the test \nt.test(horsepower ~ cylinders, data = test_auto, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  horsepower by cylinders\nt = -16.92, df = 55.789, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 4 and group 8 is not equal to 0\n95 percent confidence interval:\n -87.13950 -68.68866\nsample estimates:\nmean in group 4 mean in group 8 \n        78.2381        156.1522 \n\n\n\n\n\nfrom scipy import stats\nimport pandas as pd\n\n# Read test_auto dataframe in Python as df dataframe\ndf = pd.read_csv('data/test_auto.csv')\n\n# Select cars with 4 and 8 cylinders\ncylinders_4 = df[df[\"cylinders\"] == 4][\"horsepower\"]\ncylinders_8 = df[df[\"cylinders\"] == 8][\"horsepower\"]\n\n# Run the test\nt_stat, p_val = stats.ttest_ind(cylinders_4, cylinders_8, equal_var = False)\n\n# Print t statistic value\nprint(f\"T-statistic: {t_stat}\")\n\nT-statistic: -16.919952924079897\n\n# Print p-value of the test\nprint(f\"P-value: {p_val}\")\n\nP-value: 1.2443553344442986e-23\n\n\n\n\n\nDetails about R code and output\nIn order to run this test in R, similar to what we learned in Chapter 2 we can use t.test function in R. The function can be used to perform one or two sample t-tests. The relevant arguments of the function are as follows:\n\n\nx is (non-empty) numeric vector of data values.\n\ny is also (non-empty) numeric vector of data values (can be NULL if you run a one sample test).\n\nvar.equal is a binary value (TRUE/FALSE) to indicate if R needs to assume equal variance between population of two groups or not. By default the value of var.equal is FALSE and we manually set it to FALSE to make it clear to the reader that we are making an unequal variance assumption in our test.\n\nUsing Option 1 or Option 2 for R, in both outputs we can see the following elements:\n\nt is the test statistic defined in Equation 3.3\ndf is the degrees of freedom for the test. You can manually calculate and confirm/see that it equals \\(\\nu\\) that we defined in Equation 3.3.\np-value is the \\(\\textit{p-value}\\) of the test. Note that, by default, this is for a two-sided test. If you need to conduct a one-sided test, you can either divide the p-value by two or use the alternative argument in the t.test function. You can read more about it in help documentation of t.test function (type and run help(t.test) in R console and look for alternative argument.)\n95 percent confidence interval provides the 95% confidence interval for the parameter of \\(\\mu_1 - \\mu_2\\). You can control the level of your confidence interval by setting conf.level to other values between 0 and 1. By default it is set to conf.level = 0.95.\nsample estimates gives the sample means for each group, i.e. \\(\\bar{X}\\) and \\(\\bar{Y}\\).\n\nDetails about Python code and output\nIn this example, we perform the test in Python. We use pandas package to read data in Python and scipy for running the test. Here is a breakdown of key points:\n\ncylinders_4 and cylinders_8 are pandas objects containing the horsepower values for cars with 4 and 8 cylinders, respectively.\nThe function stats.ttest_ind computes the t-test for the means of two independent samples.\nThe argument equal_var=False tells Python to assume unequal population variances (similar to setting var.equal = FALSE in t.test function R).\nThe function returns two values: t_stat which is the computed t-statistic and p_val which is the \\(\\textit{p-value}\\) of the test. By default, ttest_ind performs a two-sided test. For a one-sided test, you would need to adjust the \\(\\textit{p-value}\\) accordingly (e.g., divide by two if the t-statistic is in the direction of interest).\nThis Python approach is directly comparable to using t.test in R, with the main conceptual difference being how arguments are named and handled.\n\n3.2.10 Storytelling\nFinally, based on the sample we have and the analysis we conducted, we can draw a conclusion about our initial question: Is the mean horsepower of cars with 8 cylinders statistically different from that of cars with 4 cylinders?\nWe observed that the \\(p-\\textit{value}\\) of the test was extremely small compared to the significance level \\(\\alpha = 0.05\\). This provides strong evidence against the null hypothesis. In simple terms, this means: There is a meaningful difference in the average horsepower between cars with 4 cylinders and those with 8 cylinders.\nSo, if you are shopping for a car with more horsepower, it looks like going for one with 8 cylinders might be the way to go — assuming you don’t mind the extra fuel costs of course!!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html#sec-z-test-ind-samples",
    "href": "book/chapter3-two-pop.html#sec-z-test-ind-samples",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "\n3.3 Two sample z-test for Independent Samples",
    "text": "3.3 Two sample z-test for Independent Samples\n\n3.3.1 Review\nIn the tests we have seen so far in Section 3.1 and Section 3.2, we estimated the variance of each group using the sample variance. For example see \\(S^2_{X}\\) or \\(S^2_{Y}\\) defined in formula Equation 3.3. This approach is realistic, as in most cases we do not know the population variance of our variable of interest and it is being estimated by sample variance. However, if the population variance for each group is known we can incorporate this information into our test.\nFor example, suppose you are analyzing wage differences between high school and college graduates using a large labor market data set. A government labor agency previously conducted extensive research and reported that the true population variances of hourly wages for both groups (based on some historical data) should be as follows:\n\nPopulation variance of wages among high school graduates: \\(\\sigma_1^2 = 75\\)\n\nPopulation variance of wages among college graduates: \\(\\sigma_2^2 = 60\\)\n\n\nNote that this is the population variance and not the sample. You want to assess whether the average wage differs between the two groups. Since the population variances are known (e.g., from reliable historical studies or large scale sampling in previous studies), a two-sample z-test is appropriate instead of a t-test. We treat these groups as independent samples since each person’s wage is recorded once and belongs to a distinct education level.\nSince we do not have access to the wages of all individuals in each education group, we take a random sample from each population. Suppose:\n\nFrom the population of high school graduates (Population 1), we obtain a sample of size \\(n\\), denoted as:\n\n\\[X_1, X_2, \\ldots, X_n\\]\n\nFrom the population of college graduates (Population 2), we obtain a sample of size \\(m\\), denoted as:\n\n\\[Y_1, Y_2, \\ldots, Y_m\\] Note that the sample sizes \\(n\\) and \\(m\\) do not need to be equal. Now, the central question becomes Is there a statistically significant difference between the mean wages of high school and college graduates? In formal terms, we test the hypotheses:\n\\[\nH_0: \\mu_1 = \\mu_2 \\quad \\text{versus} \\quad H_A: \\mu_1 \\ne \\mu_2\n\\] where \\(\\mu_1\\) is the population mean of wage in high school graduates and \\(\\mu_2\\) is the population mean of wage in college graduates.\n\n3.3.2 Study Design\nFor this example, we will be using the Wage data set from the ISLR2 package. This data set contains information about wages and demographic characteristics for 3,000 workers in the Mid-Atlantic region of the United States. Some of the variables of interest are:\n\neducation, a categorical variable that records the highest level of education completed by the individual (e.g., “HS Grad”, “College Grad”), and\nwage, a continuous variable representing the individual’s hourly wage in USD.\n\nYou may be wondering: Is the mean wage for college graduates statistically different from the mean wage for high school graduates?\nTo answer this question, we will compare the sample means of these two groups using a two-sample z-test under the assumption that the population variances are known. We use the Wage data set to extract wage data for two groups:\n\nHigh school graduates: education == \"HS Grad\"\nCollege graduates: education == \"College Grad\"\n\n3.3.3 Data Collection and Wrangling\nTo answer this question, we obtain the data set which is available in the ISLR2 package. Note that we consider this data a random sample from the population of working adults. First, we create a new copy of this data set to avoid modifying the original data (this step is optional). Then, we filter the rows to keep only individuals who are either high school graduates or college graduates.\n\n# Get a copy of data set\nwage_data &lt;- Wage\n\n# Filter rows to keep only HS Grad and College Grad \n# and then rename the levels to:\n# - HS Grad --&gt; High school graduate\n# - College Grad --&gt; College graduate\nwage_data &lt;- wage_data |&gt;\n  filter(education %in% c('2. HS Grad', '4. College Grad')) |&gt;\n  mutate(education = recode(education,\n                            '2. HS Grad' = 'High school graduate',\n                            '4. College Grad' = 'College graduate'))\n\nFinally, we randomly split the wage_data data set into training and testing sets, using a 50-50 proportion between train and test. This allows us to conduct our test using a subset of the data (train) while reserving the rest (test) for potential follow-up analysis.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Splitting the data set into train and test sets\ntrain_indices &lt;- sample(seq_len(nrow(wage_data)), size = 0.50 * nrow(wage_data))\ntrain_wage &lt;- wage_data[train_indices, ]\ntest_wage &lt;- wage_data[-train_indices, ]\n\n\n3.3.4 Exploratory Data Analysis\nOnce we have the data and it is split into training and test sets, the next step is to begin exploratory data analysis (EDA) on the training set. The wage variable is a numerical variable that represents hourly earnings. The education variable is a categorical variable that helps us divide observations into two groups.\nIn particular, we are interested in the distribution of wage in two different groups (high school graduates vs college graduates). Using a histogram for this variable is a good choice as it allows us to visually inspect how the wages vary within and between these two education levels.\n\n\n\n\n\n\n\nFigure 3.6: Side-by-side histogram of wage by education level\n\n\n\n\nWe also look at some descriptive statistics of wage in both groups for better understanding of the data. The descriptive statistics for high school graduates:\n\nsummary(train_wage |&gt;\n          filter(education == 'High school graduate') |&gt;\n          select(wage))\n\n      wage       \n Min.   : 23.27  \n 1st Qu.: 78.39  \n Median : 94.07  \n Mean   : 97.19  \n 3rd Qu.:111.72  \n Max.   :318.34  \n\n\nand for college graduates:\n\nsummary(train_wage |&gt;\n          filter(education == 'College graduate') \n        |&gt;select(wage))\n\n      wage       \n Min.   : 40.41  \n 1st Qu.: 97.49  \n Median :118.88  \n Mean   :122.83  \n 3rd Qu.:141.78  \n Max.   :277.80  \n\n\nLooking at these summary statistics, we can see that the wage distributions for the two education groups have some overlap, and there does not appear to be a large difference in their centers. In fact, the mean wage for college graduates is only slightly higher, and this is not immediately obvious from the plots.\nThere appears to be some observations with higher wage values in College Grad group compared to High School Graduate. The following density plot provides additional visual support for this observation:\n\n\n\n\n\n\n\nFigure 3.7: Density plot of wage by education level.\n\n\n\n\n\n3.3.5 Testing Settings\nWe use a significance level of \\(\\alpha = 0.05\\) to conduct the hypothesis test. Since the data we have is a sample from a larger population of workers, we define the following population parameters:\n\n\n\\(\\mu_{1}\\) is the mean wage of individuals with a high school education in the population.\n\n\n\\(\\mu_{2}\\) is the mean wage of individuals with a college education in the population.\n\nOur goal is to test whether the population mean wage differs between these two educational groups.\n\n3.3.6 Hypothesis Definitions\nWe now define the null and alternative hypothesis. Recall the main inquiry we had:\nIs the mean wage for college graduates statistically different from the mean wage for high school graduates?\nThis translates into the following null and alternative hypotheses:\n\\[H_0: \\mu_{1} = \\mu_{2} \\quad vs \\quad H_a: \\mu_{1} \\neq \\mu_{2}\\]\n\n3.3.7 Test Flavour and Components\nTo test this hypothesis, we use the two-sample z-test for independent samples with known population variances, which compares the sample means and incorporates the known variability within each population. Note that the samples are independent. The test statistic for this test is computed as:\n\\[z = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{\\sigma_1^2}{n} + \\frac{\\sigma_2^2}{m}}} \\tag{3.4}\\]\nwhere:\n\n\n\\(\\bar{X}\\) is the mean wage of individuals with a high school education in the sample.\n\n\\(\\bar{Y}\\) is the mean wage of individuals with a college education in the sample.\n\n\\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are the known population variances of the two groups\n\n\\(n\\) and \\(m\\) are the sample sizes of the two groups\n\nBecause population variances are known, the standard error of the difference in sample means is computed directly from the population variances, avoiding the need to estimate variance from the samples.\n\n\nHeads-up!\n\n\nNote that \\(z\\) defined in Equation 3.4 itself is a random variable meaning that it would vary from sample to sample.\n\n\n\n3.3.8 Inferential Conclusions\nAs you can see, the test statistic defined in Equation 3.4 computes the difference between \\(\\bar{X}\\) and \\(\\bar{Y}\\) and scales it based on the known variance of this difference (or more precisely standard deviation of the difference!). Now the question is whether this difference is significant or not? To answer this, we need to understand the behavior of the statistic we defined in Equation 3.4 (\\(z\\)) and what typical values it can take.\nAs we shown before, knowing the distribution of this statistic allows us to compute the \\(\\textit{p-value}\\) of the test as follows:\n\\[\\textit{p-value} = 2 \\times Pr(Z \\ge |z|) \\tag{3.5}\\]\n\n\nUpper case \\(Z\\) vs lower case \\(z\\)!\n\n\n\nThe lower case \\(z\\) in Equation 3.5 is the statistic computed from the sample (Equation 3.4).\nThe upper case \\(Z\\) though is a notation to use for random variable.\nThis random variable follows a Normal distribution with mean zero and standard deviation of one.\nLooking at the formula in Equation 3.5, we are essentially calculating how likely it is to observe a value as extreme as \\(z\\) under the null hypothesis - a Normal distribution with mean zero and standard deviation of one.\n\n\n\n\n\n\nInterpreting the Test Statistic and p-value!\n\n\n\nUnder the assumption that the null hypothesis is correct (i.e., \\(\\mu_1 = \\mu_2\\)), the test statistic \\(z\\) follows the standard normal distribution that textbooks usually denote it by \\(N(0,1)\\). The first element is mean (0) and the second one is the standard deviation (1).\nNote: The probability is multiplied by two because we are conducting a two-sided test (alternative hypothesis: \\(\\mu_1 \\neq \\mu_2\\)). For a one-sided test (e.g., \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\)) this multiplication is not needed.\nWe compare the \\(\\textit{p-value}\\) to our significance level \\(\\alpha\\). If the \\(\\textit{p-value}\\) is less than \\(\\alpha\\), then we have evidence against the null hypothesis.\nThe reasoning is: assuming the null hypothesis is true, the test statistic should follow a standard normal distribution. A very small \\(\\textit{p-value}\\) means our observed \\(z\\) is unlikely under this distribution, and so it is unlikely the null hypothesis is correct.\n\n\n\n\n3.3.9 How to run the two-sample z-test in R and Python\n\nThe following examples show how to implement the two-sample z-test for independent samples with known population variances in both R and Python using the Wage data set. Note that we know the population variances \\(\\sigma_1^2 = 75\\) for high school graduates and \\(\\sigma_2^2 = 60\\) for college graduates.\n\n\nR Code\nPython Code\n\n\n\n\n# Filter test_wage data to create two groups: \n# High school graduates and college graduates\nhs_wages &lt;- test_wage |&gt;\n  filter(education == 'High school graduate') |&gt;\n  pull(wage)\n\ncol_wages &lt;- test_wage |&gt;\n  filter(education == 'College graduate') |&gt;\n  pull(wage)\n\n# Set the known population variances\nsigma2_hs &lt;- 75\nsigma2_col &lt;- 60\n\n# Run two-sample z-test with known variances\nresult &lt;- z.test(\n  x = hs_wages,\n  y = col_wages,\n  sigma.x = sqrt(sigma2_hs),  # standard deviation for group 1\n  sigma.y = sqrt(sigma2_col), # standard deviation for group 2\n  alternative = \"two.sided\"\n)\n\n# Print results\nprint(result)\n\n\n    Two-sample z-Test\n\ndata:  hs_wages and col_wages\nz = -55.239, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -32.72362 -30.48102\nsample estimates:\nmean of x mean of y \n 94.29543 125.89775 \n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\n\n\n# Read data set\ndf = pd.read_csv('data/test_wage.csv')\n\n# Extract wages for high school graduates and college graduates\nhs_wages = df.loc[df['education'] == 'High school graduate', 'wage'].to_numpy()\ncol_wages = df.loc[df['education'] == 'College graduate', 'wage'].to_numpy()\n\n# Sample sizes\nn1 = len(hs_wages)\nn2 = len(col_wages)\n\n# Sample means\nmean1 = np.mean(hs_wages)\nmean2 = np.mean(col_wages)\n\n# Standard deviations (square root of known variances)\nsigma1 = np.sqrt(75)\nsigma2 = np.sqrt(60)\n\n# Calculate standard error using known population std dev\nse = np.sqrt( (sigma1**2 / n1) + (sigma2**2 / n2) )\n\n# Calculate z-statistic\nz_stat = (mean1 - mean2) / se\n\n# Two-sided p-value\np_value = 2 * norm.cdf(-abs(z_stat))\n\n# Print z-statistic and p-value\nprint(f\"Z-statistic: {z_stat}\")\n\nZ-statistic: -55.23877690006868\n\nprint(f\"P-value: {p_value}\")\n\nP-value: 0.0\n\n\n\n\n\nIn order to run the two-sample z-test with known population variances, we use the z.test function from the BSDA package in R which have a similar input and output to t.test function. This function is designed to handle one and two sample z-tests when population standard deviations are known. The key arguments of z.test for a two-sample test are:\n\n\nx: a numeric vector of data values for group 1.\n\ny: a numeric vector of data values for group 2.\n\nsigma_x: the known population standard deviation for group 1.\n\nsigma_y: the known population standard deviation for group 2.\n\nalternative: specifies the alternative hypothesis; \"two.sided\" by default for a two-sided test.\n\nNote: sigma_x and sigma_y expects the standard deviation of each group and not the variance. As a result in the R code in tabset we passed sqrt(sigma2_hs) and sqrt(sigma2_col).\nWhen you run the test, the output includes:\n\n\nstatistic: the calculated \\(z\\) test statistic defined in Equation 3.4.\n\np.value: the p-value for the test based on the standard normal distribution, defined in Equation 3.5.\n\nconf.int: the confidence interval for the difference in means \\(\\mu_1 - \\mu_2\\).\n\nestimate: the sample means for each group.\n\nNote: The z.test function assumes that the population standard deviations are known and uses them directly to compute the test statistic and confidence interval. This differs from tests that estimate variance from the samples such as t-tests that we discussed in Section 3.1 and Section 3.2.\n\n3.3.10 Storytelling\nIn summary, these results tell us that the observed difference in wages is highly unlikely to have occurred by random chance if the true means were equal. Therefore, we conclude that college graduates earn significantly more than high school graduates on average.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html#sec-t-test-paired-samples",
    "href": "book/chapter3-two-pop.html#sec-t-test-paired-samples",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "\n3.4 Two sample t-test for Paired Samples",
    "text": "3.4 Two sample t-test for Paired Samples\n\n3.4.1 Review\nIn this section, we discuss the two sample t-test for paired samples which is used when we have two sets of related observations. Unlike the two-sample t-tests for independent samples that we discussed so far (such as the Student’s or Welch’s t-test), the paired t-test is appropriate when the two samples are dependent. This means each observation in one sample can be naturally paired with an observation in the other.\nExamples of such are before-and-after measurements on the same subjects, or matched subjects across two different conditions. The key idea is that the test evaluates whether the mean difference between paired observations is significantly different from zero.\n\n\nWhen to use this test!\n\n\nWe use the paired sample t-test when:\n\nThe data consist of paired observations.\nThe differences between pairs are approximately normally distributed or at least symmetric (especially important for small sample sizes).\nAnd the measurement scale is continuous.\n\n\n\nPaired samples arise when each observation in one group is matched or linked to an observation in the other group. This structure is typical in before-and-after studies, matched-subject designs, or repeated measures on the same individuals. A classic example comes from health sciences.\nSuppose you are investigating whether a new diet plan reduces blood pressure. You recruit a group of participants and record their blood pressure before starting the diet. After following the diet for two months, you measure their blood pressure again. In this scenario, each participant contributes two measurements: one before the intervention and one after. Note that these measurements are not independent as they come from the same person. Therefore we treat them as paired.\nTo formulate the problem and hypothesis, let us assume that each individual has two measurements:\n\nBefore the diet: \\(X_1, X_2, \\ldots, X_n\\)\nAfter the diet: \\(Y_1, Y_2, \\ldots, Y_n\\)\n\nNote that in this case the sample size is the same (in both before and after diet sample we have \\(n\\) observations). We call this a paired sample. Since the samples are paired, we define the difference for each individual as follows:\n\\[D_i = Y_i - X_i \\quad \\textit{for} \\quad i = 1,2, \\ldots, n\\] Each \\(D_i\\) is the difference of blood pressure after and before using new diet for the \\(i\\)-th person. Now the main statistical question is:\nIs there a significant difference in the mean blood pressure before and after the diet?\nNow if the treatment is effective, one expect to have difference sample average of around zero. In other words, we test the following hypothesis:\n\\[H_0: \\mu_D = 0 \\quad \\text{versus} \\quad H_A: \\mu_D \\ne 0\\]\nwhere \\(\\mu_D\\) is a parameter to show the population mean differences. We talk more about this later in this section.\n\n3.4.2 Study Design\nConsider a clinical investigation examining whether an innovative medication can lower LDL cholesterol levels in adults diagnosed with hypercholesterolemia which basically means having too much cholesterol in the blood. This can increase the risk of heart problems because cholesterol can build up in blood vessels and block blood flow.\nThe study monitors a group of \\(n=120\\) participants over time, recording LDL cholesterol concentrations prior to treatment and again after an 8-week course of the drug. The outcome of interest is the LDL cholesterol measurement (in mg/dL), a widely used biomarker for cardiovascular risk.\n\n3.4.3 Data Collection and Wrangling\nIn this section, we will work with a simulated dataset that has been specifically created to illustrate the concept of paired t-test for this example. In other words, the dataset for this study is generated under a known scenario. The code below generates the dataset we will be using. While running these lines is not strictly necessary if you are only interested in following along with the analysis, we recommend reading Appendix B for a deeper understanding of how and why the data were generated in this way.\n\n# set.seed for reproducibility\nset.seed(123)\n\n# Set number of participants\nn_patients &lt;- 120   \n\n# Set the average decrease in LDL due to treatment\nDelta &lt;- 2.5\n\n# Step 1: Generate LDL before treatment\nldl_before &lt;- rnorm(n_patients, mean = 160, sd = 5)\n\n# Step 2: Generate LDL after treatment\nldl_after &lt;- ldl_before - Delta + rnorm(n_patients, mean = 0, sd = 1)\n\n# Create final dataset\ncholesterol_data &lt;- data.frame(\n  patient_id = 1:n_patients,\n  LDL_before = round(ldl_before, 1),\n  LDL_after  = round(ldl_after, 1)\n)\n\n# Display first few observations\nhead(cholesterol_data, 10)\n\n   patient_id LDL_before LDL_after\n1           1      157.2     154.8\n2           2      158.8     155.4\n3           3      167.8     164.8\n4           4      160.4     157.6\n5           5      160.6     160.0\n6           6      168.6     165.4\n7           7      162.3     160.0\n8           8      153.7     151.3\n9           9      156.6     153.1\n10         10      157.8     155.2\n\n\nNote that cholesterol_data contains two columns: LDL_before, which records each patient’s LDL level before taking the new medication, and LDL_after, which records the LDL level after taking the new medication. This setup clearly illustrates the paired nature of the data: each row corresponds to a single individual, with measurements taken at two different points in time: before and after the treatment. Because the observations are linked within each person, the analysis must account for this pairing rather than treating the two sets of values as independent samples.\nThe next step in this type of analysis is to calculate the paired differences for each individual. If the medication is effective, we expect the difference between the after and before measurements to be substantially different from zero; if it is not effective, the differences should be close to zero on average. The code below adds a new column, LDL_diff, which stores the difference LDL_diff = LDL_after - LDL_before for each patient:\n\ncholesterol_data &lt;- cholesterol_data |&gt; \n  mutate(LDL_diff = LDL_after - LDL_before)\n\nFinally, we randomly split the cholesterol_data data set into training and testing sets, using a 50-50 proportion between train and test.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Splitting the data set into train and test sets\ntrain_indices     &lt;- sample(seq_len(nrow(cholesterol_data)), size = 0.50 * nrow(cholesterol_data))\ntrain_cholesterol &lt;- cholesterol_data[train_indices, ]\ntest_cholesterol  &lt;- cholesterol_data[-train_indices, ]\n\n\n3.4.4 Exploratory Data Analysis\nBefore performing any formal statistical tests, it is important to explore the dataset to gain an initial understanding of its characteristics. For paired data like this, our main objectives is to visualize the paired differences to see whether LDL levels tend to decrease after the medication. To address this objective, we can create a density of the paired differences. If the center of this distribution (e.g., the mean) is far from zero, it may indicate that the medication is effective.\n\n\n\n\n\n\n\nFigure 3.8: Density Plot of Paired LDL Levels Differences\n\n\n\n\nFrom the density plot, most LDL differences appear to fall below zero with the distribution noticeably centered away from it. This suggests a general decrease in LDL levels after treatment, but the observation should be verified through formal statistical testing. From this plot, it is not clear if this reduction is significant or not.\n\n3.4.5 Testing Settings\nWe use a significance level of \\(\\alpha = 0.05\\) to conduct the hypothesis test. Since our data represent a sample from a larger population of patients, we define the following population parameter:\n\n\n\\(\\mu_{d}\\) is the mean difference in LDL levels (after treatment – before treatment) for all patients in the population.\n\nOur goal is to test whether the population mean difference \\(\\mu_{d}\\) is equal to zero. This would indicate no change in LDL levels after treatment versus the alternative that \\(\\mu_{d}\\) is not zero which suggests a treatment effect.\n\n\nWhat is \\(\\mu_D\\) as a parameter here?\n\n\n\nHere the notation of \\(\\mu_D\\) is the population mean of the differences of \\(D_i\\) which is an unknown parameter in the population.\nTo test this hypothesis, we use the paired t-test, which is essentially a one-sample t-test on the differences \\(D_1, D_2, \\ldots, D_n\\).\nWe test \\(\\mu_D=0\\) because if there is an actual effect of diet on blood pressure, we expect the null hypothesis to be rejected.\n\n\n\n\n3.4.6 Hypothesis Definitions\nWe now define the null and alternative hypotheses. Recall the main question we posed:\nIs the mean LDL level after treatment statistically different from the mean LDL level before treatment for the same patients?\nBecause the measurements are paired, this question is equivalent to asking whether the mean difference in LDL levels (after - before) is zero. This leads to the following hypotheses:\n\\[H_0: \\mu_d = 0 \\quad \\textit{(no mean change in LDL levels)}\\]\n\\[H_a: \\mu_d \\neq 0 \\quad \\textit{(mean LDL levels change after treatment)}\\]\n\n3.4.7 Test Flavour and Components\nTo test this hypothesis, we use the paired t-test, which compares the mean difference in LDL levels before and after treatment within the same individuals. Note that the test is done on the paired differences. The test accounts for the fact that the measurements are not independent but rather are paired. The test statistic for this test is computed as:\n\\[t = \\frac{\\bar{d}}{ s_d / \\sqrt{n} } \\tag{3.6}\\]\nwhere:\n\n\\(\\bar{d}\\) is the mean of the differences in LDL levels (post-treatment minus pre-treatment) for all participants.\n\\(s_d\\) is the standard deviation of these differences and is computed as:\n\n\\(s_d = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (d_i - \\bar{d})^2}\\)\n\n\n\\(n\\) is the number of paired observations (participants).\n\n3.4.8 Inferential Conclusions\nAs you can see in Equation 3.6, the statistic divide the sample mean of differences and scale it according to the standard deviation. The idea is very similar to the other tests that we have been working on so far. Under the null hypothesis, the test statistic defined in Equation 3.6 follows a t-distribution with \\(n-1\\) degrees of freedom. For this test, we can compute the \\(\\textit{p-value}\\) as:\n\\[\\textit{p-value} = 2 \\times \\Pr(T_{n - 1} \\ge |t|)\\]\n\n\nRemember!\n\n\n\nWhen we run this test, we make the following assumptions:\n\n\neither the sample size is large enough (we are thinking about \\(n=30\\) at least) so that central limit theorem assumptions work well, or\nthe distribution of our sample in each group is normal or symmetric enough.\n\n\nIf the normality assumption is also not satisfied (e.g., due to skewed distributions or outliers) or we have a very small sample size, we may turn to a non-parametric alternative, such as the Mann–Whitney–Wilcoxon test, which compares the ranks of the observations across groups rather than the raw values but this book will not cover it. You can read more about it LINK.\n\n\n\n\n3.4.9 How to run the two-sample paired t-test in R and Python\n\n\n\nR Code\nPython Code\n\n\n\n\n# Extract LDL measurements before and after treatment\nldl_before &lt;- test_cholesterol |&gt; pull(LDL_before)\nldl_after  &lt;- test_cholesterol |&gt; pull(LDL_after)\n\n# Run paired t-test\nt.test(x = ldl_before, y = ldl_after, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  ldl_before and ldl_after\nt = 18.248, df = 59, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.141281 2.668719\nsample estimates:\nmean difference \n          2.405 \n\n\n\n\n\nimport pandas as pd\nfrom scipy.stats import ttest_rel\n\n# Load dataset\ndf = pd.read_csv('data/test_cholesterol.csv')\n\n# Extract LDL measurements before and after treatment\nldl_before = df['LDL_before'].to_numpy()\nldl_after = df['LDL_after'].to_numpy()\n\n# Run paired t-test\nt_stat, p_value = ttest_rel(ldl_before, ldl_after)\n\n# Print results\nprint(f\"T-statistic: {t_stat}\")\n\nT-statistic: 18.248188665420518\n\nprint(f\"P-value: {p_value}\")\n\nP-value: 6.125701137511437e-26\n\n\n\n\n\nDetails about R code and output\nKey arguments of the t.test function in R for a paired test are:\n\nx: numeric vector of measurements before treatment.\ny: numeric vector of measurements after treatment.\npaired = TRUE: indicates that the samples are related.\n\nThe output includes:\n\nstatistic: the calculated t-test statistic (here, t = 22.303).\ndf: the degrees of freedom for t-distribution\np.value: the p-value for testing the null hypothesis that the true mean difference is zero.\nconf.int: the 95% confidence interval for the mean difference in LDL levels.\nestimate: the estimated mean of the paired differences.\n\nNote: The paired t-test assumes that the differences between pre and post-treatment measurements are approximately normally distributed. Unlike independent sample tests, this test directly accounts for the correlation between repeated measurements on the same patient.\nDetails about Python code and output\nWhen running a paired t-test in Python using scipy.stats.ttest_rel, the main components are:\n\nx: numeric array of measurements before treatment (ldl_before).\ny: numeric array of measurements after treatment (ldl_after).\n\nThe output includes:\n\nt_stat: the computed t-test statistic for the paired differences.\np_value: the probability of observing the data if the null hypothesis (no difference) is true.\n\n3.4.10 Storytelling\nBased on our analysis, we asked: Did LDL levels change significantly after treatment? The paired t-test gave a very small \\(\\textit{p-value}\\), far below \\(\\alpha = 0.05\\) providing strong evidence against the null hypothesis. In simple terms, this means LDL levels after treatment are significantly lower than before. On average, the reduction was about 2.4 units, with the true effect likely between 2.1 and 2.7 units. In summary, these results indicate that the observed reduction in LDL levels is highly unlikely to have occurred by chance if there were no true effect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter3-two-pop.html#tests-for-two-population-proportions",
    "href": "book/chapter3-two-pop.html#tests-for-two-population-proportions",
    "title": "3  Tests for Two Continuous Population Mean",
    "section": "\n3.5 Tests for Two Population Proportions",
    "text": "3.5 Tests for Two Population Proportions\n\n3.5.1 Review\nIn the examples we studied in [Section LINK HERE], we estimated the proportion of a population based on the sample proportion \\(\\hat{p}\\). This is often the case in practice, as the population proportion is typically unknown and must be inferred from sample data. When we are comparing two groups and wish to test whether their underlying proportions differ, we can extend this idea to a two-proportion test.\nSuppose you are investigating support for a new policy among two different regions. Each individual either supports the policy (coded as 1) or does not support it (coded as 0). You want to determine if the proportion of supporters is significantly different between Region A and Region B.\nFormally, let:\nRegion A (Population 1): each individual either supports the policy (coded as 1) or policy (coded as 0).\nRegion B (Population 2): defined in the same way.\nFrom each population, we take a random sample:\n\nFrom Region A, a sample of size \\(n\\), with observed sample proportion of support \\(\\hat{p}_1\\).\nFrom Region B, a sample of size \\(m\\), with observed sample proportion of support \\(\\hat{p}_2\\).\n\nwhere \\(p_1\\) is the population proportion of support in Region A and \\(p_2\\) is the population proportion of support in Region B.\nThe main statistical question becomes: Is there a significant difference between the true population proportions of support in the two regions? We formalize this as a hypothesis test:\n\\[\nH_0: p_1 = p_2 \\quad \\text{versus} \\quad H_A: p_1 \\ne p_2\n\\] To answer this, we use the two-sample z-test for population proportions, which compares the observed sample proportions while accounting for the variability expected under the null hypothesis.\n\n3.5.2 Study Design\nConsider a survey aimed at estimating support for a new environmental policy in two regions of a country. Researchers randomly sample Region A (n = 150 individuals) and Region B (m = 200 individuals), recording whether each participant supports the policy. The main outcome of interest is whether an individual supports or does not support the policy.\n\n3.5.3 Data Collection and Wrangling\nWe will simulate a dataset to illustrate the concept:\n\n# Set seed\nset.seed(123)\n\n# Set sample sizes\nn_A &lt;- 150\nn_B &lt;- 200\n\n# Simulate true population proportions\np_A &lt;- 0.55\np_B &lt;- 0.45\n\n# Generate binary outcomes for survey responses\nsupport_A &lt;- rbinom(n_A, size = 1, prob = p_A)\nsupport_B &lt;- rbinom(n_B, size = 1, prob = p_B)\n\n# Create dataset\npolicy_data &lt;- data.frame(\n  region = c(rep(\"A\", n_A), rep(\"B\", n_B)),\n  support = c(support_A, support_B) )\n\n# Display first few rows\nhead(policy_data, 10)\n\n   region support\n1       A       1\n2       A       0\n3       A       1\n4       A       0\n5       A       0\n6       A       1\n7       A       1\n8       A       0\n9       A       0\n10      A       1\n\n\nThe dataset contains a region column and a support column indicating if the participant supports the policy.\n\n3.5.4 Exploratory Data Analysis\n\npolicy_summary &lt;- policy_data |&gt;\n  group_by(region) |&gt;\n  summarise(n = n(), supporters = sum(support), prop_support = mean(support))\npolicy_summary\n\n# A tibble: 2 × 4\n  region     n supporters prop_support\n  &lt;chr&gt;  &lt;int&gt;      &lt;int&gt;        &lt;dbl&gt;\n1 A        150         83        0.553\n2 B        200         85        0.425\n\n\nA simple bar plot can also help visualize the differences:\n\nggplot(policy_summary, aes(x = region, y = prop_support, fill = region)) +\n  geom_bar(stat = 'identity', alpha = 0.7) +\n  labs(\n    x = 'Region', \n    y = 'Proportion Supporting Policy',\n    title = 'Proportion of Policy Supporters by Region'\n    ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n3.5.5 Testing Settings\nWe use a significance level of \\(\\alpha = 0.05\\). The population parameters are:\n\n\\(p_1\\): true proportion of supporters in Region A.\n\\(p_2\\): true proportion of supporters in Region B.\n\nThe goal is to test if the difference \\(p_1 - p_2\\) is significantly different from zero.\n\n3.5.6 Hypothesis Definitions\nThe null and alternative hypotheses are:\n\\[H_0:p_1 = p_2 \\quad vs \\quad H_1:p_1 \\neq p_2\\]\n\n3.5.7 Test Flavour and Components\nThe two-sample z-test for proportions uses the pooled proportion:\n\\[\\hat p = \\frac{x_1 + x_2}{n_1 + n_2}\\]\n\\[z = \\frac{\\hat p_1 - \\hat p_2}{ \\hat p (1 - \\hat p) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\]\nUnder \\(H_0\\), this statistic approximately follows a standard normal distribution. The p-value is calculated as:\n\\[\\text{p-value} = 2 \\times Pr(Z \\ge |z|)\\]\n\n3.5.8 Inferential Conclusions\nIf the p-value is smaller than \\(\\alpha = 0.05\\), we reject \\(H_0\\), indicating a significant difference in policy support between the two regions. Otherwise, we fail to reject \\(H_0\\), suggesting no evidence of a difference.\n\n3.5.9 How to run the two-sample paired t-test in R and Python\n\n\n\nR Code\nPython Code\n\n\n\n\n# Using policy_summary to get counts and sample sizes\nx1 &lt;- policy_summary$supporters[policy_summary$region == \"A\"]\nx2 &lt;- policy_summary$supporters[policy_summary$region == \"B\"]\n\nn1 &lt;- policy_summary$n[policy_summary$region == \"A\"]\nn2 &lt;- policy_summary$n[policy_summary$region == \"B\"]\n\n# Run two-proportion z-test\nprop.test(c(x1, x2), c(n1, n2), correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(x1, x2) out of c(n1, n2)\nX-squared = 5.6557, df = 1, p-value = 0.0174\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.0233411 0.2333256\nsample estimates:\n   prop 1    prop 2 \n0.5533333 0.4250000 \n\n\n\n\n\nimport pandas as pd\nfrom statsmodels.stats.proportion import proportions_ztest\n\n\n\n\n\n3.5.10 Storytelling\n\nUnder dev",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tests for Two Continuous Population Mean</span>"
    ]
  },
  {
    "objectID": "book/chapter4-anova.html",
    "href": "book/chapter4-anova.html",
    "title": "4  ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "",
    "text": "4.1 The ANOVA Datasets\nThe simulated datasets discussed in this chapter relate to an experimental context known as A/B/n testing, which is an expansion of traditional A/B testing as previously discussed. In a typical A/B testing, participants are randomly assigned to one of two strategies, referred to as treatments: treatment \\(A\\) (the control treatment) and treatment \\(B\\) (the experimental treatment). This approach enables us to infer causation concerning the following inquiry:\nIn an ANOVA setting, the outcome must be continuous. The primary goal of A/B testing is to determine whether there is a statistically significant difference between treatments \\(A\\) and \\(B\\) concerning the outcome. Additionally, because we are conducting a proper randomized experiment, we can infer causation (which goes further than mere association), as treatment randomization enables us to get rid of the effect of further confounders.\nWhen conducting experiments with more than two treatments in A/B testing, the experiment is referred to as A/B/n testing. In this case, the “n” does not indicate the sample size; rather, it simply represents any number of additional treatments beyond treatments \\(A\\) and \\(B\\). With this clarification, we can now proceed with our simulated datasets.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA-related Tests for $k$ Continuous Population Means</span>"
    ]
  },
  {
    "objectID": "book/chapter4-anova.html#sec-ANOVA-datasets",
    "href": "book/chapter4-anova.html#sec-ANOVA-datasets",
    "title": "4  ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "",
    "text": "Will changing from treatment \\(A\\) to treatment \\(B\\) cause my outcome of interest, \\(Y\\), to increase (or decrease, if that is the case)?\n\n\n\n\nHeads-up on confounding!\n\n\nIn causal inference, confounding refers to the mixing of effects between the outcome of interest, denoted as \\(Y\\), the randomized factor \\(X\\) (which is a two-level factor in A/B testing, corresponding to treatment \\(A\\) and treatment \\(B\\)), and a third, uncontrollable factor known as the confounder \\(C\\). This confounder is associated with the factor \\(X\\) and independently affects the outcome \\(Y\\), as shown by Figure 4.2.\n\n\n\n\n\nFigure 4.2: Diagram depicting confounding (Rodríguez-Arelis et al. 2024).\n\n\n\n\n\n\n\nImage by Pabitra Kaity via Pixabay.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA-related Tests for $k$ Continuous Population Means</span>"
    ]
  },
  {
    "objectID": "book/chapter4-anova.html#sec-one-way-ANOVA",
    "href": "book/chapter4-anova.html#sec-one-way-ANOVA",
    "title": "4  ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "\n4.2 One-way ANOVA",
    "text": "4.2 One-way ANOVA\n\n4.2.1 Study Design\n\n4.2.2 Data Collection and Wrangling\n\n\nR Code\nPython Code\n\n\n\n# Loading dataset\nOneWay_ABn_customer_data &lt;- read.csv(\"data/ABn_customer_data_one_factor.csv\")\n\n# Showing the first 100 customers of the A/B/n testing\nhead(OneWay_ABn_customer_data, n = 100)\n\n\n# Importing library\nimport pandas as pd\n\n# Loading dataset\nOneWay_ABn_customer_data = pd.read_csv(\"data/ABn_customer_data_one_factor.csv\")\n\n# Showing the first 100 customers of the A/B/n testing\nprint(OneWay_ABn_customer_data.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\nTable 4.1: First 100 rows of our A/B/n simulated data with webpage design as a standalone experimental factor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.2: First 100 rows of our A/B/n simulated data with webpage design as a standalone experimental factor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code\nPython Code\n\n\n\n# Loading libraries\nlibrary(tidyverse)\nlibrary(rsample)\n\n# Seed for reproducibility\nset.seed(123)\n\n# Randomly splitting into training and testing sets by webpage_design\nOneWay_ABn_customer_data_splitting &lt;- initial_split(OneWay_ABn_customer_data,\n  prop = 0.5, strata = webpage_design\n)\n\n# Assigning data points to training and testing sets\nOneWay_ABn_training &lt;- training(OneWay_ABn_customer_data_splitting)\nOneWay_ABn_testing &lt;- testing(OneWay_ABn_customer_data_splitting)\n\n# Sanity check\ncat(\"Training shape:\", dim(OneWay_ABn_training), \"\\n\")\ncat(\"Testing shape:\", dim(OneWay_ABn_testing), \"\\n\\n\")\ncat(\"Training proportions:\\n\")\nprint(prop.table(table(OneWay_ABn_training$webpage_design)))\ncat(\"\\nTesting proportions:\\n\")\nprint(prop.table(table(OneWay_ABn_testing$webpage_design)))\n\n\n# Importing library\nfrom sklearn.model_selection import train_test_split\n\n# Seed for reproducibility\nrandom_state = 123\n\n# Randomly splitting into training and testing sets by webpage_design\nOneWay_ABn_training, OneWay_ABn_testing = train_test_split(\n    OneWay_ABn_customer_data,\n    test_size=0.5,\n    stratify=OneWay_ABn_customer_data[\"webpage_design\"],\n    random_state=random_state\n)\n\n# Sanity check\nprint(\"Training shape:\", OneWay_ABn_training.shape)\nprint(\"Testing shape:\", OneWay_ABn_testing.shape)\nprint(\"\\nTraining proportions:\")\nprint(OneWay_ABn_training[\"webpage_design\"].value_counts(normalize=True))\nprint(\"\\nTesting proportions:\")\nprint(OneWay_ABn_testing[\"webpage_design\"].value_counts(normalize=True))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTraining shape: 300 2 \n\n\nTesting shape: 300 2 \n\n\nTraining proportions:\n\n\n\n       D1        D2        D3 \n0.3333333 0.3333333 0.3333333 \n\n\n\nTesting proportions:\n\n\n\n       D1        D2        D3 \n0.3333333 0.3333333 0.3333333 \n\n\n\n\n\n\nTraining shape: (300, 2)\n\n\nTesting shape: (300, 2)\n\n\n\nTraining proportions:\n\n\nwebpage_design\nD2    0.333333\nD3    0.333333\nD1    0.333333\nName: proportion, dtype: float64\n\n\n\nTesting proportions:\n\n\nwebpage_design\nD1    0.333333\nD3    0.333333\nD2    0.333333\nName: proportion, dtype: float64\n\n\n\n\n\n\n4.2.3 Exploratory Data Analysis\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.3: Descriptive statistics of one-factor A/B/n testing via training set.\n\n# Computing summaries\nOneWay_summary_table &lt;- OneWay_ABn_training |&gt;\n  group_by(webpage_design) |&gt;\n  summarise(\n    mean_conversion = round(mean(conversion_score), 2),\n    sd_conversion = round(sd(conversion_score), 2),\n    n = n()\n  ) |&gt;\n  arrange(webpage_design)\nOneWay_summary_table\n\n# A tibble: 3 × 4\n  webpage_design mean_conversion sd_conversion     n\n  &lt;chr&gt;                    &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 D1                        60.1          2.9    100\n2 D2                        60.4          3.05   100\n3 D3                        60.3          3.05   100\n\n\n\n\n\n\n\n\nTable 4.4: Descriptive statistics of one-factor A/B/n testing via training set.\n\n# Importing library\nimport numpy as np\n\n# Importing R training set via R library reticulate\nOneWay_ABn_training = r.OneWay_ABn_training\n\nOneWay_summary_table = (\n    OneWay_ABn_training\n    .groupby(\"webpage_design\")\n    .agg(\n        mean_conversion=(\"conversion_score\", lambda x: round(np.mean(x), 2)),\n        sd_conversion=(\"conversion_score\", lambda x: round(np.std(x, ddof=1), 2)),\n        n=(\"conversion_score\", \"count\")\n    )\n    .reset_index()\n    .sort_values(\"webpage_design\")\n)\nOneWay_summary_table\n\n  webpage_design  mean_conversion  sd_conversion    n\n0             D1            60.11           2.90  100\n1             D2            60.35           3.05  100\n2             D3            60.28           3.05  100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Histograms of conversion score by webpage design via training data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Side-by-side boxplots of conversion score by webpage design via training data.\n\n\n\n\n\n4.2.4 Testing Settings\n\n4.2.5 Hypothesis Definitions\n\n4.2.6 Test Flavour and Components\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.5: One-way ANOVA: Conversion score by webpage design via testing set.\n\n# Loading library\nlibrary(broom)\n\n# Running one-way ANOVA\nOneWay_aov &lt;- aov(conversion_score ~ webpage_design,\n  data = OneWay_ABn_testing\n)\n\n# Displaying full one-way ANOVA table\ntidy(OneWay_aov) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 2 × 6\n  term              df   sumsq meansq statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 webpage_design     2    0.49   0.24      0.03    0.98\n2 Residuals        297 2901.     9.77     NA      NA   \n\n\n\n\n\n\n\n\nTable 4.6: One-way ANOVA: Conversion score by webpage design via testing set.\n\n# Importing library\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Importing R testing set via R library reticulate\nOneWay_ABn_testing = r.OneWay_ABn_testing\n\n# Running one-way ANOVA\nOneWay_model = ols('conversion_score ~ C(webpage_design)', data=OneWay_ABn_testing).fit()\nOneWay_aov = sm.stats.anova_lm(OneWay_model, typ=2)\n\n# Displaying full one-way ANOVA table\nOneWay_aov = OneWay_aov.round(2)\nOneWay_aov\n\n                    sum_sq     df     F  PR(&gt;F)\nC(webpage_design)     0.49    2.0  0.03    0.98\nResidual           2900.76  297.0   NaN     NaN\n\n\n\n\n\n\n\n\n4.2.7 Inferential Conclusions\n\n\n\n\n\n\n\nFigure 4.5: Diagnostic Q-Q plot for one-way ANOVA of conversion score by webpage design via residuals.\n\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.7: Shapiro–Wilks normality test for one-way ANOVA via residuals.\n\n# Extracting residuals\nOneWay_resid &lt;- residuals(OneWay_aov)\n\n# Running Shapiro–Wilk test\nOneWay_shapiro &lt;- shapiro.test(OneWay_resid)\n\n# Displaying outputs\ntidy(OneWay_shapiro) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 3\n  statistic p.value method                     \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1      0.99     0.3 Shapiro-Wilk normality test\n\n\n\n\n\n\n\n\nTable 4.8: Shapiro–Wilks normality test for one-way ANOVA via residuals.\n\n# Importing library\nfrom scipy.stats import shapiro\n\n# Extracting residuals\nOneWay_resid = OneWay_model.resid\n\n# Running Shapiro–Wilk tests\nOneWay_shapiro = shapiro(OneWay_resid)\n\n# Displaying outputs\nOneWay_shapiro = pd.DataFrame({\n    \"W Statistic\": [round(OneWay_shapiro.statistic, 2)],\n    \"p-value\": [round(OneWay_shapiro.pvalue, 2)]\n})\nOneWay_shapiro\n\n   W Statistic  p-value\n0         0.99      0.3\n\n\n\n\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.9: Levene’s test for homogeneity of variances for one-way ANOVA via testing data.\n\n# Loading library\nlibrary(car)\n\n# Running Levene test\nOneWay_levene &lt;- leveneTest(conversion_score ~ as.factor(webpage_design),\n  data = OneWay_ABn_testing\n)\n\n# Displaying outputs\ntidy(OneWay_levene) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 4\n  statistic p.value    df df.residual\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1      0.26    0.77     2         297\n\n\n\n\n\n\n\n\nTable 4.10: Levene’s test for homogeneity of variances for one-way ANOVA via testing data.\n\n# Importing library\nfrom scipy.stats import levene\n\n# Running Levene test\ngroups_OneWay = [group[\"conversion_score\"].values for _, group in OneWay_ABn_testing.groupby(\"webpage_design\")]\nstat1, p1 = levene(*groups_OneWay)\n\n# Displaying outputs\nOneWay_levene = pd.DataFrame({\n    \"Levene Statistic\": [round(stat1, 2)],\n    \"p-value\": [round(p1, 2)]\n})\nOneWay_levene\n\n   Levene Statistic  p-value\n0              0.26     0.77\n\n\n\n\n\n\n\n\n4.2.8 Storytelling",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA-related Tests for $k$ Continuous Population Means</span>"
    ]
  },
  {
    "objectID": "book/chapter4-anova.html#sec-two-way-ANOVA",
    "href": "book/chapter4-anova.html#sec-two-way-ANOVA",
    "title": "4  ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "\n4.3 Two-way ANOVA",
    "text": "4.3 Two-way ANOVA\n\n4.3.1 Study Design\n\n4.3.2 Data Collection and Wrangling\n\n\nR Code\nPython Code\n\n\n\n# Loading dataset\nTwoWay_ABn_customer_data &lt;- read.csv(\"data/ABn_customer_data_two_factors.csv\")\n\n# Showing the first 100 customers of the A/B/n testing\nhead(TwoWay_ABn_customer_data, n = 100)\n\n\n# Loading dataset\nTwoWay_ABn_customer_data = pd.read_csv(\"data/ABn_customer_data_two_factors.csv\")\n\n# Showing the first 100 customers of the A/B/n testing\nprint(TwoWay_ABn_customer_data(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\nTable 4.11: First 100 rows of our A/B/n simulated data with webpage design and discount framing as a experimental factors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.12: First 100 rows of our A/B/n simulated data with webpage design and discount framing as a experimental factors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code\nPython Code\n\n\n\n# Seed for reproducibility\nset.seed(123)\n\n# Create a combined stratification variable\nTwoWay_ABn_customer_data &lt;- TwoWay_ABn_customer_data |&gt;\n  mutate(strata = interaction(webpage_design, discount_framing))\n\n# Randomly splitting into training and testing sets by both factors\nTwoWay_ABn_customer_data_splitting &lt;- initial_split(\n  TwoWay_ABn_customer_data,\n  prop = 0.5,\n  strata = strata\n)\n\n# Assigning data points to training and testing sets\nTwoWay_ABn_training &lt;- training(TwoWay_ABn_customer_data_splitting)\nTwoWay_ABn_testing  &lt;- testing(TwoWay_ABn_customer_data_splitting)\n\n# Sanity check\ncat(\"Training shape:\", dim(TwoWay_ABn_training), \"\\n\")\ncat(\"Testing shape:\", dim(TwoWay_ABn_testing), \"\\n\\n\")\n\ncat(\"Training proportions:\\n\")\nprint(prop.table(table(TwoWay_ABn_training$webpage_design, TwoWay_ABn_training$discount_framing)))\n\ncat(\"\\nTesting proportions:\\n\")\nprint(prop.table(table(TwoWay_ABn_testing$webpage_design, TwoWay_ABn_testing$discount_framing)))\n\n\n# Seed for reproducibility\nrandom_state = 123\n\n# Create a combined stratification variable\nTwoWay_ABn_customer_data[\"strata\"] = (\n    TwoWay_ABn_customer_data[\"webpage_design\"].astype(str)\n    + \"_\"\n    + TwoWay_ABn_customer_data[\"discount_framing\"].astype(str)\n)\n\n# Randomly splitting into training and testing sets by both factors\nTwoWay_ABn_training, TwoWay_ABn_testing = train_test_split(\n    TwoWay_ABn_customer_data,\n    test_size=0.5,\n    stratify=TwoWay_ABn_customer_data[\"strata\"],\n    random_state=random_state\n)\n\n# Sanity check\nprint(\"Training shape:\", TwoWay_ABn_training.shape)\nprint(\"Testing shape:\", TwoWay_ABn_testing.shape)\n\nprint(\"\\nTraining proportions:\")\nprint(TwoWay_ABn_training.groupby([\"webpage_design\", \"discount_framing\"]).size().div(len(TwoWay_ABn_training)))\n\nprint(\"\\nTesting proportions:\")\nprint(TwoWay_ABn_testing.groupby([\"webpage_design\", \"discount_framing\"]).size().div(len(TwoWay_ABn_testing)))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTraining shape: 600 4 \n\n\nTesting shape: 600 4 \n\n\nTraining proportions:\n\n\n    \n          High       Low\n  D1 0.1666667 0.1666667\n  D2 0.1666667 0.1666667\n  D3 0.1666667 0.1666667\n\n\n\nTesting proportions:\n\n\n    \n          High       Low\n  D1 0.1666667 0.1666667\n  D2 0.1666667 0.1666667\n  D3 0.1666667 0.1666667\n\n\n\n\n\n\nTraining shape: (600, 4)\n\n\nTesting shape: (600, 4)\n\n\n\nTraining proportions:\n\n\nwebpage_design  discount_framing\nD1              High                0.166667\n                Low                 0.166667\nD2              High                0.166667\n                Low                 0.166667\nD3              High                0.166667\n                Low                 0.166667\ndtype: float64\n\n\n\nTesting proportions:\n\n\nwebpage_design  discount_framing\nD1              High                0.166667\n                Low                 0.166667\nD2              High                0.166667\n                Low                 0.166667\nD3              High                0.166667\n                Low                 0.166667\ndtype: float64\n\n\n\n\n\n\n4.3.3 Exploratory Data Analysis\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.13: Descriptive statistics of two-factor A/B/n testing via training set.\n\n# Computing summaries\nTwoWay_summary_table &lt;- TwoWay_ABn_training |&gt;\n  group_by(webpage_design, discount_framing) |&gt;\n  summarise(\n    mean_conversion = round(mean(conversion_score), 2),\n    sd_conversion = round(sd(conversion_score), 2),\n    n = n()\n  ) |&gt;\n  arrange(webpage_design, discount_framing)\nTwoWay_summary_table\n\n# A tibble: 6 × 5\n# Groups:   webpage_design [3]\n  webpage_design discount_framing mean_conversion sd_conversion     n\n  &lt;chr&gt;          &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 D1             High                        51.1          3.79   100\n2 D1             Low                         33.5          3.89   100\n3 D2             High                        62.1          3.73   100\n4 D2             Low                         47.8          4.04   100\n5 D3             High                        54.1          4.27   100\n6 D3             Low                         62.0          3.91   100\n\n\n\n\n\n\n\n\nTable 4.14: Descriptive statistics of two-factor A/B/n testing via training set.\n\n# Importing R training set via R library reticulate\nTwoWay_ABn_training = r.TwoWay_ABn_training\n\n# Computing summaries\nTwoWay_summary_table = (\n    TwoWay_ABn_training\n    .groupby([\"webpage_design\", \"discount_framing\"])\n    .agg(\n        mean_conversion=(\"conversion_score\", lambda x: round(np.mean(x), 2)),\n        sd_conversion=(\"conversion_score\", lambda x: round(np.std(x, ddof=1), 2)),\n        n=(\"conversion_score\", \"count\")\n    )\n    .reset_index()\n    .sort_values([\"webpage_design\", \"discount_framing\"])\n)\nTwoWay_summary_table\n\n  webpage_design discount_framing  mean_conversion  sd_conversion    n\n0             D1             High            51.10           3.79  100\n1             D1              Low            33.54           3.89  100\n2             D2             High            62.07           3.73  100\n3             D2              Low            47.83           4.04  100\n4             D3             High            54.14           4.27  100\n5             D3              Low            61.95           3.91  100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Histograms of conversion score by webpage design and discount framing via training data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: Side-by-side boxplots of conversion score by webpage design and discount framing via training data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Interaction plot of conversion score by webpage design and discount framing via training data.\n\n\n\n\n\n4.3.4 Testing Settings\n\n4.3.5 Hypothesis Definitions\n\n4.3.6 Test Flavour and Components\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.15: Two-way ANOVA: Conversion score by webpage design and discount framing via testing set.\n\n# Running two-way ANOVA\nTwoWay_aov &lt;- aov(conversion_score ~ webpage_design * discount_framing,\n  data = TwoWay_ABn_testing\n)\n\n# Displaying full two-way ANOVA table\nTwoWay_aov &lt;- tidy(TwoWay_aov) |&gt;\n  mutate(across(where(is.numeric) & !matches(\"p.value\"), ~ round(.x, 2)))\nTwoWay_aov\n\n# A tibble: 4 × 6\n  term                               df  sumsq  meansq statistic    p.value\n  &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 webpage_design                      2 26750. 13375.       817.  3.00e-171\n2 discount_framing                    1  8140.  8140.       497.  1.72e- 80\n3 webpage_design:discount_framing     2 18207.  9103.       556.  7.93e-137\n4 Residuals                         594  9723.    16.4       NA  NA        \n\n\n\n\n\n\n\n\nTable 4.16: Two-way ANOVA: Conversion score by webpage design and discount framing via testing set.\n\n# Importing R testing set via R library reticulate\nTwoWay_ABn_testing = r.TwoWay_ABn_testing\n\n# Running two-way ANOVA\nTwoWay_model = ols('conversion_score ~ C(webpage_design) * C(discount_framing)', data=TwoWay_ABn_testing).fit()\nTwoWay_aov = sm.stats.anova_lm(TwoWay_model, typ=2)\n\n# Round all numeric columns except p-values\ncols_to_round = [c for c in TwoWay_aov.columns if c != \"PR(&gt;F)\"]\nTwoWay_aov[cols_to_round] = TwoWay_aov[cols_to_round].round(2)\n\n# Displaying full two-way ANOVA table\nTwoWay_aov\n\n                                         sum_sq     df       F         PR(&gt;F)\nC(webpage_design)                      26749.51    2.0  817.05  3.002041e-171\nC(discount_framing)                     8139.80    1.0  497.25   1.718558e-80\nC(webpage_design):C(discount_framing)  18206.69    2.0  556.12  7.926552e-137\nResidual                                9723.46  594.0     NaN            NaN\n\n\n\n\n\n\n\n\n4.3.7 Inferential Conclusions\n\n\n\n\n\n\n\nFigure 4.9: Diagnostic Q-Q plot for two-way ANOVA of conversion score by webpage design and discount framing via residuals.\n\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.17: Shapiro–Wilks normality test for two-way ANOVA via residuals.\n\n# Extracting residuals\nTwoWay_resid &lt;- residuals(TwoWay_aov)\n\n# Running Shapiro–Wilk test\nTwoWay_shapiro &lt;- shapiro.test(TwoWay_resid)\n\n# Displaying outputs\ntidy(TwoWay_shapiro) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 3\n  statistic p.value method                     \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n1         1    0.19 Shapiro-Wilk normality test\n\n\n\n\n\n\n\n\nTable 4.18: Shapiro–Wilks normality test for two-way ANOVA via residuals.\n\n# Extracting residuals\nTwoWay_resid = TwoWay_model.resid\n\n# Running Shapiro–Wilk tests\nTwoWay_shapiro = shapiro(TwoWay_resid)\n\n# Displaying outputs\nTwoWay_shapiro = pd.DataFrame({\n    \"W Statistic\": [round(TwoWay_shapiro.statistic, 2)],\n    \"p-value\": [round(TwoWay_shapiro.pvalue, 2)]\n})\nTwoWay_shapiro\n\n   W Statistic  p-value\n0          1.0     0.19\n\n\n\n\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTable 4.19: Levene’s test for homogeneity of variances for two-way ANOVA via testing data.\n\n# Loading library\nlibrary(car)\n\n# Running Levene test\nTwoWay_levene &lt;- leveneTest(conversion_score ~ as.factor(webpage_design) * as.factor(discount_framing),\n  data = TwoWay_ABn_testing\n)\n\n# Displaying outputs\ntidy(TwoWay_levene) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 4\n  statistic p.value    df df.residual\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1      1.92    0.09     5         594\n\n\n\n\n\n\n\n\nTable 4.20: Levene’s test for homogeneity of variances for two-way ANOVA via testing data.\n\n# Running Levene test\nTwoWay_ABn_testing[\"group\"] = TwoWay_ABn_testing[\"webpage_design\"].astype(str) + \"_\" + TwoWay_ABn_testing[\"discount_framing\"].astype(str)\ngroups_TwoWay = [group[\"conversion_score\"].values for _, group in TwoWay_ABn_testing.groupby(\"group\")]\nstat1, p1 = levene(*groups_TwoWay)\n\n# Displaying outputs\nTwoWay_levene = pd.DataFrame({\n    \"Levene Statistic\": [round(stat1, 2)],\n    \"p-value\": [round(p1, 2)]\n})\nTwoWay_levene\n\n   Levene Statistic  p-value\n0              1.92     0.09\n\n\n\n\n\n\n\n\n4.3.8 Storytelling",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA-related Tests for $k$ Continuous Population Means</span>"
    ]
  },
  {
    "objectID": "book/chapter4-anova.html#sec-chapter-4-summary",
    "href": "book/chapter4-anova.html#sec-chapter-4-summary",
    "title": "4  ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "\n4.4 Chapter Summary",
    "text": "4.4 Chapter Summary\n\n\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers. Edinburgh Oliver & Boyd.\n\n\nKennedy-Shaffer, Lee. 2024. “Teaching the Difficult Past of Statistics to Improve the Future.” Journal of Statistics and Data Science Education 32 (1): 108–19. https://doi.org/10.1080/26939169.2023.2224407.\n\n\nMacKenzie, D. A. 1981. Statistics in Britain, 1865-1930: The Social Construction of Scientific Knowledge. Edinburgh University Press.\n\n\nRodríguez-Arelis, G. Alexi, Daniel Chen, Benjamin Bloem-Redd, Tiffany Timbers, and Vincenzo Coia. 2024. “DSCI 554: Experimentation and Causal Inference.” https://ubc-mds.github.io/DSCI_554_exper-causal-inf/README.html.\n\n\nTabery, James, and Sahotra Sarkar. 2015. “R. A. Fisher, Lancelot Hogben, and the ‘Competition’ for the Chair of Social Biology at the London School of Economics in 1930: Correcting the Legend.” Notes and Records: The Royal Society Journal of the History of Science 69 (4): 437–46. https://doi.org/10.1098/rsnr.2014.0065.\n\n\nTarran, Brian. 2020. “Award ‘Retired’ Over R. A. Fisher’s Links to Eugenics.” Significance 17 (4): 2–3. https://doi.org/10.1111/1740-9713.01411.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA-related Tests for $k$ Continuous Population Means</span>"
    ]
  },
  {
    "objectID": "book/chapter5-chi-square.html",
    "href": "book/chapter5-chi-square.html",
    "title": "\n5  Chapter 5: Chi-square Tests\n",
    "section": "",
    "text": "5.1 Hypotheses\nThe chi-square test is a non-parametric statistical test used for categorical data. Chi-square tests are used to make statistical inferences about categorical variables. Depending on the research question and the number of categorical variables, the specific chi-square test can differ, so it is important to identify the correct scenario for application.\nThe core idea behind chi-square tests is to compare observed counts with expected counts based on a population or theoretical distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Chi-square Tests</span>"
    ]
  },
  {
    "objectID": "book/chapter5-chi-square.html#hypotheses",
    "href": "book/chapter5-chi-square.html#hypotheses",
    "title": "\n5  Chapter 5: Chi-square Tests\n",
    "section": "",
    "text": "Null hypothesis (\\(H_0\\)): The observed counts (\\(O_i\\)) and expected counts (\\(E_i\\)) are equal.\n\n\nAlternative hypothesis (\\(H_1\\)): The observed counts (\\(O_i\\)) and expected counts (\\(E_i\\)) are not equal.\n\n\n5.1.1 Test Statistic\nThe chi-square test statistic is calculated as:\n\\[\n\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\nUnder the null hypothesis, this statistic follows a chi-square distribution with degrees of freedom that depend on the type of test being performed.\nIntuitively:\n- If the observed and expected counts are close, the test statistic is small and not significant.\n- If the observed counts differ substantially from the expected counts, the test statistic is large and significant, indicating a meaningful difference between the sample and the expected distribution.\n\n5.1.2 Assumptions\n\nObservations are independent.\n\nExpected counts are sufficiently large (typically greater than 5).\n\nThe chi-square distribution is defined by its degrees of freedom, which vary depending on the test design and the number of categories.\nThere are two main types of chi-square tests:\n\n\nGoodness-of-Fit Test\n\nCompares observed frequencies to expected frequencies for a single categorical variable.\n\nExample: Do the proportions of penguin species in the dataset match a hypothesized distribution (45% Adelie, 35% Gentoo, 20% Chinstrap)?\n\n\n\n\nTest of Independence (or Homogeneity)\n\nTests whether two categorical variables are independent.\n\nExample: Is penguin species independent of the island they were observed on?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Chi-square Tests</span>"
    ]
  },
  {
    "objectID": "book/chapter5-chi-square.html#i-chi-square-goodness-of-fit-test",
    "href": "book/chapter5-chi-square.html#i-chi-square-goodness-of-fit-test",
    "title": "\n5  Chapter 5: Chi-square Tests\n",
    "section": "\n5.2 (i) Chi-Square Goodness-of-Fit Test",
    "text": "5.2 (i) Chi-Square Goodness-of-Fit Test\nThis test examines whether the distribution of a categorical variable matches a hypothesized distribution.\n\n5.2.1 Hypotheses\n\n\n\\(H_0\\): The observed distribution matches the expected distribution.\n\n\n\\(H_1\\): The observed distribution does not match the expected distribution.\n\n5.2.2 Study Design\nSuppose we hypothesize that the penguin species occur in the following proportions:\n\nAdelie: 45%\n\nGentoo: 35%\n\nChinstrap: 20%\n\nWe want to test whether the observed species distribution in the Palmer Penguins dataset matches these proportions.\n\n5.2.3 Data Collection & Wrangling\nHere, we first load the Palmer Penguins dataset and remove any missing values to ensure complete observations. Next, we count how many penguins of each species are observed and calculate the expected counts based on our hypothesized proportions. These observed and expected frequencies will later be compared using the Chi-square test statistic.\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# Load dataset\npenguins_clean &lt;- penguins %&gt;% \n  drop_na()\n\n# Count observed species\nobserved_counts &lt;- table(penguins_clean$species)\nprint(\"Observed counts:\")\n\n[1] \"Observed counts:\"\n\nprint(observed_counts)\n\n\n   Adelie Chinstrap    Gentoo \n      146        68       119 \n\n# Expected proportions\nexpected_props &lt;- c(0.45, 0.35, 0.20)\n\n# Convert to expected counts\nn &lt;- nrow(penguins_clean)\nexpected_counts &lt;- expected_props * n\nprint(\"Expected counts (based on proportions):\")\n\n[1] \"Expected counts (based on proportions):\"\n\nprint(expected_counts)\n\n[1] 149.85 116.55  66.60\n\n\n\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Load dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\n\n# Count observed species\nobserved_counts = penguins_clean[\"species\"].value_counts()\nprint(\"Observed counts:\")\n\nObserved counts:\n\nprint(observed_counts)\n\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64\n\n# Expected proportions\nexpected_props = [0.45, 0.35, 0.20]\n\n# Convert to expected counts\nn = len(penguins_clean)\nexpected_counts = [p * n for p in expected_props]\n\nprint(\"\\nExpected counts (based on proportions):\")\n\n\nExpected counts (based on proportions):\n\nprint(expected_counts)\n\n[149.85, 116.55, 66.60000000000001]\n\n\n\n\n\n\n5.2.4 Exploratory Data Analysis (EDA)\nBefore running the statistical test, it’s helpful to visualize the observed counts. A simple bar plot allows us to see whether any species appear more or less frequent than expected, which helps build intuition about potential differences.\n\n\nR Code\nPython Code\n\n\n\n\n# Bar plot of observed counts\nbarplot(\n  observed_counts,\n  col = \"skyblue\",\n  border = \"black\",\n  main = \"Observed Penguin Species Counts\",\n  xlab = \"Species\",\n  ylab = \"Count\"\n)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nobserved_counts.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Observed Penguin Species Counts\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.2.5 Implementation\nNow we apply the Chi-square goodness-of-fit test, comparing observed and expected counts. The test statistic measures how far the observed frequencies deviate from the expected ones. A large value suggests that the observed distribution differs significantly from what was hypothesized.\n\n\nR Code\nPython Code\n\n\n\n\n# Chi-square goodness-of-fit test\nchi2_test &lt;- chisq.test(\n  x = observed_counts,\n  p = expected_props\n)\nprint(chi2_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed_counts\nX-squared = 61.551, df = 2, p-value = 4.31e-14\n\n\n\n\n\nfrom scipy.stats import chisquare\n\nchi2_stat, p_value = chisquare(\n    f_obs=observed_counts,\n    f_exp=expected_counts\n)\nprint(f\"Chi-square = {chi2_stat:.3f}, p = {p_value:.4f}\")\n\nChi-square = 0.180, p = 0.9140\n\n\n\n\n\n\n5.2.6 Interpretation\nIf \\(p &lt; 0.05\\): Reject \\(H_0\\) → the species distribution differs significantly from the expected proportions.\nIf \\(p \\ge 0.05\\): Fail to reject \\(H_0\\) → no significant difference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Chi-square Tests</span>"
    ]
  },
  {
    "objectID": "book/chapter5-chi-square.html#ii-chi-square-test-of-independence",
    "href": "book/chapter5-chi-square.html#ii-chi-square-test-of-independence",
    "title": "\n5  Chapter 5: Chi-square Tests\n",
    "section": "\n5.3 (ii) Chi-Square Test of Independence",
    "text": "5.3 (ii) Chi-Square Test of Independence\nThis test evaluates whether two categorical variables are independent.\n\n5.3.1 Hypotheses\n\\(H_0\\): The two categorical variables are independent.\n\\(H_1\\): The two categorical variables are not independent (they are associated).\n\n5.3.2 Study Design\nWe want to test whether penguin species is independent of island — in other words, whether certain species are more common on certain islands.\n\n5.3.3 Data Collection & Wrangling\nWe create a contingency table summarizing the counts of species across islands. This table serves as the basis for calculating expected frequencies and testing independence.\n\n\nR Code\nPython Code\n\n\n\n\n# Contingency table: species vs island\ncontingency_table &lt;- table(\n  penguins_clean$species,\n  penguins_clean$island\n)\nprint(\"Contingency table:\")\n\n[1] \"Contingency table:\"\n\nprint(contingency_table)\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    55        47\n  Chinstrap      0    68         0\n  Gentoo       119     0         0\n\n\n\n\n\n# Create contingency table: species vs island\ncontingency_table = pd.crosstab(\n    penguins_clean[\"species\"], \n    penguins_clean[\"island\"]\n)\nprint(\"Contingency table:\")\n\nContingency table:\n\nprint(contingency_table)\n\nisland     Biscoe  Dream  Torgersen\nspecies                            \nAdelie         44     55         47\nChinstrap       0     68          0\nGentoo        119      0          0\n\n\n\n\n\n\n5.3.4 Exploratory Data Analysis (EDA)\nVisualizing the contingency table helps us see any apparent association between species and island. If bars differ noticeably in height across islands, it hints that species distribution might depend on island location.\n\n\nR Code\nPython Code\n\n\n\n\n# Stacked bar plot of species by island\nbarplot(\n  contingency_table,\n  col = c(\"orange\", \"skyblue\", \"green\"),\n  border = \"black\",\n  main = \"Penguin Species by Island\",\n  xlab = \"Species\",\n  ylab = \"Count\",\n  legend.text = TRUE\n)\n\n\n\n\n\n\n\n\n\n\ncontingency_table.plot(\n    kind=\"bar\", stacked=True, edgecolor=\"black\"\n)\nplt.title(\"Penguin Species by Island\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3.5 Implementation\nWe now apply the Chi-square test of independence to evaluate whether the distribution of species differs by island. If the test is significant, it suggests a relationship between the two categorical variables.\n\n\nR Code\nPython Code\n\n\n\n\n# Chi-square test of independence\nchi2_indep &lt;- chisq.test(contingency_table)\nprint(chi2_indep)\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 284.59, df = 4, p-value &lt; 2.2e-16\n\n\n\n\n\nfrom scipy.stats import chi2_contingency\n\nchi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n\nprint(f\"Chi-square = {chi2_stat:.3f}, df = {dof}, p = {p_value:.4f}\")\n\nChi-square = 284.590, df = 4, p = 0.0000\n\n\n\n\n\n\n5.3.6 Interpretation\nIf \\(p &lt; 0.05\\): Reject \\(H_0\\) → species and island are not independent (association exists).\nIf \\(p \\ge 0.05\\): Fail to reject \\(H_0\\) → species and island appear independent.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Chi-square Tests</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Fisher, R. A. 1925. Statistical Methods for Research Workers.\nEdinburgh Oliver & Boyd.\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nKennedy-Shaffer, Lee. 2024. “Teaching the Difficult Past of\nStatistics to Improve the Future.” Journal of Statistics and\nData Science Education 32 (1): 108–19. https://doi.org/10.1080/26939169.2023.2224407.\n\n\nLohr, S. L. 2021. Sampling: Design and Analysis. Chapman;\nHall/CRC. https://doi.org/https://doi.org/10.1201/9780429298899.\n\n\nMacKenzie, D. A. 1981. Statistics in Britain, 1865-1930: The Social\nConstruction of Scientific Knowledge. Edinburgh University Press.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria: R Foundation for Statistical\nComputing. https://www.R-project.org/.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete\nGuide. 1st ed. San Francisco, CA: No Starch Press. https://www.statisticsdonewrong.com/index.html.\n\n\nRodríguez-Arelis, G. Alexi, Daniel Chen, Benjamin Bloem-Redd, Tiffany\nTimbers, and Vincenzo Coia. 2024. “DSCI 554: Experimentation and\nCausal Inference.” https://ubc-mds.github.io/DSCI_554_exper-causal-inf/README.html.\n\n\nTabery, James, and Sahotra Sarkar. 2015. “R. A. Fisher, Lancelot\nHogben, and the ‘Competition’ for the Chair of Social\nBiology at the London School of Economics in 1930: Correcting the\nLegend.” Notes and Records: The Royal Society Journal of the\nHistory of Science 69 (4): 437–46. https://doi.org/10.1098/rsnr.2014.0065.\n\n\nTarran, Brian. 2020. “Award\n‘Retired’ Over R. A. Fisher’s Links to\nEugenics.” Significance 17 (4): 2–3. https://doi.org/10.1111/1740-9713.01411.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas:\nPandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nTukey, John W. 1962. “The Future of Data\nAnalysis.” The Annals of Mathematical Statistics\n33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-greek-alphabet.html",
    "href": "book/A-greek-alphabet.html",
    "title": "Appendix A — Greek Alphabet",
    "section": "",
    "text": "In the context of hypothesis testing in statistics, mathematical notation serves an important purpose: it distinguishes between unknown population parameters and sample-based estimates. A key convention in this framework is the use of Greek letters to represent population parameters. For example, \\(\\mu\\) represents the population mean, \\(\\sigma\\) denotes the population standard deviation, and \\(\\pi\\) signifies the population proportion. In a frequentist framework, these letters indicate unknown and fixed parameters that characterize the entire population of interest. Since hypothesis testing primarily focuses on making inferential conclusions about these parameters, we will consistently use this notation throughout all chapters of this mini-book.\n\n\nHeads-up on the use of \\(\\pi\\)!\n\n\nIn this textbook, unless otherwise stated, the letter \\(\\pi\\) will represent a population parameter and not the mathematical constant \\(3.141592...\\)\n\n\n\n\n\nImage by meineresterampe via Pixabay.\n\n\nIt is important to remember that each hypothesis test involves formulating null and alternative hypotheses regarding the population parameter(s) of interest. For instance, when inferring a population mean \\(\\mu\\), the null hypothesis in a two-sided one-sample \\(t\\)-test might indicate that this mean is equal to 100 (i.e., \\(\\text{$H_0$: } \\mu = 100\\)), while the alternative hypothesis will indicate that the mean is not equal to 100 (i.e., \\(\\text{$H_1$: } \\mu \\neq 100\\)). Using Greek letters to define our hypotheses helps frame the entire test clearly and precisely. If at any point throughout the chapters this notation feels unfamiliar, we recommend consulting Table A.1 as a reference resource. Regular exposure to this notation will enhance your conceptual clarity when performing statistical inference.\n\n\n\nTable A.1: Greek alphabet composed of 24 letters, from left to right you can find the name of letter along with its corresponding uppercase and lowercase forms.\n\n\n\n\n\nName\nUppercase\nLowercase\n\n\n\n\nAlpha\n\\(\\text{A}\\)\n\\(\\alpha\\)\n\n\nBeta\n\\(\\text{B}\\)\n\\(\\beta\\)\n\n\nGamma\n\\(\\Gamma\\)\n\\(\\gamma\\)\n\n\nDelta\n\\(\\Delta\\)\n\\(\\delta\\)\n\n\nEpsilon\n\\(\\text{E}\\)\n\\(\\epsilon\\)\n\n\nZeta\n\\(\\text{Z}\\)\n\\(\\zeta\\)\n\n\nEta\n\\(\\text{H}\\)\n\\(\\eta\\)\n\n\nTheta\n\\(\\Theta\\)\n\\(\\theta\\)\n\n\nIota\n\\(\\text{I}\\)\n\\(\\iota\\)\n\n\nKappa\n\\(\\text{K}\\)\n\\(\\kappa\\)\n\n\nLambda\n\\(\\Lambda\\)\n\\(\\lambda\\)\n\n\nMu\n\\(\\text{M}\\)\n\\(\\mu\\)\n\n\nNu\n\\(\\text{N}\\)\n\\(\\nu\\)\n\n\nXi\n\\(\\Xi\\)\n\\(\\xi\\)\n\n\nO\n\\(\\text{O}\\)\n\\(\\text{o}\\)\n\n\nPi\n\\(\\Pi\\)\n\\(\\pi\\)\n\n\nRho\n\\(\\text{P}\\)\n\\(\\rho\\)\n\n\nSigma\n\\(\\Sigma\\)\n\\(\\sigma\\)\n\n\nTau\n\\(\\text{T}\\)\n\\(\\tau\\)\n\n\nUpsilon\n\\(\\Upsilon\\)\n\\(\\upsilon\\)\n\n\nPhi\n\\(\\Phi\\)\n\\(\\phi\\)\n\n\nChi\n\\(\\text{X}\\)\n\\(\\chi\\)\n\n\nPsi\n\\(\\Psi\\)\n\\(\\psi\\)\n\n\nOmega\n\\(\\Omega\\)\n\\(\\omega\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Greek Alphabet</span>"
    ]
  },
  {
    "objectID": "book/B-simulated-datasets.html",
    "href": "book/B-simulated-datasets.html",
    "title": "Appendix B — Simulated Datasets",
    "section": "",
    "text": "B.1 \\(t\\)-test for Paired Samples Dataset\nWhile we have made an effort to include real datasets wherever possible in this mini-book, we will utilize simulated data to demonstrate the application of the test workflow from Chapter 1 for certain hypothesis testings. This simulation-based approach allows us to have suitable datasets to illustrate how each test’s modelling assumptions must be satisfied to ensure that we deliver robust inferential conclusions to our stakeholders. Therefore, this appendix will explain the generative modelling process used to create these simulated datasets.\nFor each of the datasets listed below, besides providing the simulation code, we will elaborate on the dataset context along with the relevant equations (if necessary) that generate this data.\nThis dataset is used in Chapter 3, more specifically in Section 3.4, to demonstrate the \\(t\\)-test for paired samples via a hypothetical scenario from medical research. Consider a clinical investigation examining whether an innovative medication can decrease low-density lipoprotein (LDL) cholesterol levels in adults diagnosed with hypercholesterolemia, a condition characterized by elevated cholesterol levels in the blood. This matter can increase the risk of heart problems because cholesterol can build up in blood vessels and block blood flow.\nThis simulated study monitors a group of \\(n = 120\\) participants over time, recording LDL cholesterol concentrations prior to treatment and again after an eight-week course of the drug. The outcome of interest is the LDL cholesterol measurement (in \\(\\text{mg/dL}\\)), a widely used biomarker for cardiovascular risk. Since each participant provides two observations (i.e., one before and one after treatment), the data are naturally paired (the same individual is measured twice), allowing the analysis to focus on changes within individuals rather than differences between unrelated groups.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated Datasets</span>"
    ]
  },
  {
    "objectID": "book/B-simulated-datasets.html#t-test-for-paired-samples-dataset",
    "href": "book/B-simulated-datasets.html#t-test-for-paired-samples-dataset",
    "title": "Appendix B — Simulated Datasets",
    "section": "",
    "text": "Image by Fakhruddin Memon via Pixabay.\n\n\n\n\nHeads-up on the use of this dataset!\n\n\nThis data is simulated and by no means should be considered for medical research or advice.\n\n\n\n\nB.1.1 Generative Modelling Process\nThe simulated dataset reflects the following study characteristics:\n\n\nBaseline LDL levels: Participants in the simulated population have a mean pre-treatment LDL cholesterol level of \\(\\mu = 160 \\text{ mg/dL}\\), with a standard deviation of \\(\\sigma = 5 \\text{ mg/dL}\\) to account for individual variation.\n\nExpected treatment effect: On average, in this simulated population of participants, LDL cholesterol levels decline by \\(\\Delta = 2.5 \\text{ mg/dL}\\) after the intervention using the new medication.\n\nMeasurement variation: Biological variability and measurement error introduce random fluctuations in LDL values, even within the same patient.\n\nSince formally, for the \\(i\\)th participant, the LDL cholesterol level before treatment is assumed to follow a Normal distribution with \\(\\mu = 160 \\text{ mg/dL}\\) and \\(\\sigma = 5 \\text{ mg/dL}\\), we can express the following:\n\\[\\text{LDL}_{\\text{before}, i} \\sim \\text{Normal}(\\mu = 160, \\sigma^2= 5^2).\\]\nThen, again for the \\(i\\)th participant, the LDL cholesterol level after the treatment is another random variable that is a combination of three components as follows:\n\\[\\text{LDL}_{\\text{after}, i} = \\text{LDL}_{\\text{before}, i} - \\Delta + \\varepsilon_i,\\]\nwhere:\n\n\n\\(\\Delta = 2.5 \\text{ mg/dL}\\) is the average decrease in LDL cholesterol level due to this new treatment in the simulated population.\n\n\\(\\varepsilon_i \\sim \\text{Normal}(0, 1)\\) represents additional variation specific to the after-treatment measurement for the \\(i\\)th participant. This is what we already defined as the measurement variation.\n\nB.1.2 Code\nLet us check the corresponding code to simulate this data. We are simulating 120 participants in this study, which gives us a sample size of \\(n = 120\\). Additionally, the simulation will utilize the {numpy} (Harris et al. 2020) and {pandas} libraries in Python. The final data frame will be stored in cholesterol_data, which will have the following columns: patient_id, LDL_before, and LDL_after.\n\n\nR Code\nPython Code\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Set number of participants\nn_patients &lt;- 120   \n\n# Set the average decrease in LDL cholesterol levels due to new treatment\nDelta &lt;- 2.5\n\n# Step 1: Generate LDL cholesterol levels before treatment\nldl_before &lt;- rnorm(n_patients, mean = 160, sd = 5)\n\n# Step 2: Generate LDL cholesterol levels after treatment\nldl_after &lt;- ldl_before - Delta + rnorm(n_patients, mean = 0, sd = 1)\n\n# Create final dataset\ncholesterol_data &lt;- data.frame(\n  patient_id = 1:n_patients,\n  LDL_before = round(ldl_before, 1),\n  LDL_after  = round(ldl_after, 1)\n)\n\n# Showing the first 20 participants \nhead(cholesterol_data, 20)\n\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Set number of participants\nn_patients = 120\n\n# Set the average decrease in LDL cholesterol levels due to new treatment\nDelta = 2.5\n\n# Step 1: Generate LDL cholesterol levels before treatment\nldl_before = np.random.normal(loc=160, scale=5, size=n_patients)\n\n# Step 2: Generate LDL cholesterol levels after treatment\nldl_after = ldl_before - Delta + np.random.normal(loc=0, scale=1, size=n_patients)\n\n# Create final dataset\ncholesterol_data = pd.DataFrame({\n    \"patient_id\": np.arange(1, n_patients + 1),\n    \"LDL_before\": np.round(ldl_before, 1),\n    \"LDL_after\": np.round(ldl_after, 1)\n})\n\n# Showing the first 20 participants \nprint(cholesterol_data.head(20))\n\n\n\n\n\nR Output\nPython Output",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated Datasets</span>"
    ]
  },
  {
    "objectID": "book/B-simulated-datasets.html#anova-datasets",
    "href": "book/B-simulated-datasets.html#anova-datasets",
    "title": "Appendix B — Simulated Datasets",
    "section": "\nB.2 ANOVA Datasets",
    "text": "B.2 ANOVA Datasets\nThese datasets are used in Chapter 4 to elaborate on analysis of variance (ANOVA) and pertain to an experimental context. Suppose a data-driven marketing team at a well-known tech company, which operates a global online store, is conducting two different A/B/n testings aimed at increasing the customer conversion score (i.e., the outcome). In these experiments, the customer conversion score is defined as a unitless and standardized engagement index. This index combines various elements, such as clicks, time spent on the webpage, and the probability of making a purchase, with a baseline mean set at \\(50\\). This score measures customer responsiveness on the online store: the higher the score, the greater the customer responsiveness.\n\n\nImage by Pabitra Kaity via Pixabay.\n\n\nB.2.1 One-way ANOVA\nLet us start by outlining the generative modelling process for a one-way ANOVA experiment. In this context, we will simulate a dataset that represents a continuous response variable (in this case, the customer conversion score) affected by a single experimental factor, specifically the design of a webpage. This setup allows us to assess whether the average response systematically varies across different design options.\nGenerative Modelling Process\nThe experiment involves webpage design as a controllable factor determined by the experimenter. This factor includes three different layouts: \\(D_1\\) (the current layout), \\(D_2\\) (a new layout), and \\(D_3\\) (another new layout), making it a three-level factor. This study will be conducted as a full factorial experiment characterized by the following elements:\n\nOne factor, namely, webpage design.\nThere are \\(3\\) treatments (i.e., the above three factor levels), which classifies this study as A/B/n testing.\nWe will simulate 200 customers (i.e., replicates) per treatment in our final dataset.\nThe outcome variable \\(Y\\) is the customer conversion score, which has been previously explained.\n\nOur data structure will be an additive model conceptually depicted as:\n\\[\n\\begin{align*}\n\\text{Outcome} &= \\text{First Main Effect} + \\text{Random Error}.\n\\end{align*}\n\\tag{B.1}\\]\nThen, for the data generation process, let \\(Y_{i,k}\\) represent the customer conversion score for the \\(k\\)th replicate of the treatment related to the \\(i\\)th webpage design level. Equation B.1 is translated as:\n\\[\nY_{i,k} = \\alpha_i + \\varepsilon_{i,k},\n\\tag{B.2}\\]\nwhere\n\n\n\\(\\alpha_i\\) is the fixed main effect corresponding to the \\(i\\)th level of webpage design for \\(i = D_1, D_2, D_3\\);\n\n\\(\\varepsilon_{i,k}\\) is the random error associated to each \\(Y_{i,k}\\), capturing the variability and measurement error that introduces randomness in the response \\(Y_{i,k}\\).\n\nIn this case, Equation B.2 breaks down the outcome on the right-hand side into two additive components, which serve as the foundation for how ANOVA models the data. Unlike the random error \\(\\varepsilon_{i,k}\\), the term \\(\\alpha_i\\) is assumed to be fixed within the data-generating process, as we are using a frequentist approach. Since \\(\\varepsilon_{i,k}\\) is random, we will assume it follows a Normal distribution with a mean of \\(0\\) and a variance of \\(\\sigma^2\\), which is another fixed parameter in the simulation:\n\\[\n\\varepsilon_{i,k} \\sim \\text{Normal}(0, \\sigma^2).\n\\] In terms of our simulation, imagine you have a population of customers with the following fixed parameters:\n\nA vector of webpage design effects (i.e., the main effect)\n\n\\[\\boldsymbol{\\alpha} =\n\\begin{bmatrix} \\alpha_{D_1} \\\\ \\alpha_{D_2} \\\\ \\alpha_{D_3} \\end{bmatrix} =\n\\begin{bmatrix} 60 \\\\ 60 \\\\ 60 \\end{bmatrix}.\\]\n\nAn overall variance\n\n\\[\n\\sigma^2 = 10.\n\\]\nThe rationale for this simulation setup lies in constructing a baseline generative process that explicitly embodies the null hypothesis of a one-way ANOVA. By assigning identical mean effects to all three webpage designs, we model a population in which, on average, each webpage design yields the same customer conversion performance (that is, all group means are equal as specified in the vector \\(\\boldsymbol{\\alpha}\\)). Consequently, any observed differences among sample means arise purely from random sampling variability, not from systematic treatment effects.\nIn addition, this generative design encodes two fundamental assumptions of the ANOVA framework: normality and homoscedasticity. The normality assumption on the random component \\(\\varepsilon_{i,k}\\) ensures that each group’s response distribution is symmetric and bell-shaped, supporting the validity of parametric inference represented by the \\(F\\)-test in ANOVA. The homoscedasticity assumption (expressed here as a constant variance \\(\\sigma^2 = 10\\)) asserts that the variability in customer responses is identical across all webpage designs. This condition guarantees that any detected mean differences can be attributed to true design effects rather than unequal levels of random noise.\nCode\nLet us move to the corresponding code to simulate this data. Recall we are simulating 200 customers for each one of the three treatments, which will give us an overall sample size of \\(n = 600\\). Furthermore, note that Python additionally uses the {numpy} and {pandas} libraries. The final data frame will be stored in ABn_customer_data_one_factor whose columns will be webpage_design and conversion_score.\n\n\nR Code\nPython Code\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Factor levels and sampled customers per treatment\nwebpage_design_levels &lt;- c(\"D1\", \"D2\", \"D3\")\nn_per_treatment &lt;- 200\n\n# Population fixed additive parameters\nalpha &lt;- c(60, 60, 60)\n\n# Simulating data\ndata_list &lt;- list()\nfor (i in 1:3) {\n  mean_i &lt;- alpha[i]\n  y &lt;- rnorm(n_per_treatment, mean = mean_i, sd = sqrt(10))\n  df_i &lt;- data.frame(\n    webpage_design = as.factor(webpage_design_levels[i]),\n    conversion_score = round(y, 2)\n  )\n  data_list[[length(data_list) + 1]] &lt;- df_i\n}\nABn_customer_data_one_factor &lt;- do.call(rbind, data_list)\n\n# Showing the first 100 customers of the A/B/n testing\nhead(ABn_customer_data_one_factor, n = 100)\n\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Factor levels and sampled customers per treatment\nwebpage_design_levels = [\"D1\", \"D2\", \"D3\"]\nn_per_treatment = 200\n\n# Population fixed additive parameters\nalpha = [60, 60, 60]          \n\n# Simulating data\ndata_list = []\n\nfor i in range(3):  \n      mean_i = alpha[i]\n      y = np.random.normal(loc=mean_i, scale=np.sqrt(10), size = n_per_treatment)\n      y_rounded = np.round(y, 2)\n        \n      df_i = pd.DataFrame({\n          'webpage_design': [webpage_design_levels[i]] * n_per_treatment,\n          'conversion_score': y_rounded\n      })\n        \n      data_list.append(df_i)\n\n# Concatenate all groups into one DataFrame\nABn_customer_data_one_factor = pd.concat(data_list, ignore_index = True)\n\n# Showing the first 100 customers of the A/B/n testing\nprint(ABn_customer_data.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.2.2 Two-way ANOVA\nLet us extend our generative modelling framework to a two-way ANOVA experiment. In this context, we will simulate a dataset that represents a continuous response variable (i.e., the customer conversion score) affected by two experimental factors: webpage design and discount framing strategy. Each factor has its own main effect on the response, and their combination may also result in an interaction effect, which indicates how the impact of one factor depends on the level of the other. This setup allows us to assess whether variations in customer conversion arise from the individual effects of design and discount framing, as well as their combined influence. This approach offers a more comprehensive understanding of experimental variation and aids in practical decision-making.\nGenerative Modelling Process\nThis second experiment has the following controllable factors by the experimenter:\n\n\nWebpage design: Three different layouts \\(D_1\\) (the current layout), \\(D_2\\) (a new layout), and \\(D_3\\) (another new layout). This makes a three-level factor.\n\nDiscount framing: \\(\\text{Low}\\) (i.e., “Save 10% today”) or \\(\\text{High}\\) (i.e., “Save up to 40% today”). This makes a two-level factor.\n\nThis study will be a full factorial experiment characterized by the following elements:\n\nTwo factors: webpage design and discount framing.\nThere are \\(3 \\times 2 = 6\\) treatments (i.e., six different combinations of all the factor levels), which classifies this study as A/B/n testing.\nWe will simulate 200 customers (i.e., replicates) per treatment in our final dataset.\nThe outcome variable \\(Y\\) is the customer conversion score, which has been previously explained.\n\nOur data structure will be an additive model conceptually depicted as:\n\\[\n\\begin{align*}\n\\text{Outcome} &= \\text{First Main Effect} + \\text{Second Main Effect} + \\\\\n& \\qquad \\text{Interaction Effect} + \\text{Random Error}.\n\\end{align*}\n\\tag{B.3}\\]\nThen, for the data generation process, let \\(Y_{i,j,k}\\) represent the customer conversion score for the \\(k\\)th replicate of the treatment related to the \\(i\\)th webpage design and the \\(j\\)th discount framing levels. Equation B.3 is translated as:\n\\[\nY_{i,j,k} = \\alpha_i + \\beta_j + (\\alpha \\beta)_{i,j} + \\varepsilon_{i,j,k},\n\\tag{B.4}\\]\nwhere\n\n\n\\(\\alpha_i\\) is the fixed first main effect corresponding to the \\(i\\)th level of webpage design for \\(i = D_1, D_2, D_3\\);\n\n\\(\\beta_j\\) is the second fixed main effect corresponding to the \\(j\\)th level of discount framing for \\(j = \\text{Low}, \\text{High}\\);\n\n\\((\\alpha \\beta)_{i,j}\\) is the fixed interaction effect between the \\(i\\)th and \\(j\\)th levels of webpage design and discount framing respectively, and\n\n\\(\\varepsilon_{i,j,k}\\) is the random error associated to each \\(Y_{i,j,k}\\), capturing the variability and measurement error that introduces randomness in the response \\(Y_{i,j,k}\\).\n\n\n\nHeads-up on the mathematical representation of the interaction term!\n\n\nThe \\((\\alpha \\beta)_{i,j}\\) in Equation B.4 does not indicate that the main effects are multiplying each other. Mathematically, it is just another additive term on the right-hand side of the equation.\n\n\nIn this case, Equation B.4 is breaking down the outcome on the right-hand side into four additive components, which form the basis on how ANOVA models the data. With the exception of the random error \\(\\varepsilon_{i,j,k}\\), the other three terms are assumed to be fixed within the data-generating process, given that we are using a frequentist approach. Since \\(\\varepsilon_{i,j,k}\\) is random, we will assume that it follows a Normal distribution with a mean of \\(0\\) and a variance of \\(\\sigma^2\\) (which is another fixed parameter in the simulation):\n\\[\n\\varepsilon_{i,j,k} \\sim \\text{Normal}(0, \\sigma^2).\n\\]\nIn terms of our simulation, imagine you have a population of customers with the following fixed parameters:\n\nA vector of webpage design effects (i.e., the first main effect)\n\n\\[\\boldsymbol{\\alpha} =\n\\begin{bmatrix} \\alpha_{D_1} \\\\ \\alpha_{D_2} \\\\ \\alpha_{D_3} \\end{bmatrix} =\n\\begin{bmatrix} 40 \\\\ 50 \\\\ 60 \\end{bmatrix}.\\]\n\nA vector of discount framing effects (i.e., the second main effect)\n\n\\[\\boldsymbol{\\beta} =\n\\begin{bmatrix} \\alpha_{\\text{Low}} \\\\ \\alpha_{\\text{High}} \\end{bmatrix} =\n\\begin{bmatrix} -6 \\\\ 6 \\end{bmatrix}.\\]\n\nA matrix of interaction effects, whose rows correspond to the levels of webpage design and columns to the levels of discount framing,\n\n\\[\n\\boldsymbol{(\\alpha \\beta)} =\n\\begin{bmatrix}\n(\\alpha \\beta)_{D_1,\\text{Low}} & (\\alpha \\beta)_{D_1,\\text{High}} \\\\\n(\\alpha \\beta)_{D_2,\\text{Low}} & (\\alpha \\beta)_{D_2,\\text{High}} \\\\\n(\\alpha \\beta)_{D_3,\\text{Low}} & (\\alpha \\beta)_{D_3,\\text{High}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 5 \\\\\n4 & 6 \\\\\n8 & -12\n\\end{bmatrix}.\n\\]\n\nAn overall variance\n\n\\[\n\\sigma^2 = 16.\n\\]\nThe purpose of this simulation is to define a generative process that captures the essential structure of a two-way ANOVA design. In this setting, both webpage design and discount framing are treated as controlled experimental factors, each contributing to the overall pattern of customer conversion scores. By specifying fixed main effects for each factor and an accompanying matrix of interaction effects, we represent a population where conversion performance arises from both individual influences and their combined interplay. This design enables us to study how the effect of one factor may depend on the level of the other, which is an idea central to interpreting two-way ANOVA outcomes.\nMoreover, the simulation is grounded in the core distributional assumptions of the ANOVA framework: normality and homoscedasticity. The random term \\(\\varepsilon_{i,j,k}\\) is assumed to follow a Normal distribution, ensuring that the response within each treatment combination is approximately symmetric and well-behaved. The homoscedasticity condition, specified through a constant variance \\(\\sigma^2 = 16\\), implies that the degree of variability in customer responses remains consistent across all combinations of factors. These assumptions provide the stability necessary for the \\(F\\)-test to reliably separate true factor effects from random fluctuations.\nCode\nLet us move to the corresponding code to simulate this data. Recall we are simulating 200 customers for each one of the six treatments, which will give us an overall sample size of \\(n = 1,200\\). Furthermore, note that Python additionally uses the {numpy} and {pandas} libraries. The final data frame will be stored in ABn_customer_data_two_factors whose columns will be webpage_design, discount_framing, and conversion_score.\n\n\nR Code\nPython Code\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Factor levels and sampled customers per treatment\nwebpage_design_levels &lt;- c(\"D1\", \"D2\", \"D3\")\ndiscount_framing_levels &lt;- c(\"Low\", \"High\")\nn_per_treatment &lt;- 200\n\n# Population fixed additive parameters\nalpha &lt;- c(40, 50, 60)\nbeta &lt;- c(-6, 6)\ninteraction &lt;- matrix(\n  c(\n    0, 5,\n    4, 6,\n    8, -12\n  ),\n  nrow = 3, byrow = TRUE\n)\n\n# Simulating data\ndata_list &lt;- list()\nfor (i in 1:3) {\n  for (j in 1:2) {\n    mean_ij &lt;- alpha[i] + beta[j] + interaction[i, j]\n    y &lt;- rnorm(n_per_treatment, mean = mean_ij, sd = sqrt(16))\n    df_ij &lt;- data.frame(\n      webpage_design = as.factor(webpage_design_levels[i]),\n      discount_framing = as.factor(discount_framing_levels[j]),\n      conversion_score = round(y, 2)\n    )\n    data_list[[length(data_list) + 1]] &lt;- df_ij\n  }\n}\nABn_customer_data_two_factors &lt;- do.call(rbind, data_list)\n\n# Showing the first 100 customers of the A/B/n testing\nhead(ABn_customer_data_two_factors, n = 100)\n\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Factor levels and sampled customers per treatment\nwebpage_design_levels = [\"D1\", \"D2\", \"D3\"]\ndiscount_framing_levels = [\"Low\", \"High\"]\nn_per_treatment = 200\n\n# Population fixed additive parameters\nalpha = [40, 50, 60]          \nbeta = [-6, 6]              \ninteraction = np.array([\n    [0, 5],\n    [4, 6],\n    [8, -12]\n])\n\n# Simulating data\ndata_list = []\n\nfor i in range(3):  \n    for j in range(2):  \n        mean_ij = alpha[i] + beta[j] + interaction[i, j]\n        y = np.random.normal(loc=mean_ij, scale=np.sqrt(16), size = n_per_treatment)\n        y_rounded = np.round(y, 2) \n        \n        df_ij = pd.DataFrame({\n            'webpage_design': [webpage_design_levels[i]] * n_per_treatment,\n            'discount_framing': [discount_framing_levels[j]] * n_per_treatment,\n            'conversion_score': y_rounded\n        })\n        \n        data_list.append(df_ij)\n\n# Concatenate all groups into one DataFrame\nABn_customer_data_two_factors = pd.concat(data_list, ignore_index = True)\n\n# Showing the first 100 customers of the A/B/n testing\nprint(ABn_customer_data_two_factors.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated Datasets</span>"
    ]
  }
]