[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Mini Test Book",
    "section": "",
    "text": "Preface\nWe have experienced this sense of overwhelm throughout our academic journeys as well. However, we also understand that statistical inference is a powerful tool for gaining insights into complex populations across various fields of study. Whether analyzing electoral preferences in political science or assessing the effectiveness of innovative medical treatments in randomized clinical trials, the applications are extensive. Hence, in response to these challenges, we have created this mini-book as a handy resource to help structure and simplify the learning of different fundamental hypothesis tests. Our goal is to present these concepts in a reader-friendly manner while clearly explaining the necessary statistical jargon, making these inferential methods accessible to a broader audience.\nNote that, after conducting extensive research into the available educational literature, we discovered that there is no comprehensive resource that explains various inferential methods simultaneously using two essential programming languages in the field of data science: R and Python. Furthermore, we could not find reproducible and transparent tools that would enable learners to implement and adapt these methods in their own computational environments. Based on our teaching experience, these shortcomings hinder effective learning in the practice of statistical inference, especially given the numerous tests required to achieve mastery.\nTo address this gap, we have developed a bilingual resource in both R and Python, which features a common test workflow consisting of eight distinct stages applicable to each hypothesis test: study design, data collection and wrangling, exploratory data analysis, testing settings, hypothesis definitions, test flavors and components, inferential conclusions, and storytelling. Additionally, all the tests we discuss are organized through different mind maps to help readers visualize their learning process. Finally, by offering this resource as an Open Educational Resource (OER) in Quarto via a GitHub repository, we aim to inspire and empower academic communities worldwide to share and adapt this knowledge to suit their specific needs.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#g.-alexi-rodríguez-arelis",
    "href": "index.html#g.-alexi-rodríguez-arelis",
    "title": "The Mini Test Book",
    "section": "G. Alexi Rodríguez-Arelis",
    "text": "G. Alexi Rodríguez-Arelis\n\n\n\n\n\nI’m an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I’ve been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I’m incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#kate-manskaia",
    "href": "index.html#kate-manskaia",
    "title": "The Mini Test Book",
    "section": "Kate Manskaia",
    "text": "Kate Manskaia",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#payman-nickchi",
    "href": "index.html#payman-nickchi",
    "title": "The Mini Test Book",
    "section": "Payman Nickchi",
    "text": "Payman Nickchi",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/privacy-policy.html",
    "href": "book/privacy-policy.html",
    "title": "Website Privacy Policy",
    "section": "",
    "text": "Information Collection and Use\nWe use Google Analytics, a web analytics service provided by Google, LLC. (“Google”). Google Analytics uses cookies to help analyze how students interact with the textbook, including tracking which sections are accessed most frequently. Information generated by cookies about your use of our website (including IP address) will be transmitted to and stored by Google on servers in the United States.\nGoogle will use this information solely for evaluating textbook usage, compiling usage reports to enhance the educational effectiveness of the textbook, and providing related services.\nYou may refuse the use of cookies by selecting the appropriate settings in your browser; however, please note this may affect your textbook browsing experience.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/privacy-policy.html#personal-information",
    "href": "book/privacy-policy.html#personal-information",
    "title": "Website Privacy Policy",
    "section": "Personal Information",
    "text": "Personal Information\nWe do not collect personally identifiable information through Google Analytics. Any personally identifiable information, such as your name and email address, would only be collected if voluntarily submitted for specific educational purposes (e.g., feedback or course-related inquiries). We will never sell or distribute your personal information to third parties.\nFor any questions or concerns, please contact us at alexrod@stat.ubc.ca.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/intro.html",
    "href": "book/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The Test Mind Map",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/intro.html#the-test-mind-map",
    "href": "book/intro.html#the-test-mind-map",
    "title": "Introduction",
    "section": "",
    "text": "mindmap\n  root((Frequentist\n  Hypothesis \n  Testings\n  ))\n    Simulation Based&lt;br/&gt;Tests\n    Classical&lt;br/&gt;Tests\n      (Chapter 1: &lt;br/&gt;Tests for One&lt;br/&gt;Continuous&lt;br/&gt;Population Mean)\n        {{Unbounded&lt;br/&gt;Response}}\n        {{Proportion between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from a &lt;br/&gt;Binary Response}}\n      (Chapter 2: &lt;br/&gt;Tests for Two&lt;br/&gt;Continuous&lt;br/&gt;Population Means)\n        Two&lt;br/&gt;Independent&lt;br/&gt;Populations\n          {{Unbounded&lt;br/&gt;Responses}}\n          {{Proportions between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from two &lt;br/&gt;Binary Responses}}\n        Two&lt;br/&gt;Related&lt;br/&gt;Populations or&lt;br/&gt;Measurements\n          {{Unbounded&lt;br/&gt;Responses}}\n      (Chapter 3: &lt;br/&gt;ANOVA related &lt;br/&gt;Tests for&lt;br/&gt;k Continuous&lt;br/&gt;Population Means)\n        {{Unbounded&lt;br/&gt;Responses}}\n\n\n\n\n\n\n\n\nFigure 1: A general hypothesis testing mind map outlining all techniques explored in this book. Depending on the overall approach to be used, these techniques are divided into two broad categories: classical and simulation-based tests.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/intro.html#the-test-workflow",
    "href": "book/intro.html#the-test-workflow",
    "title": "Introduction",
    "section": "The Test Workflow",
    "text": "The Test Workflow\n\n\n\n\n\n\nFigure 2: A classical-based hypothesis testing workflow structured in four substages: general settings, hypotheses definitions, test flavour and components, and inferential conclusions.\n\n\n\n\n\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/chapter1.html",
    "href": "book/chapter1.html",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "",
    "text": "One-sample z-test for the mean\nUse this test when: - The population variance σ² is known, and - The sample comes from a normally distributed population, or the sample size is large (typically ( n )).\nThe test statistic is:\n\\[ z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}} \\]\nWhere: - ( \\(\\bar{x}\\) ) is the sample mean\n- ( \\(\\mu_0\\) ) is the hypothesized population mean\n- ( \\(\\sigma\\) ) is the known population standard deviation\n- ( n ) is the sample size\nWe compare the calculated ( z )-value to a standard normal distribution to compute a p-value or make a decision based on a critical value.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter1.html#one-sample-t-test-for-the-mean",
    "href": "book/chapter1.html#one-sample-t-test-for-the-mean",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "One-sample t-test for the mean",
    "text": "One-sample t-test for the mean\nUse this test when: - The population variance is unknown, and - The sample is either normally distributed or large enough to rely on the central limit theorem.\nImagine you want to assess whether a new method of teaching introductory physics improves student performance compared to the traditional method previously used. To explore this, you test the new method at the University of British Columbia (UBC) and compare the results to historical data from students who were taught using the traditional approach. This historical data serves as your reference value.\nSuppose the population has an unknown average physics score, denoted as:\n\\[\n\\mu \\quad \\text{(mean physics score at UBC)}\n\\]\nSince we do not have access to the grades of all students, we take a random sample from the population. Let this sample consist of \\(n\\) students, with observed scores:\n\\[\nX_1, X_2, \\dots, X_n\n\\]\nThe central question becomes:\n\nIs the mean physics score in our sample statistically different from a given reference value?\n\nIf, for example, the historical average physics score is known to be 75, then our question becomes more specific:\n\nIs the mean physics score in the sample statistically different from 75?\n\nHypotheses\nWe can formally express this with the following hypotheses:\n\n\nNull hypothesis \\(H_0\\): \\(\\mu = 75\\)\n\n\nAlternative hypothesis \\(H_1\\): \\(\\mu \\ne 75\\)\n\n\nUnder the null hypothesis, we assume that the average score under the new method is equal to the historical average of 75. If the null is rejected, we conclude that there is a statistically significant difference, suggesting that the new method may lead to either higher or lower average performance.\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nWhere: - ( s ) is the sample standard deviation (used instead of ( \\(\\sigma\\) ))\nThis statistic follows a t-distribution with ( n - 1 ) degrees of freedom.\nStudy Design\nIn this example we use the Palmer Station Penguins dataset collected by the LTER in Antarctica (2007 – 2009).\nThe dataset spans three penguin species and includes continuous variables such as flipper length, bill size, and body mass.\n\nResearch question:Is the average flipper length of penguins significantly different from 200 mm?\n\nData Collection & Wrangling\nWe obtain the dataset Palmer Station Penguins dataset collected by the ‘LTER’\n\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Drop rows with missing values\npenguins_clean = penguins.dropna()\n\n# 80/20 train–test split\ntrain_set, test_set = train_test_split(\n    penguins_clean, test_size=0.2, random_state=42\n)\n\nExploratory Data Analysis (EDA)\nBefore conducting the statistical test, we begin with an exploratory analysis to understand the distribution and characteristics of the flipper_length_mm variable.\nFirst, we examine summary statistics such as the mean, standard deviation, and quartiles. This helps us get a sense of the central tendency and spread of the data:\n\nprint(train_set[\"flipper_length_mm\"].describe())\n\ncount    266.00000\nmean     201.00000\nstd       13.91592\nmin      172.00000\n25%      190.00000\n50%      197.00000\n75%      213.00000\nmax      231.00000\nName: flipper_length_mm, dtype: float64\n\n\nNext, we visualize the distribution of flipper lengths using a histogram. This allows us to assess whether the data are approximately symmetric and whether any outliers are present:\n\nimport matplotlib.pyplot as plt\n\ntrain_set[\"flipper_length_mm\"].hist(edgecolor=\"black\", color=\"skyblue\")\nplt.title(\"Distribution of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\nTo explore the relationship between flipper length and another continuous variable, we create a scatter plot of flipper length versus body mass. This helps us visually assess whether larger penguins tend to have longer flippers, and whether this relationship is linear or varies across ranges:\n\nplt.scatter(\n    train_set[\"body_mass_g\"],\n    train_set[\"flipper_length_mm\"],\n    alpha=0.6\n)\nplt.title(\"Body Mass vs. Flipper Length\")\nplt.xlabel(\"Body Mass (g)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.show()\n\n\n\n\n\n\n\nNow, we can perform one‑Sample t-Test\n\nimport scipy.stats as stats\n\nt_stat, p_value = stats.ttest_1samp(\n    train_set[\"flipper_length_mm\"], popmean=200\n)\nprint(f\"t = {t_stat:.3f},  p = {p_value:.4f}\")\n\nt = 1.172,  p = 0.2422\n\n\nA one-sample t-test was conducted to determine whether the average flipper length of penguins is significantly different from 200 mm. Based on a training sample, the test produced a t-statistic of t and a p-value of p.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter1.html#given-a-significance-level-of-0.05-if-the-p-value-is-less-than-0.05-we-reject-the-null-hypothesis-and-conclude-that-the-average-flipper-length-is-significantly-different-from-200-mm.-if-not-we-do-not-have-sufficient-evidence-to-say-it-differs.",
    "href": "book/chapter1.html#given-a-significance-level-of-0.05-if-the-p-value-is-less-than-0.05-we-reject-the-null-hypothesis-and-conclude-that-the-average-flipper-length-is-significantly-different-from-200-mm.-if-not-we-do-not-have-sufficient-evidence-to-say-it-differs.",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "Given a significance level of 0.05, if the p-value is less than 0.05, we reject the null hypothesis and conclude that the average flipper length is significantly different from 200 mm. If not, we do not have sufficient evidence to say it differs.",
    "text": "Given a significance level of 0.05, if the p-value is less than 0.05, we reject the null hypothesis and conclude that the average flipper length is significantly different from 200 mm. If not, we do not have sufficient evidence to say it differs.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter1.html#one-sample-z-test-for-proportions",
    "href": "book/chapter1.html#one-sample-z-test-for-proportions",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "One-sample z-test for proportions",
    "text": "One-sample z-test for proportions\nUse this test when: - The variable is binary (success/failure, yes/no, etc.), and - You want to test a population proportion ( p ), using a large enough sample.\nThe test statistic is:\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\\]\nWhere: - ( \\(\\hat{p}\\) ) is the sample proportion\n- ( \\(p_0\\) ) is the hypothesized population proportion\n- ( \\(n\\) ) is the sample size\nThis test assumes ( \\(np_0 \\geq 5\\) ) and ( \\(n(1 - p_0)\\) \\(\\geq\\) 5 ) to justify the normal approximation to the binomial distribution.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html",
    "href": "book/chapter2.html",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "",
    "text": "Two sample Student’s t-test for Independent Samples\nIndependent samples arise when the observations in one group do not influence or relate to the observations in the other. In statistical terms we call this two independent samples. A classic example from educational research is described below:\nSuppose you’re interested in whether a new method of teaching introductory physics improves student performance. To investigate this, you decide to test the method at two universities: the University of British Columbia (UBC) and Simon Fraser University (SFU). You apply the new teaching method at SFU and compare the results to students taught with the traditional method at UBC.\nIn this scenario, students at UBC and SFU form two distinct, unrelated groups. Since the students are not paired or matched across schools, and each individual belongs to only one group, the samples are independent.\nLet us assume that each population has an unknown average physics score denoted by:\n\\[\n    \\mu_1 \\quad \\text{(mean for UBC)}, \\quad \\mu_2 \\quad \\text{(mean for SFU)}.\n\\]\nSince we do not have access to all students’ grades, we take a random sample from each school. Suppose:\nNote that the sample sizes \\(n\\) and \\(m\\) do not need to be equal. Now, the central question becomes:\nIs there a statistically significant difference between the mean physics scores of the two groups?\nIn formal terms, we test the hypotheses:\n\\[H_0: \\mu_1 = \\mu_2 \\quad \\text{versus} \\quad H_A: \\mu_1 \\ne \\mu_2\\]\nTo test this, we use the two-sample t-test, which compares the sample means and incorporates variability within and between the samples. If we assume equal population variances, the test statistic is:\n\\[t = \\frac{(\\bar{X} - \\bar{Y})}{s_p \\sqrt{\\frac{1}{n} + \\frac{1}{m}}}\\]\nwhere:\nUnder the assumption that null hypothesis is correct (i.e. \\(\\mu_1=\\mu_2\\)) then the test statistic defined above follows a t-distribution with \\(n+m-2\\) degrees of freedom (which we denote it by \\(T_{n+m-2}\\)). Knowing the distribution of this statistic helps us to compute \\(\\textit{p-value}\\) of the test as follows:\n\\[\\textit{p-value} = 2 \\times Pr(T_{n+m-2} \\ge |t|)\\] Note: The probability is multiplied by two since we have a two sided hypothesis (alternative is \\(\\mu_1 \\neq \\mu_2\\)). For a one sided test (when alternative hypothesis is \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\)) we do not need to multiply by two.\nNow let us see how to run the two-sample test on some example datasets in R. The following sections closely follow the test workflow from introduction section.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#two-sample-students-t-test-for-independent-samples",
    "href": "book/chapter2.html#two-sample-students-t-test-for-independent-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "",
    "text": "From UBC (Population 1), we obtain a sample of size \\(n\\), denoted as: \\[X_1, X_2, \\ldots, X_n\\]\nFrom SFU (Population 2), we obtain a sample of size \\(m\\), denoted as: \\[Y_1, Y_2, \\ldots, Y_m\\]\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means for UBC and SFU, respectively,\n\n\\(s_p\\) is the , computed as:\n\\(s_p = \\sqrt{\\frac{(n - 1)s_X^2 + (m - 1)s_Y^2}{n + m - 2}}\\)\n\n\\(s_X^2\\) and \\(s_Y^2\\) are the sample variances of the two groups.\n\n\n\n\nStudy design\nFor this example, we will be using Auto dataset from ISLR package. This dataset contains gas mileage, horsepower, and other information for 392 vehicles. Some of variables of interest are: 1) cylinders an integer (numerical) value between 4 and 8 which indicates the number of cylinders of car, and 2) horsepower which shows engine horsepower. You may wondering if the average of horsepower in cars with 4 cylinders is statistically different than the means in cars with 5 cylinders?\nData Collection and Wrangling\nWe obtain the dataset which is available in ISLR package. First we created a new copy of this dataset to avoid touching the actual data. Also we filter rows to those cars with 4 or 8 cylinders only.\n\n# Get a copy of dataset\nauto_data &lt;- Auto\n\n# Filter rows\nauto_data &lt;- auto_data %&gt;% filter(cylinders %in% c(4,8) )\n\nFinally, we randomly create test and train set from this dataset. We use a proportion of 50-50 between train and test.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Splitting the dataset into training and testing sets\ntrain_indices &lt;- sample(seq_len(nrow(auto_data)), size = 0.50 * nrow(auto_data))\ntrain_auto &lt;- auto_data[train_indices, ]\ntest_auto &lt;- auto_data[-train_indices, ]\n\nExplanatory Data Analysis\nOnce we have the data and it is split into training and test sets, the next step is to begin exploratory data analysis (EDA) on train set. This step is crucial, as it helps us gain a better understanding of the distribution of variables in our dataset. The horsepower variable in dataset is a numerical variable. The cylinders variable is an integer variable that helps to divide observations into two groups.\nIn particular, we are interested in the distribution of horsepower in two different groups (cars with 4 cylinders vs cars with 8 cylinders). Using a histogram for this variable is a good choice as we have a variable with numerical values.\n\nggplot(train_auto, aes(x = horsepower)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 20) +\n  facet_wrap(~ cylinders, nrow = 1) +\n  labs(title = \"Side-by-side histogram of horsepower by number of cylinders\",\n       x = \"Horsepower\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe also look at some descriptive statistics of horsepower in both groups for better understanding of data. The descriptive statistics in cars with 4 cylinders:\n\nsummary(train_auto %&gt;% filter(cylinders == 4) %&gt;% select(horsepower))\n\n   horsepower    \n Min.   : 46.00  \n 1st Qu.: 68.00  \n Median : 78.50  \n Mean   : 78.33  \n 3rd Qu.: 88.00  \n Max.   :113.00  \n\n\nand with 8 cylinders:\n\nsummary(train_auto %&gt;% filter(cylinders == 8) %&gt;% select(horsepower))\n\n   horsepower \n Min.   :105  \n 1st Qu.:140  \n Median :150  \n Mean   :160  \n 3rd Qu.:175  \n Max.   :225  \n\n\nLooking at histogram and also summary statistics in different groups we can observe that:\n\nthe distribution of\nTesting Settings\nHypothesis Definitions\nTest Flavour and Components\nInferential Conclusions\nStorytelling\nIgnore the following content for now. Still editting\nIn this example, we start with a dataset that records the time MDS students spend on course website of DSCI 554. You will encounter this dataset again in DSCI 554 when we discuss A/B/n testing. For now, here’s what the dataset looks like:\n\nABn_data &lt;- read_csv('data/ABn_data.csv', show_col_types = FALSE)\nABn_data &lt;- ABn_data %&gt;% mutate(Colour = as.factor(Colour), Font = as.factor(Font))\nABn_data\n\n# A tibble: 72 × 3\n   Duration Colour Font  \n      &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1       90 Red    Small \n 2       95 Red    Large \n 3      107 Red    Medium\n 4       92 Red    Small \n 5       89 Red    Large \n 6       92 Red    Medium\n 7       81 Red    Small \n 8       92 Red    Large \n 9       93 Red    Medium\n10       80 Blue   Small \n# ℹ 62 more rows\n\n\nThe columns in the dataset are as follows:\n\nFont: A factor variable with three levels — small, medium, and large.\nButton Colour: A factor variable with two levels — Red and Blue.\nDuration: A continuous variable representing the duration of each visit, recorded in minutes.\n\nThe main statistical question we are asking here is:\nIs there a statistically significant difference in the mean visit duration between websites with red buttons and those with blue buttons?\nThis means we are interested in the duration time users spend on website in two different populations: red button design and blue button design. First we do some data selection to create two vector of numbers: one for the visit duration visits in website with red buttons and one for the duration of visits in website with blue buttons. The following code can take care of this. For Red group:\n\nduration_red &lt;- ABn_data %&gt;% filter(Colour == 'Red') %&gt;% pull(Duration)\nduration_red \n\n [1]  90  95 107  92  89  92  81  92  93  83  80  95  98  98 106  74  81  74  85\n[20]  88  88 112 104  91  82  78  94  86  78  89  79  86  87  85  89  83\n\n\nand for Blue group:\n\nduration_blue &lt;- ABn_data %&gt;% filter(Colour == 'Blue') %&gt;% pull(Duration)\nduration_blue\n\n [1]  80  87 100 121 110 119  78  98 122 102 109 105  99  94 123 136 133 132  60\n[20] 104 114  90 118 113 119 122 136  73 114 114 109 131 126 116 136 133\n\n\nHow to run this test in R?\nIn order to run this test, similar to what we learned in (LINK to chapter 1) we can use t.test function in R. The function can be used to perform one or two sample t-tests. The relevant arguments of the function are as follows:\n\n\nx is (non-empty) numeric vector of data values.\n\ny is also (non-empty) numeric vector of data values (can be NULL if you run a one sample test).\n\nvar.equal is a binary value (TRUE/FALSE) to indicate if R needs to assume equal variance or not.\n\nTherefore we can run the test for two different cases.\n\nCase 1: Under the assumption that variances between two populations are equal:\n\n\nt.test(x = duration_red, y = duration_blue, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  duration_red and duration_blue\nt = -6.1183, df = 70, p-value = 4.838e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -28.43489 -14.45400\nsample estimates:\nmean of x mean of y \n  89.0000  110.4444 \n\n\n\nCase 2: Under the assumption that variances between two populations are NOT equal:\n\n\nt.test(x = duration_red, y = duration_blue, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  duration_red and duration_blue\nt = -6.1183, df = 50.043, p-value = 1.429e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -28.48424 -14.40465\nsample estimates:\nmean of x mean of y \n  89.0000  110.4444 \n\n\nIn both outputs, we can see the following:\n\nt is the test statistic.\ndf is the degrees of freedom for the test.\n\np-value is the p-value of the test. Note that, by default, this is for a two-sided test. If you need to conduct a one-sided test, you can either divide the p-value by two or use the alternative argument in the t.test function.\n\n95 percent confidence interval provides the 95% confidence interval for \\(\\mu_1 - \\mu_2\\).\nsample estimates gives the sample means for each group.\n\nNote: By default the value of var.equal is FALSE.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#two-sample-welchs-t-test-for-independent-samples",
    "href": "book/chapter2.html#two-sample-welchs-t-test-for-independent-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "Two sample Welch’s t-test for independent samples",
    "text": "Two sample Welch’s t-test for independent samples\nIf the assumption of equal variances is questionable, we instead use Welch’s t-test, which adjusts the standard error and degrees of freedom accordingly. The Welch’s test statistic is computed as:\n\\[t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{s_X^2}{n} + \\frac{s_Y^2}{m}}}\\]\nUnder the assumption that null hypothesis is correct, the test statistics defined above still follows a t-distribuion but with a different degrees of freedom. The degree of freedom when we do not make equal variance assumption is:\n\\[\\nu = \\frac{\\left( \\frac{s_1^2}{n} + \\frac{s_2^2}{m} \\right)^2}\n{\\frac{\\left( \\frac{s_1^2}{n} \\right)^2}{n - 1} + \\frac{\\left( \\frac{s_2^2}{m} \\right)^2}{m - 1}}\\]\nNote that this degree of freedom is not necessaily an integer number (could be a real number). When we run t-test, we operate under the assumption that: 1) either the sample size is large enough (we are thinking about \\(n=30\\) at least) so that central limit theorem assumptions work well, or 2) the distribution of our sample in each group is normal or symmetric enough.\nIf the normality assumption is also not satisfied (e.g., due to skewed distributions or outliers) or we have a very small sample size, we may turn to a non-parametric alternative, such as the Mann–Whitney–Wilcoxon test, which compares the ranks of the observations across groups rather than the raw values but this book will not cover it. You can read more about it LINK.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#paired-samples",
    "href": "book/chapter2.html#paired-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "Paired Samples",
    "text": "Paired Samples\nPaired samples arise when each observation in one group is matched or linked to an observation in the other group. This structure is typical in before-and-after studies, matched-subject designs, or repeated measures on the same individuals. A classic example comes from health sciences.\nSuppose you’re investigating whether a new diet plan reduces blood pressure. You recruit a group of participants and record their blood pressure before starting the diet. After following the diet for two months, you measure their blood pressure again. In this scenario, each participant contributes two measurements: one before the intervention and one after. These measurements are not independent as they come from the same person. Therefore we treat them as paired.\nTo formulate the problem and hypothesis, let us assume that each individual has two measurements:\n\nBefore the diet: \\(X_1, X_2, \\ldots, X_n\\)\nAfter the diet: \\(Y_1, Y_2, \\ldots, Y_n\\)\n\nNote that in this case the sample size is the same (in both before and after diet sample we have \\(n\\) observations). We call this a paired sample. Since the samples are paired, we define the difference for each individual as follows:\n\\[D_i = Y_i - X_i \\quad \\textit{for} \\quad i = 1,2, \\ldots, n\\] Each \\(D_i\\) is the difference of blood pressure after and before using new diet. The main statistical question now is:\nIs there a statistically significant difference in the mean blood pressure before and after the diet?\nIn other words, we test the following hypothesis:\n\\[H_0: \\mu_D = 0 \\quad \\text{versus} \\quad H_A: \\mu_D \\ne 0\\] Here the notation of \\(\\mu_D\\) is the population mean of the differences of \\(D_i\\) which is an unknown parameter in the population. To test this hypothesis, we use the paired t-test, which is essentially a one-sample t-test on the differences \\(D_1, D_2, \\ldots, D_n\\). We test \\(\\mu_D=0\\) because if there is an actual effect of diet on blood pressure, we expect the null hypothesis to be rejected.\nThe test statistic for this hypothesis testing is:\n\\[t = \\frac{\\bar{D}}{s_D / \\sqrt{n}}\\]\nwhere:\n\n\n\\(\\bar{D}\\) is the sample mean of the differences,\n\n\\(s_D\\) is the sample standard deviation of the differences,\n\n\\(n\\) is the number of pairs.\n\nThe standard deviation of the differences is calculated as:\n\\[s_D = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^n (D_i - \\bar{D})^2 }\\]\nUnder the null hypothesis, the test statistic follows a t-distribution with \\(n-1\\) degrees of freedom. For this test, we can compute the \\(\\textit{p-value}\\) as:\n\\[\\textit{p-value} = 2 \\times \\Pr(T_{n - 1} \\ge |t|)\\]\nExample dataset in R\n\nTBD\nHow to run the test in R?",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter3.html",
    "href": "book/chapter3.html",
    "title": "Chapter 3: ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "",
    "text": "mindmap\n  root((Frequentist\n  Hypothesis \n  Testings\n  ))\n    Simulation Based&lt;br/&gt;Tests\n    Classical&lt;br/&gt;Tests\n      (Chapter 1: &lt;br/&gt;Tests for One&lt;br/&gt;Continuous&lt;br/&gt;Population Mean)\n      (Chapter 2: &lt;br/&gt;Tests for Two&lt;br/&gt;Continuous&lt;br/&gt;Population Means)\n      (Chapter 3: &lt;br/&gt;ANOVA-related &lt;br/&gt;Tests for&lt;br/&gt;k Continuous&lt;br/&gt;Population Means)\n        {{Unbounded&lt;br/&gt;Responses}}\n          One&lt;br/&gt;Factor type&lt;br/&gt;Feature\n            )One way&lt;br/&gt;ANOVA(\n          Two&lt;br/&gt;Factor type&lt;br/&gt;Features\n            )Two way&lt;br/&gt;ANOVA(\n\n\n\n\n\n\n\n\nFigure 1: A specific hypothesis testing mind map outlining the techniques explored in this chapter, which include ANOVA-related tests for \\(k\\) population means.",
    "crumbs": [
      "Chapter 3: ANOVA-related Tests for $k$ Continuous Population Means"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Tukey, John W. 1962. “The Future of Data\nAnalysis.” The Annals of Mathematical Statistics\n33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.",
    "crumbs": [
      "References"
    ]
  }
]