[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Mini Test Book",
    "section": "",
    "text": "Preface\nThis is a mini book for hypothesis testing in statistics. This book covers the tests from DSCI 552 in MDS program at UBC.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#g.-alexi-rodríguez-arelis",
    "href": "index.html#g.-alexi-rodríguez-arelis",
    "title": "The Mini Test Book",
    "section": "G. Alexi Rodríguez-Arelis",
    "text": "G. Alexi Rodríguez-Arelis\n\n\n\n\n\n\n\n\n\nI’m an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I’ve been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I’m incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#kate-manskaia",
    "href": "index.html#kate-manskaia",
    "title": "The Mini Test Book",
    "section": "Kate Manskaia",
    "text": "Kate Manskaia",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#payman-nickchi",
    "href": "index.html#payman-nickchi",
    "title": "The Mini Test Book",
    "section": "Payman Nickchi",
    "text": "Payman Nickchi",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/privacy-policy.html",
    "href": "book/privacy-policy.html",
    "title": "Website Privacy Policy",
    "section": "",
    "text": "Information Collection and Use\nYour privacy is important to us. This policy outlines how this online textbook created for courses at the University of British Columbia (UBC) (“we,” “us,” or “our”) collects, uses, and protects your information.\nWe use Google Analytics, a web analytics service provided by Google, LLC. (“Google”). Google Analytics uses cookies to help analyze how students interact with the textbook, including tracking which sections are accessed most frequently. Information generated by cookies about your use of our website (including IP address) will be transmitted to and stored by Google on servers in the United States.\nGoogle will use this information solely for evaluating textbook usage, compiling usage reports to enhance the educational effectiveness of the textbook, and providing related services.\nYou may refuse the use of cookies by selecting the appropriate settings in your browser; however, please note this may affect your textbook browsing experience.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/privacy-policy.html#personal-information",
    "href": "book/privacy-policy.html#personal-information",
    "title": "Website Privacy Policy",
    "section": "Personal Information",
    "text": "Personal Information\nWe do not collect personally identifiable information through Google Analytics. Any personally identifiable information, such as your name and email address, would only be collected if voluntarily submitted for specific educational purposes (e.g., feedback or course-related inquiries). We will never sell or distribute your personal information to third parties.\nFor any questions or concerns, please contact us at alexrod@stat.ubc.ca.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/intro.html",
    "href": "book/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The Test Mind Map\nThis is a mini-book on hypothesis testing in statistics. It covers the tests taught in DSCI 552 in the MDS program at UBC.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/intro.html#the-test-mind-map",
    "href": "book/intro.html#the-test-mind-map",
    "title": "Introduction",
    "section": "",
    "text": "mindmap\n  root((Frequentist\n  Hypothesis \n  Testings\n  ))\n    Simulation Based&lt;br/&gt;Tests\n    Classical&lt;br/&gt;Tests\n      (Chapter 1: &lt;br/&gt;Tests for One&lt;br/&gt;Continuous&lt;br/&gt;Population Mean)\n        {{Unbounded&lt;br/&gt;Response}}\n        {{Proportion between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from a &lt;br/&gt;Binary Response}}\n      (Chapter 2: &lt;br/&gt;Tests for Two&lt;br/&gt;Continuous&lt;br/&gt;Population Means)\n        Two&lt;br/&gt;Independent&lt;br/&gt;Populations\n          {{Unbounded&lt;br/&gt;Responses}}\n          {{Proportions between&lt;br/&gt;0 and 1&lt;br/&gt;obtained from two &lt;br/&gt;Binary Responses}}\n        Two&lt;br/&gt;Related&lt;br/&gt;Populations or&lt;br/&gt;Measurements\n          {{Unbounded&lt;br/&gt;Responses}}\n      (Chapter 3: &lt;br/&gt;ANOVA related &lt;br/&gt;Tests for&lt;br/&gt;k Continuous&lt;br/&gt;Population Means)\n        {{Unbounded&lt;br/&gt;Responses}}\n\n\n\n\n\n\n\n\nFigure 1: A general hypothesis testing mind map outlining all techniques explored in this book. Depending on the overall approach to be used, these techniques are divided into two broad categories: classical and simulation-based tests.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/intro.html#the-test-workflow",
    "href": "book/intro.html#the-test-workflow",
    "title": "Introduction",
    "section": "The Test Workflow",
    "text": "The Test Workflow\n\n\n\n\n\n\nFigure 2: A classical-based hypothesis testing workflow structured in four substages: general settings, hypotheses definitions, test flavour and components, and inferential conclusions.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "book/chapter1.html",
    "href": "book/chapter1.html",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "",
    "text": "One-sample z-test for the mean\nThis chapter introduces statistical tests designed to analyze a single sample, which is a fundamental task in data analysis across many disciplines. Whether you’re evaluating whether the average recovery time from a treatment differs from a known standard, assessing whether student test scores exceed a benchmark, or testing if the proportion of success in a group differs from an expected rate, these methods help determine whether the observed values are statistically significant or simply due to chance.\nThere are several statistical tests used to evaluate hypotheses about a single sample. The appropriate test depends on the type of variable (mean or proportion), sample size, and whether population parameters like variance are known.\nWe test whether a population mean equals a specific value. The right test depends on:\nIn this chapter, we focus on statistical tests used to evaluate hypotheses about a single population mean or proportion, based on sample data. These tests help determine whether a sample provides sufficient evidence to conclude that the population mean (or proportion) differs from a specified value.\nWe cover two cases for the mean — depending on whether the population variance is known or unknown — and one test for binary outcomes where we’re testing a population proportion.\nKey tests include:\nUse this test when: - The population variance σ² is known, and - The sample comes from a normally distributed population, or the sample size is large (typically ( n )).\nThe test statistic is:\n\\[ z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}} \\]\nWhere: - ( \\(\\bar{x}\\) ) is the sample mean\n- ( \\(\\mu_0\\) ) is the hypothesized population mean\n- ( \\(\\sigma\\) ) is the known population standard deviation\n- ( n ) is the sample size\nWe compare the calculated ( z )-value to a standard normal distribution to compute a p-value or make a decision based on a critical value.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter1.html#one-sample-t-test-for-the-mean",
    "href": "book/chapter1.html#one-sample-t-test-for-the-mean",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "One-sample t-test for the mean",
    "text": "One-sample t-test for the mean\nUse this test when: - The population variance is unknown, and - The sample is either normally distributed or large enough to rely on the central limit theorem.\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nWhere: - ( s ) is the sample standard deviation (used instead of ( \\(\\sigma\\) ))\nThis statistic follows a t-distribution with ( n - 1 ) degrees of freedom. It is more appropriate for real-world scenarios where we rarely know the true population variance.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter1.html#one-sample-z-test-for-proportions",
    "href": "book/chapter1.html#one-sample-z-test-for-proportions",
    "title": "Chapter 1: Tests for One Continuous Population Mean",
    "section": "One-sample z-test for proportions",
    "text": "One-sample z-test for proportions\nUse this test when: - The variable is binary (success/failure, yes/no, etc.), and - You want to test a population proportion ( p ), using a large enough sample.\nThe test statistic is:\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\\]\nWhere: - ( \\(\\hat{p}\\) ) is the sample proportion\n- ( \\(p_0\\) ) is the hypothesized population proportion\n- ( \\(n\\) ) is the sample size\nThis test assumes ( \\(np_0 \\geq 5\\) ) and ( \\(n(1 - p_0)\\) \\(\\geq\\) 5 ) to justify the normal approximation to the binomial distribution.",
    "crumbs": [
      "Chapter 1: Tests for One Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html",
    "href": "book/chapter2.html",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "",
    "text": "Two sample Student’s t-test for Independent Samples\nThis chapter introduces statistical tests designed to compare two samples which is a fundamental task in data analysis across many disciplines. Whether you’re comparing average recovery times between two medical treatments, student test scores under different teaching methods, comparing the proportion among two samples, or reaction times under varying stress conditions, these methods help determine whether observed differences are statistically significant or simply due to chance.\nIn this chapter, we review tests for comparing two continuous population means under two conditions: when the populations are independent and when they are dependent. Throughout the sections below, we provide details about these tests and required formula for each case. Broadly speaking, there are two main types of tests to compare the means between two continuous populations:\nThe choice of test depends on the structure of your data. This chapter introduces both types of comparisons, beginning with independent samples. Each section includes definitions, theoretical background, and R code examples using real or simulated datasets to help ground the concepts in practice.\nIndependent samples arise when the observations in one group do not influence or relate to the observations in the other. In statistical terms we call this two independent samples. A classic example from educational research is described below:\nSuppose you’re interested in whether a new method of teaching introductory physics improves student performance. To investigate this, you decide to test the method at two universities: the University of British Columbia (UBC) and Simon Fraser University (SFU). You apply the new teaching method at SFU and compare the results to students taught with the traditional method at UBC.\nIn this scenario, students at UBC and SFU form two distinct, unrelated groups. Since the students are not paired or matched across schools, and each individual belongs to only one group, the samples are independent.\nLet us assume that each population has an unknown average physics score denoted by:\n\\[\n    \\mu_1 \\quad \\text{(mean for UBC)}, \\quad \\mu_2 \\quad \\text{(mean for SFU)}.\n\\]\nSince we do not have access to all students’ grades, we take a random sample from each school. Suppose:\nNote that the sample sizes \\(n\\) and \\(m\\) do not need to be equal. Now, the central question becomes:\nIs there a statistically significant difference between the mean physics scores of the two groups?\nIn formal terms, we test the hypotheses:\n\\[H_0: \\mu_1 = \\mu_2 \\quad \\text{versus} \\quad H_A: \\mu_1 \\ne \\mu_2\\]\nTo test this, we use the two-sample t-test, which compares the sample means and incorporates variability within and between the samples. If we assume equal population variances, the test statistic is:\n\\[t = \\frac{(\\bar{X} - \\bar{Y})}{s_p \\sqrt{\\frac{1}{n} + \\frac{1}{m}}}\\]\nwhere:\nUnder the assumption that null hypothesis is correct (i.e. \\(\\mu_1=\\mu_2\\)) then the test statistic defined above follows a t-distribution with \\(n+m-2\\) degrees of freedom (which we denote it by \\(T_{n+m-2}\\)). Knowing the distribution of this statistic helps us to compute \\(\\textit{p-value}\\) of the test as follows:\n\\[\\textit{p-value} = 2 \\times Pr(T_{n+m-2} \\ge |t|)\\] Note: The probability is multiplied by two since we have a two sided hypothesis (alternative is \\(\\mu_1 \\neq \\mu_2\\)). For a one sided test (when alternative hypothesis is \\(\\mu_1 &gt; \\mu_2\\) or \\(\\mu_1 &lt; \\mu_2\\)) we do not need to multiply by two.\nNow let us see how to run the two-sample test on some example datasets in R. We will first describe the dataset we want to use, and then show how to run the test in R.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#two-sample-students-t-test-for-independent-samples",
    "href": "book/chapter2.html#two-sample-students-t-test-for-independent-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "",
    "text": "From UBC (Population 1), we obtain a sample of size \\(n\\), denoted as: \\[X_1, X_2, \\ldots, X_n\\]\nFrom SFU (Population 2), we obtain a sample of size \\(m\\), denoted as: \\[Y_1, Y_2, \\ldots, Y_m\\]\n\n\n\n\n\n\n\n\n\n\n\\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means for UBC and SFU, respectively,\n\n\\(s_p\\) is the , computed as:\n\\(s_p = \\sqrt{\\frac{(n - 1)s_X^2 + (m - 1)s_Y^2}{n + m - 2}}\\)\n\n\\(s_X^2\\) and \\(s_Y^2\\) are the sample variances of the two groups.\n\n\n\n\nData Collection and Wrangling\nExplanatory Data Analysis\nTesting Settings\nHypothesis Definitions\nTest Flavour and Components\nInferential Conclusions\nStorytelling\nIgnore the following content for now. Still editting\nIn this example, we start with a dataset that records the time MDS students spend on course website of DSCI 554. You will encounter this dataset again in DSCI 554 when we discuss A/B/n testing. For now, here’s what the dataset looks like:\n\nABn_data &lt;- read_csv('data/ABn_data.csv', show_col_types = FALSE)\nABn_data &lt;- ABn_data %&gt;% mutate(Colour = as.factor(Colour), Font = as.factor(Font))\nABn_data\n\n# A tibble: 72 × 3\n   Duration Colour Font  \n      &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1       90 Red    Small \n 2       95 Red    Large \n 3      107 Red    Medium\n 4       92 Red    Small \n 5       89 Red    Large \n 6       92 Red    Medium\n 7       81 Red    Small \n 8       92 Red    Large \n 9       93 Red    Medium\n10       80 Blue   Small \n# ℹ 62 more rows\n\n\nThe columns in the dataset are as follows:\n\nFont: A factor variable with three levels — small, medium, and large.\nButton Colour: A factor variable with two levels — Red and Blue.\nDuration: A continuous variable representing the duration of each visit, recorded in minutes.\n\nThe main statistical question we are asking here is:\nIs there a statistically significant difference in the mean visit duration between websites with red buttons and those with blue buttons?\nThis means we are interested in the duration time users spend on website in two different populations: red button design and blue button design. First we do some data selection to create two vector of numbers: one for the visit duration visits in website with red buttons and one for the duration of visits in website with blue buttons. The following code can take care of this. For Red group:\n\nduration_red &lt;- ABn_data %&gt;% filter(Colour == 'Red') %&gt;% pull(Duration)\nduration_red \n\n [1]  90  95 107  92  89  92  81  92  93  83  80  95  98  98 106  74  81  74  85\n[20]  88  88 112 104  91  82  78  94  86  78  89  79  86  87  85  89  83\n\n\nand for Blue group:\n\nduration_blue &lt;- ABn_data %&gt;% filter(Colour == 'Blue') %&gt;% pull(Duration)\nduration_blue\n\n [1]  80  87 100 121 110 119  78  98 122 102 109 105  99  94 123 136 133 132  60\n[20] 104 114  90 118 113 119 122 136  73 114 114 109 131 126 116 136 133\n\n\nHow to run this test in R?\nIn order to run this test, similar to what we learned in (LINK to chapter 1) we can use t.test function in R. The function can be used to perform one or two sample t-tests. The relevant arguments of the function are as follows:\n\n\nx is (non-empty) numeric vector of data values.\n\ny is also (non-empty) numeric vector of data values (can be NULL if you run a one sample test).\n\nvar.equal is a binary value (TRUE/FALSE) to indicate if R needs to assume equal variance or not.\n\nTherefore we can run the test for two different cases.\n\nCase 1: Under the assumption that variances between two populations are equal:\n\n\nt.test(x = duration_red, y = duration_blue, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  duration_red and duration_blue\nt = -6.1183, df = 70, p-value = 4.838e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -28.43489 -14.45400\nsample estimates:\nmean of x mean of y \n  89.0000  110.4444 \n\n\n\nCase 2: Under the assumption that variances between two populations are NOT equal:\n\n\nt.test(x = duration_red, y = duration_blue, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  duration_red and duration_blue\nt = -6.1183, df = 50.043, p-value = 1.429e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -28.48424 -14.40465\nsample estimates:\nmean of x mean of y \n  89.0000  110.4444 \n\n\nIn both outputs, we can see the following:\n\nt is the test statistic.\ndf is the degrees of freedom for the test.\n\np-value is the p-value of the test. Note that, by default, this is for a two-sided test. If you need to conduct a one-sided test, you can either divide the p-value by two or use the alternative argument in the t.test function.\n\n95 percent confidence interval provides the 95% confidence interval for \\(\\mu_1 - \\mu_2\\).\nsample estimates gives the sample means for each group.\n\nNote: By default the value of var.equal is FALSE.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#two-sample-welchs-t-test-for-independent-samples",
    "href": "book/chapter2.html#two-sample-welchs-t-test-for-independent-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "Two sample Welch’s t-test for independent samples",
    "text": "Two sample Welch’s t-test for independent samples\nIf the assumption of equal variances is questionable, we instead use Welch’s t-test, which adjusts the standard error and degrees of freedom accordingly. The Welch’s test statistic is computed as:\n\\[t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{s_X^2}{n} + \\frac{s_Y^2}{m}}}\\]\nUnder the assumption that null hypothesis is correct, the test statistics defined above still follows a t-distribuion but with a different degrees of freedom. The degree of freedom when we do not make equal variance assumption is:\n\\[\\nu = \\frac{\\left( \\frac{s_1^2}{n} + \\frac{s_2^2}{m} \\right)^2}\n{\\frac{\\left( \\frac{s_1^2}{n} \\right)^2}{n - 1} + \\frac{\\left( \\frac{s_2^2}{m} \\right)^2}{m - 1}}\\]\nNote that this degree of freedom is not necessaily an integer number (could be a real number). When we run t-test, we operate under the assumption that: 1) either the sample size is large enough (we are thinking about \\(n=30\\) at least) so that central limit theorem assumptions work well, or 2) the distribution of our sample in each group is normal or symmetric enough.\nIf the normality assumption is also not satisfied (e.g., due to skewed distributions or outliers) or we have a very small sample size, we may turn to a non-parametric alternative, such as the Mann–Whitney–Wilcoxon test, which compares the ranks of the observations across groups rather than the raw values but this book will not cover it. You can read more about it LINK.",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter2.html#paired-samples",
    "href": "book/chapter2.html#paired-samples",
    "title": "Chapter 2: Tests for Two Continuous Population Mean",
    "section": "Paired Samples",
    "text": "Paired Samples\nPaired samples arise when each observation in one group is matched or linked to an observation in the other group. This structure is typical in before-and-after studies, matched-subject designs, or repeated measures on the same individuals. A classic example comes from health sciences.\nSuppose you’re investigating whether a new diet plan reduces blood pressure. You recruit a group of participants and record their blood pressure before starting the diet. After following the diet for two months, you measure their blood pressure again. In this scenario, each participant contributes two measurements: one before the intervention and one after. These measurements are not independent as they come from the same person. Therefore we treat them as paired.\nTo formulate the problem and hypothesis, let us assume that each individual has two measurements:\n\nBefore the diet: \\(X_1, X_2, \\ldots, X_n\\)\nAfter the diet: \\(Y_1, Y_2, \\ldots, Y_n\\)\n\nNote that in this case the sample size is the same (in both before and after diet sample we have \\(n\\) observations). We call this a paired sample. Since the samples are paired, we define the difference for each individual as follows:\n\\[D_i = Y_i - X_i \\quad \\textit{for} \\quad i = 1,2, \\ldots, n\\] Each \\(D_i\\) is the difference of blood pressure after and before using new diet. The main statistical question now is:\nIs there a statistically significant difference in the mean blood pressure before and after the diet?\nIn other words, we test the following hypothesis:\n\\[H_0: \\mu_D = 0 \\quad \\text{versus} \\quad H_A: \\mu_D \\ne 0\\] Here the notation of \\(\\mu_D\\) is the population mean of the differences of \\(D_i\\) which is an unknown parameter in the population. To test this hypothesis, we use the paired t-test, which is essentially a one-sample t-test on the differences \\(D_1, D_2, \\ldots, D_n\\). We test \\(\\mu_D=0\\) because if there is an actual effect of diet on blood pressure, we expect the null hypothesis to be rejected.\nThe test statistic for this hypothesis testing is:\n\\[t = \\frac{\\bar{D}}{s_D / \\sqrt{n}}\\]\nwhere:\n\n\n\\(\\bar{D}\\) is the sample mean of the differences,\n\n\\(s_D\\) is the sample standard deviation of the differences,\n\n\\(n\\) is the number of pairs.\n\nThe standard deviation of the differences is calculated as:\n\\[s_D = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^n (D_i - \\bar{D})^2 }\\]\nUnder the null hypothesis, the test statistic follows a t-distribution with \\(n-1\\) degrees of freedom. For this test, we can compute the \\(\\textit{p-value}\\) as:\n\\[\\textit{p-value} = 2 \\times \\Pr(T_{n - 1} \\ge |t|)\\]\nExample dataset in R\n\nTBD\nHow to run the test in R?",
    "crumbs": [
      "Chapter 2: Tests for Two Continuous Population Mean"
    ]
  },
  {
    "objectID": "book/chapter3.html",
    "href": "book/chapter3.html",
    "title": "Chapter 3: ANOVA-related Tests for \\(k\\) Continuous Population Means",
    "section": "",
    "text": "mindmap\n  root((Frequentist\n  Hypothesis \n  Testings\n  ))\n    Simulation Based&lt;br/&gt;Tests\n    Classical&lt;br/&gt;Tests\n      (Chapter 1: &lt;br/&gt;Tests for One&lt;br/&gt;Continuous&lt;br/&gt;Population Mean)\n      (Chapter 2: &lt;br/&gt;Tests for Two&lt;br/&gt;Continuous&lt;br/&gt;Population Means)\n      (Chapter 3: &lt;br/&gt;ANOVA-related &lt;br/&gt;Tests for&lt;br/&gt;k Continuous&lt;br/&gt;Population Means)\n        {{Unbounded&lt;br/&gt;Responses}}\n          One&lt;br/&gt;Factor type&lt;br/&gt;Feature\n            )One way&lt;br/&gt;ANOVA(\n          Two&lt;br/&gt;Factor type&lt;br/&gt;Features\n            )Two way&lt;br/&gt;ANOVA(\n\n\n\n\n\n\n\n\nFigure 1: A specific hypothesis testing mind map outlining the techniques explored in this chapter, which include ANOVA-related tests for \\(k\\) population means.",
    "crumbs": [
      "Chapter 3: ANOVA-related Tests for $k$ Continuous Population Means"
    ]
  }
]